{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def add_start_end(string):\n",
    "    string = string.split()\n",
    "    strings = []\n",
    "    for s in string:\n",
    "        if s[-1] == '-':\n",
    "            s = s[:-1]\n",
    "        s = list(s)\n",
    "        s[0] = '<%s'%(s[0])\n",
    "        s[-1] = '%s>'%(s[-1])\n",
    "        strings.extend(s)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stemmer-data-v5.txt','r') as fopen:\n",
    "    texts = fopen.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41528, 41528)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after, before = [], []\n",
    "for i in texts:\n",
    "    splitted = i.encode('ascii', 'ignore').decode(\"utf-8\").lower().split('\\t')\n",
    "    if len(splitted) < 2:\n",
    "        continue\n",
    "    after.append(add_start_end(splitted[1]))\n",
    "    before.append(add_start_end(splitted[0]))\n",
    "    \n",
    "len(after), len(before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s', 'p', 'i', 'd', 'e', 'r>'],\n",
       " ['<t', 'r', 'a', 'n', 's', 'e', 'k', 's', 'u', 'a', 'l>'],\n",
       " ['<k', 'h', 'u', 's', 'u', 's>'],\n",
       " ['<k', 'r', 'i', 'm', 'i', 'n', 'o', 'l', 'o', 'g', 'i>'],\n",
       " ['<n', 'e', 'g', 'e', 'r', 'i', 'p', 'i', 'n', 'g', 'a', 't>']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s', 'p', 'i', 'd', 'e', 'r>'],\n",
       " ['<t', 'r', 'a', 'n', 's', 'e', 'k', 's', 'u', 'a', 'l>'],\n",
       " ['<k', 'h', 'u', 's', 'u', 's>'],\n",
       " ['<k', 'r', 'i', 'm', 'i', 'n', 'o', 'l', 'o', 'g', 'i>'],\n",
       " ['<n', 'e', 'g', 'e', 'r', 'i', 'p', 'i', 'n', 'g', 'a', 't>']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 80\n",
      "Most common words [('a', 44151), ('e', 26905), ('n', 22703), ('i', 21583), ('r', 16110), ('u', 12472)]\n",
      "Sample data [27, 20, 7, 19, 5, 38, 35, 8, 4, 6] ['<s', 'p', 'i', 'd', 'e', 'r>', '<t', 'r', 'a', 'n']\n",
      "filtered vocab size: 84\n",
      "% of vocab used: 105.0%\n"
     ]
    }
   ],
   "source": [
    "concat_from = list(itertools.chain(*before))\n",
    "vocabulary_size_from = len(list(set(concat_from)))\n",
    "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
    "print('vocab from size: %d'%(vocabulary_size_from))\n",
    "print('Most common words', count_from[4:10])\n",
    "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])\n",
    "print('filtered vocab size:',len(dictionary_from))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_from)/vocabulary_size_from,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 80\n",
      "Most common words [('a', 35149), ('e', 18010), ('i', 17934), ('n', 15750), ('r', 12021), ('u', 11371)]\n",
      "Sample data [20, 21, 6, 17, 5, 32, 30, 8, 4, 7] ['<s', 'p', 'i', 'd', 'e', 'r>', '<t', 'r', 'a', 'n']\n",
      "filtered vocab size: 84\n",
      "% of vocab used: 105.0%\n"
     ]
    }
   ],
   "source": [
    "concat_to = list(itertools.chain(*after))\n",
    "vocabulary_size_to = len(list(set(concat_to)))\n",
    "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
    "print('vocab from size: %d'%(vocabulary_size_to))\n",
    "print('Most common words', count_to[4:10])\n",
    "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])\n",
    "print('filtered vocab size:',len(dictionary_to))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary_to)/vocabulary_size_to,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary_from['GO']\n",
    "PAD = dictionary_from['PAD']\n",
    "EOS = dictionary_from['EOS']\n",
    "UNK = dictionary_from['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(after)):\n",
    "    after[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 from_dict_size, to_dict_size, learning_rate, \n",
    "                 dropout = 0.8, beam_width = 15, force_teaching_ratio=0.5):\n",
    "        \n",
    "        def lstm_cell(reuse=False):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        encoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.encoder_out, self.encoder_state = tf.nn.dynamic_rnn(cell = encoder_cells, \n",
    "                                                                 inputs = encoder_embedded, \n",
    "                                                                 sequence_length = self.X_seq_len,\n",
    "                                                                 dtype = tf.float32)\n",
    "        \n",
    "        encoder_state = tuple(self.encoder_state[-1] for _ in range(num_layers))\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        dense_layer = tf.layers.Dense(to_dict_size)\n",
    "\n",
    "        with tf.variable_scope('decode'):\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            num_units = size_layer, \n",
    "            memory = encoder_embedded,\n",
    "            memory_sequence_length = self.X_seq_len)\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)]),\n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = size_layer)\n",
    "            main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "            decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "            training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "            inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
    "                sequence_length = self.Y_seq_len,\n",
    "                embedding = decoder_embeddings,\n",
    "                sampling_probability = 1 - force_teaching_ratio,\n",
    "                time_major = False)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell = decoder_cell,\n",
    "                helper = training_helper,\n",
    "                initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\n",
    "                output_layer = dense_layer)\n",
    "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = training_decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            encoder_out_tiled = tf.contrib.seq2seq.tile_batch(encoder_embedded, beam_width)\n",
    "            encoder_state_tiled = tf.contrib.seq2seq.tile_batch(encoder_state, beam_width)\n",
    "            X_seq_len_tiled = tf.contrib.seq2seq.tile_batch(self.X_seq_len, beam_width)\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units = size_layer, \n",
    "                memory = encoder_out_tiled,\n",
    "                memory_sequence_length = X_seq_len_tiled)\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(reuse=True) for _ in range(num_layers)]),\n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = size_layer)\n",
    "            predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell = decoder_cell,\n",
    "                embedding = decoder_embeddings,\n",
    "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                end_token = EOS,\n",
    "                initial_state = decoder_cell.zero_state(batch_size * beam_width, tf.float32).clone(cell_state = encoder_state_tiled),\n",
    "                beam_width = beam_width,\n",
    "                output_layer = dense_layer,\n",
    "                length_penalty_weight = 0.0)\n",
    "            predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = predicting_decoder,\n",
    "                impute_finished = False,\n",
    "                maximum_iterations = 2 * tf.reduce_max(self.X_seq_len))\n",
    "            \n",
    "            \n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.predicting_ids = tf.identity(predicting_decoder_output.predicted_ids[:, :, 0],name=\"logits\")\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 2\n",
    "embedded_size = 64\n",
    "learning_rate = 1e-3\n",
    "batch_size = 128\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Stemmer(size_layer, num_layers, embedded_size, len(dictionary_from), \n",
    "                len(dictionary_to), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic, UNK=3):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i:\n",
    "            ints.append(dic.get(k, UNK))\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(before, dictionary_from)\n",
    "Y = str_idx(after, dictionary_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:17<00:00, 15.21it/s, accuracy=0.79, cost=0.775] \n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 32.30it/s, accuracy=0.673, cost=1.19] \n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:14, 17.23it/s, accuracy=0.758, cost=0.891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg loss: 1.332408, avg accuracy: 0.670189\n",
      "epoch: 0, avg loss test: 0.987139, avg accuracy test: 0.735963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 15.36it/s, accuracy=0.791, cost=0.726]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 31.24it/s, accuracy=0.798, cost=0.696]\n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:15, 16.79it/s, accuracy=0.761, cost=0.809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 0.864946, avg accuracy: 0.760973\n",
      "epoch: 1, avg loss test: 0.741874, avg accuracy test: 0.787838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 16.33it/s, accuracy=0.814, cost=0.555]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 33.61it/s, accuracy=0.85, cost=0.497] \n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:19, 13.38it/s, accuracy=0.856, cost=0.475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, avg loss: 0.614769, avg accuracy: 0.818570\n",
      "epoch: 2, avg loss test: 0.523826, avg accuracy test: 0.842014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 15.82it/s, accuracy=0.896, cost=0.334]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 31.91it/s, accuracy=0.859, cost=0.439]\n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:15, 17.20it/s, accuracy=0.899, cost=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, avg loss: 0.453879, avg accuracy: 0.861574\n",
      "epoch: 3, avg loss test: 0.408042, avg accuracy test: 0.875363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 16.33it/s, accuracy=0.862, cost=0.44] \n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 30.80it/s, accuracy=0.893, cost=0.346]\n",
      "train minibatch loop:   0%|          | 0/260 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, avg loss: 0.358987, avg accuracy: 0.889736\n",
      "epoch: 4, avg loss test: 0.346308, avg accuracy test: 0.893755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 16.64it/s, accuracy=0.931, cost=0.234]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 33.10it/s, accuracy=0.923, cost=0.242]\n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:16, 15.29it/s, accuracy=0.909, cost=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, avg loss: 0.292531, avg accuracy: 0.910269\n",
      "epoch: 5, avg loss test: 0.276578, avg accuracy test: 0.916534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 15.53it/s, accuracy=0.94, cost=0.212] \n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 30.45it/s, accuracy=0.923, cost=0.24] \n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:15, 16.77it/s, accuracy=0.924, cost=0.24] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, avg loss: 0.246287, avg accuracy: 0.925488\n",
      "epoch: 6, avg loss test: 0.242350, avg accuracy test: 0.927027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 15.75it/s, accuracy=0.923, cost=0.264]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 27.27it/s, accuracy=0.934, cost=0.231]\n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:16, 15.83it/s, accuracy=0.929, cost=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, avg loss: 0.219649, avg accuracy: 0.934410\n",
      "epoch: 7, avg loss test: 0.231226, avg accuracy test: 0.931023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 15.81it/s, accuracy=0.933, cost=0.227]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 30.91it/s, accuracy=0.935, cost=0.211]\n",
      "train minibatch loop:   1%|          | 2/260 [00:00<00:15, 16.72it/s, accuracy=0.945, cost=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, avg loss: 0.189146, avg accuracy: 0.943443\n",
      "epoch: 8, avg loss test: 0.196329, avg accuracy test: 0.941194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 260/260 [00:16<00:00, 16.80it/s, accuracy=0.939, cost=0.208]\n",
      "test minibatch loop: 100%|██████████| 65/65 [00:02<00:00, 31.72it/s, accuracy=0.947, cost=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, avg loss: 0.168858, avg accuracy: 0.949601\n",
      "epoch: 9, avg loss test: 0.175603, avg accuracy test: 0.947418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "for EPOCH in range(epoch):\n",
    "    lasttime = time.time()\n",
    "    total_loss, total_accuracy, total_loss_test, total_accuracy_test = 0, 0, 0, 0\n",
    "    train_X, train_Y = shuffle(train_X, train_Y)\n",
    "    test_X, test_Y = shuffle(test_X, test_Y)\n",
    "    pbar = tqdm(range(0, len(train_X), batch_size), desc='train minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(train_X))\n",
    "        batch_x, seq_x = pad_sentence_batch(train_X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(train_Y[k: k+batch_size], PAD)\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss += loss\n",
    "        total_accuracy += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "        \n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc='test minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(test_X))\n",
    "        batch_x, seq_x = pad_sentence_batch(test_X[k: k+batch_size], PAD)\n",
    "        batch_y, seq_y = pad_sentence_batch(test_Y[k: k+batch_size], PAD)\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss_test += loss\n",
    "        total_accuracy_test += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "        \n",
    "    total_loss /= (len(train_X) / batch_size)\n",
    "    total_accuracy /= (len(train_X) / batch_size)\n",
    "    total_loss_test /= (len(test_X) / batch_size)\n",
    "    total_accuracy_test /= (len(test_X) / batch_size)\n",
    "        \n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(EPOCH, total_loss, total_accuracy))\n",
    "    print('epoch: %d, avg loss test: %f, avg accuracy test: %f'%(EPOCH, total_loss_test, total_accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = sess.run(model.predicting_ids, \n",
    "                     feed_dict={model.X:batch_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 1\n",
      "BEFORE: <selcat>\n",
      "REAL AFTER: <selcat>\n",
      "PREDICTED AFTER: <selcat> \n",
      "\n",
      "row 2\n",
      "BEFORE: <enphc>\n",
      "REAL AFTER: <enphc>\n",
      "PREDICTED AFTER: <enphc> \n",
      "\n",
      "row 3\n",
      "BEFORE: <activists>\n",
      "REAL AFTER: <activists>\n",
      "PREDICTED AFTER: <activists> \n",
      "\n",
      "row 4\n",
      "BEFORE: <kelembapan>\n",
      "REAL AFTER: <lembap>\n",
      "PREDICTED AFTER: <lembap> \n",
      "\n",
      "row 5\n",
      "BEFORE: <dok>\n",
      "REAL AFTER: <dok>\n",
      "PREDICTED AFTER: <dok> \n",
      "\n",
      "row 6\n",
      "BEFORE: <skiers>\n",
      "REAL AFTER: <skiers>\n",
      "PREDICTED AFTER: <skiers> \n",
      "\n",
      "row 7\n",
      "BEFORE: <gfmas>\n",
      "REAL AFTER: <gfmas>\n",
      "PREDICTED AFTER: <gfmas> \n",
      "\n",
      "row 8\n",
      "BEFORE: <shamsul>\n",
      "REAL AFTER: <shamsul>\n",
      "PREDICTED AFTER: <shamsul> \n",
      "\n",
      "row 9\n",
      "BEFORE: <menakutkan>\n",
      "REAL AFTER: <takut>\n",
      "PREDICTED AFTER: <akut> \n",
      "\n",
      "row 10\n",
      "BEFORE: <transcoding>\n",
      "REAL AFTER: <transcoding>\n",
      "PREDICTED AFTER: <transcoding> \n",
      "\n",
      "row 11\n",
      "BEFORE: <sejat>\n",
      "REAL AFTER: <sejat>\n",
      "PREDICTED AFTER: <sejat> \n",
      "\n",
      "row 12\n",
      "BEFORE: <exceptional>\n",
      "REAL AFTER: <exceptional>\n",
      "PREDICTED AFTER: <exceptional> \n",
      "\n",
      "row 13\n",
      "BEFORE: <atu>\n",
      "REAL AFTER: <atu>\n",
      "PREDICTED AFTER: <atu> \n",
      "\n",
      "row 14\n",
      "BEFORE: <ssdhaliwal>\n",
      "REAL AFTER: <ssdhaliwal>\n",
      "PREDICTED AFTER: <sswhilawal> \n",
      "\n",
      "row 15\n",
      "BEFORE: <raket>\n",
      "REAL AFTER: <raket>\n",
      "PREDICTED AFTER: <raket> \n",
      "\n",
      "row 16\n",
      "BEFORE: <cad>\n",
      "REAL AFTER: <cad>\n",
      "PREDICTED AFTER: <cad> \n",
      "\n",
      "row 17\n",
      "BEFORE: <aktivisme>\n",
      "REAL AFTER: <aktivisme>\n",
      "PREDICTED AFTER: <aktivisme> \n",
      "\n",
      "row 18\n",
      "BEFORE: <diterapkan>\n",
      "REAL AFTER: <terap>\n",
      "PREDICTED AFTER: <terap> \n",
      "\n",
      "row 19\n",
      "BEFORE: <ditempuh>\n",
      "REAL AFTER: <tempuh>\n",
      "PREDICTED AFTER: <tempuh> \n",
      "\n",
      "row 20\n",
      "BEFORE: <entri>\n",
      "REAL AFTER: <entri>\n",
      "PREDICTED AFTER: <entri> \n",
      "\n",
      "row 21\n",
      "BEFORE: <rashad>\n",
      "REAL AFTER: <rashad>\n",
      "PREDICTED AFTER: <rashad> \n",
      "\n",
      "row 22\n",
      "BEFORE: <mission>\n",
      "REAL AFTER: <mission>\n",
      "PREDICTED AFTER: <mission> \n",
      "\n",
      "row 23\n",
      "BEFORE: <museum>\n",
      "REAL AFTER: <museum>\n",
      "PREDICTED AFTER: <museum> \n",
      "\n",
      "row 24\n",
      "BEFORE: <situations>\n",
      "REAL AFTER: <situations>\n",
      "PREDICTED AFTER: <situations> \n",
      "\n",
      "row 25\n",
      "BEFORE: <terancam>\n",
      "REAL AFTER: <ancam>\n",
      "PREDICTED AFTER: <ancam> \n",
      "\n",
      "row 26\n",
      "BEFORE: <farabi>\n",
      "REAL AFTER: <farabi>\n",
      "PREDICTED AFTER: <farabi> \n",
      "\n",
      "row 27\n",
      "BEFORE: <wide>\n",
      "REAL AFTER: <wide>\n",
      "PREDICTED AFTER: <wide> \n",
      "\n",
      "row 28\n",
      "BEFORE: <berkenaa>\n",
      "REAL AFTER: <berkenaa>\n",
      "PREDICTED AFTER: <kena> \n",
      "\n",
      "row 29\n",
      "BEFORE: <mendaulat>\n",
      "REAL AFTER: <daulat>\n",
      "PREDICTED AFTER: <aaulat> \n",
      "\n",
      "row 30\n",
      "BEFORE: <menyeluruh>\n",
      "REAL AFTER: <seluruh>\n",
      "PREDICTED AFTER: <seluruh> \n",
      "\n",
      "row 31\n",
      "BEFORE: <affandy>\n",
      "REAL AFTER: <affandy>\n",
      "PREDICTED AFTER: <affandy> \n",
      "\n",
      "row 32\n",
      "BEFORE: <foto-foto>\n",
      "REAL AFTER: <foto>\n",
      "PREDICTED AFTER: <foto-foto> \n",
      "\n",
      "row 33\n",
      "BEFORE: <mencederakan>\n",
      "REAL AFTER: <cedera>\n",
      "PREDICTED AFTER: <cedera> \n",
      "\n",
      "row 34\n",
      "BEFORE: <manir>\n",
      "REAL AFTER: <manir>\n",
      "PREDICTED AFTER: <manir> \n",
      "\n",
      "row 35\n",
      "BEFORE: <menyelamatkannya>\n",
      "REAL AFTER: <selamat>\n",
      "PREDICTED AFTER: <selamat> \n",
      "\n",
      "row 36\n",
      "BEFORE: <edu>\n",
      "REAL AFTER: <edu>\n",
      "PREDICTED AFTER: <edu> \n",
      "\n",
      "row 37\n",
      "BEFORE: <reutersspeaker>\n",
      "REAL AFTER: <reutersspeaker>\n",
      "PREDICTED AFTER: <reutersespeper> \n",
      "\n",
      "row 38\n",
      "BEFORE: <lithuania>\n",
      "REAL AFTER: <lithuania>\n",
      "PREDICTED AFTER: <lithuania> \n",
      "\n",
      "row 39\n",
      "BEFORE: <khanazah>\n",
      "REAL AFTER: <khanazah>\n",
      "PREDICTED AFTER: <khanaaah> \n",
      "\n",
      "row 40\n",
      "BEFORE: <otoritas>\n",
      "REAL AFTER: <otoritas>\n",
      "PREDICTED AFTER: <otoritas> \n",
      "\n",
      "row 41\n",
      "BEFORE: <diizinkan>\n",
      "REAL AFTER: <izin>\n",
      "PREDICTED AFTER: <iiin> \n",
      "\n",
      "row 42\n",
      "BEFORE: <ramaikah>\n",
      "REAL AFTER: <ramai>\n",
      "PREDICTED AFTER: <ramai \n",
      "\n",
      "row 43\n",
      "BEFORE: <bodo>\n",
      "REAL AFTER: <bodo>\n",
      "PREDICTED AFTER: <bodo> \n",
      "\n",
      "row 44\n",
      "BEFORE: <pendiam>\n",
      "REAL AFTER: <diam>\n",
      "PREDICTED AFTER: <diam> \n",
      "\n",
      "row 45\n",
      "BEFORE: <i-activsense>\n",
      "REAL AFTER: <i-activsense>\n",
      "PREDICTED AFTER: <icatvivessoe> \n",
      "\n",
      "row 46\n",
      "BEFORE: <kebolehupayaan>\n",
      "REAL AFTER: <kebolehupayaan>\n",
      "PREDICTED AFTER: <kebellhpepaaan> \n",
      "\n",
      "row 47\n",
      "BEFORE: <terselah>\n",
      "REAL AFTER: <lah>\n",
      "PREDICTED AFTER: <selah> \n",
      "\n",
      "row 48\n",
      "BEFORE: <bicaranya>\n",
      "REAL AFTER: <bicara>\n",
      "PREDICTED AFTER: <bicar> \n",
      "\n",
      "row 49\n",
      "BEFORE: <schumer>\n",
      "REAL AFTER: <schumer>\n",
      "PREDICTED AFTER: <schumer> \n",
      "\n",
      "row 50\n",
      "BEFORE: <ilmin>\n",
      "REAL AFTER: <ilmin>\n",
      "PREDICTED AFTER: <ilmin> \n",
      "\n",
      "row 51\n",
      "BEFORE: <prucuma>\n",
      "REAL AFTER: <prucuma>\n",
      "PREDICTED AFTER: <prucuma> \n",
      "\n",
      "row 52\n",
      "BEFORE: <worthy>\n",
      "REAL AFTER: <worthy>\n",
      "PREDICTED AFTER: <worthy> \n",
      "\n",
      "row 53\n",
      "BEFORE: <penghabisan>\n",
      "REAL AFTER: <habis>\n",
      "PREDICTED AFTER: <habis> \n",
      "\n",
      "row 54\n",
      "BEFORE: <kaviti>\n",
      "REAL AFTER: <kaviti>\n",
      "PREDICTED AFTER: <kaviti> \n",
      "\n",
      "row 55\n",
      "BEFORE: <dosanya>\n",
      "REAL AFTER: <dosa>\n",
      "PREDICTED AFTER: <dosa> \n",
      "\n",
      "row 56\n",
      "BEFORE: <hampa>\n",
      "REAL AFTER: <hampa>\n",
      "PREDICTED AFTER: <hampa> \n",
      "\n",
      "row 57\n",
      "BEFORE: <komaruddin>\n",
      "REAL AFTER: <komaruddin>\n",
      "PREDICTED AFTER: <komaruddin> \n",
      "\n",
      "row 58\n",
      "BEFORE: <diode>\n",
      "REAL AFTER: <diode>\n",
      "PREDICTED AFTER: <diode> \n",
      "\n",
      "row 59\n",
      "BEFORE: <monsun>\n",
      "REAL AFTER: <monsun>\n",
      "PREDICTED AFTER: <monsun> \n",
      "\n",
      "row 60\n",
      "BEFORE: <paripurnakan>\n",
      "REAL AFTER: <paripurna>\n",
      "PREDICTED AFTER: <paripurnakan> \n",
      "\n",
      "row 61\n",
      "BEFORE: <wanitakena>\n",
      "REAL AFTER: <wanitakena>\n",
      "PREDICTED AFTER: <wanitakena> \n",
      "\n",
      "row 62\n",
      "BEFORE: <progressive>\n",
      "REAL AFTER: <progressive>\n",
      "PREDICTED AFTER: <progressive> \n",
      "\n",
      "row 63\n",
      "BEFORE: <diasah>\n",
      "REAL AFTER: <asah>\n",
      "PREDICTED AFTER: <asah> \n",
      "\n",
      "row 64\n",
      "BEFORE: <those>\n",
      "REAL AFTER: <those>\n",
      "PREDICTED AFTER: <those> \n",
      "\n",
      "row 65\n",
      "BEFORE: <disubsidi>\n",
      "REAL AFTER: <subsidi>\n",
      "PREDICTED AFTER: <susbini> \n",
      "\n",
      "row 66\n",
      "BEFORE: <mufassirin>\n",
      "REAL AFTER: <mufassirin>\n",
      "PREDICTED AFTER: <mufassirin> \n",
      "\n",
      "row 67\n",
      "BEFORE: <digalak>\n",
      "REAL AFTER: <galak>\n",
      "PREDICTED AFTER: <galak> \n",
      "\n",
      "row 68\n",
      "BEFORE: <sayangi>\n",
      "REAL AFTER: <sayang>\n",
      "PREDICTED AFTER: <sayang> \n",
      "\n",
      "row 69\n",
      "BEFORE: <negarakita>\n",
      "REAL AFTER: <negarakita>\n",
      "PREDICTED AFTER: <negarakita> \n",
      "\n",
      "row 70\n",
      "BEFORE: <the>\n",
      "REAL AFTER: <the>\n",
      "PREDICTED AFTER: <the> \n",
      "\n",
      "row 71\n",
      "BEFORE: <dihaluskan>\n",
      "REAL AFTER: <halus>\n",
      "PREDICTED AFTER: <halus> \n",
      "\n",
      "row 72\n",
      "BEFORE: <prk>\n",
      "REAL AFTER: <prk>\n",
      "PREDICTED AFTER: <prk> \n",
      "\n",
      "row 73\n",
      "BEFORE: <fiberization>\n",
      "REAL AFTER: <fiberization>\n",
      "PREDICTED AFTER: <fiberization> \n",
      "\n",
      "row 74\n",
      "BEFORE: <ketika>\n",
      "REAL AFTER: <ketika>\n",
      "PREDICTED AFTER: <ketika> \n",
      "\n",
      "row 75\n",
      "BEFORE: <konsumtif>\n",
      "REAL AFTER: <konsumtif>\n",
      "PREDICTED AFTER: <konsumtif> \n",
      "\n",
      "row 76\n",
      "BEFORE: <adzhar>\n",
      "REAL AFTER: <adzhar>\n",
      "PREDICTED AFTER: <adzhar> \n",
      "\n",
      "row 77\n",
      "BEFORE: <cun>\n",
      "REAL AFTER: <cun>\n",
      "PREDICTED AFTER: <cun> \n",
      "\n",
      "row 78\n",
      "BEFORE: <jepangperdana>\n",
      "REAL AFTER: <jepangperdana>\n",
      "PREDICTED AFTER: <jepangerdanda> \n",
      "\n",
      "row 79\n",
      "BEFORE: <merahsiakannya>\n",
      "REAL AFTER: <merahsiakannya>\n",
      "PREDICTED AFTER: <rahisa> \n",
      "\n",
      "row 80\n",
      "BEFORE: <harimurti>\n",
      "REAL AFTER: <harimurti>\n",
      "PREDICTED AFTER: <harimurti> \n",
      "\n",
      "row 81\n",
      "BEFORE: <tamadun>\n",
      "REAL AFTER: <tamadun>\n",
      "PREDICTED AFTER: <tamadun> \n",
      "\n",
      "row 82\n",
      "BEFORE: <siklus>\n",
      "REAL AFTER: <siklus>\n",
      "PREDICTED AFTER: <siklus> \n",
      "\n",
      "row 83\n",
      "BEFORE: <ditekankan>\n",
      "REAL AFTER: <tekan>\n",
      "PREDICTED AFTER: <tekan> \n",
      "\n",
      "row 84\n",
      "BEFORE: <naza>\n",
      "REAL AFTER: <naza>\n",
      "PREDICTED AFTER: <naza> \n",
      "\n",
      "row 85\n",
      "BEFORE: <aging>\n",
      "REAL AFTER: <aging>\n",
      "PREDICTED AFTER: <aging> \n",
      "\n",
      "row 86\n",
      "BEFORE: <pertikaikanjika>\n",
      "REAL AFTER: <pertikaikanjika>\n",
      "PREDICTED AFTER: <perkitakijanika> \n",
      "\n",
      "row 87\n",
      "BEFORE: <jazair>\n",
      "REAL AFTER: <jazair>\n",
      "PREDICTED AFTER: <jazair> \n",
      "\n",
      "row 88\n",
      "BEFORE: <keberterimaan>\n",
      "REAL AFTER: <terima>\n",
      "PREDICTED AFTER: <kerima> \n",
      "\n",
      "row 89\n",
      "BEFORE: <sulawesi>\n",
      "REAL AFTER: <sulawesi>\n",
      "PREDICTED AFTER: <sulawesi> \n",
      "\n",
      "row 90\n",
      "BEFORE: <atheis>\n",
      "REAL AFTER: <atheis>\n",
      "PREDICTED AFTER: <atheis> \n",
      "\n",
      "row 91\n",
      "BEFORE: <rektum>\n",
      "REAL AFTER: <rektum>\n",
      "PREDICTED AFTER: <rektum> \n",
      "\n",
      "row 92\n",
      "BEFORE: <this>\n",
      "REAL AFTER: <this>\n",
      "PREDICTED AFTER: <this> \n",
      "\n",
      "row 93\n",
      "BEFORE: <entomologi>\n",
      "REAL AFTER: <entomologi>\n",
      "PREDICTED AFTER: <entomologi> \n",
      "\n",
      "row 94\n",
      "BEFORE: <penyebarnya>\n",
      "REAL AFTER: <sebar>\n",
      "PREDICTED AFTER: <sebar> \n",
      "\n",
      "row 95\n",
      "BEFORE: <heartbreaking>\n",
      "REAL AFTER: <heartbreaking>\n",
      "PREDICTED AFTER: <heartbrreking> \n",
      "\n",
      "row 96\n",
      "BEFORE: <imagine>\n",
      "REAL AFTER: <imagine>\n",
      "PREDICTED AFTER: <imagine> \n",
      "\n",
      "row 97\n",
      "BEFORE: <cerah>\n",
      "REAL AFTER: <cerah>\n",
      "PREDICTED AFTER: <cerah> \n",
      "\n",
      "row 98\n",
      "BEFORE: <marcia>\n",
      "REAL AFTER: <marcia>\n",
      "PREDICTED AFTER: <marcia> \n",
      "\n",
      "row 99\n",
      "BEFORE: <ajaran-ajaran>\n",
      "REAL AFTER: <ajar>\n",
      "PREDICTED AFTER: <ajar> \n",
      "\n",
      "row 100\n",
      "BEFORE: <meaning>\n",
      "REAL AFTER: <meaning>\n",
      "PREDICTED AFTER: <meaning> \n",
      "\n",
      "row 101\n",
      "BEFORE: <zulkoffli>\n",
      "REAL AFTER: <zulkoffli>\n",
      "PREDICTED AFTER: <zulkoffli> \n",
      "\n",
      "row 102\n",
      "BEFORE: <dicemuh>\n",
      "REAL AFTER: <cuh>\n",
      "PREDICTED AFTER: <cemuh> \n",
      "\n",
      "row 103\n",
      "BEFORE: <klk>\n",
      "REAL AFTER: <klk>\n",
      "PREDICTED AFTER: <klk> \n",
      "\n",
      "row 104\n",
      "BEFORE: <jsr>\n",
      "REAL AFTER: <jsr>\n",
      "PREDICTED AFTER: <jsr> \n",
      "\n",
      "row 105\n",
      "BEFORE: <bharu>\n",
      "REAL AFTER: <bharu>\n",
      "PREDICTED AFTER: <bharu> \n",
      "\n",
      "row 106\n",
      "BEFORE: <vokalis>\n",
      "REAL AFTER: <vokalis>\n",
      "PREDICTED AFTER: <vokalis> \n",
      "\n",
      "row 107\n",
      "BEFORE: <tekak>\n",
      "REAL AFTER: <tekak>\n",
      "PREDICTED AFTER: <tekak> \n",
      "\n",
      "row 108\n",
      "BEFORE: <daha>\n",
      "REAL AFTER: <daha>\n",
      "PREDICTED AFTER: <daha> \n",
      "\n",
      "row 109\n",
      "BEFORE: <krane>\n",
      "REAL AFTER: <krane>\n",
      "PREDICTED AFTER: <krane> \n",
      "\n",
      "row 110\n",
      "BEFORE: <pendokong>\n",
      "REAL AFTER: <pendokong>\n",
      "PREDICTED AFTER: <dokong> \n",
      "\n",
      "row 111\n",
      "BEFORE: <anjar>\n",
      "REAL AFTER: <anjar>\n",
      "PREDICTED AFTER: <anjar> \n",
      "\n",
      "row 112\n",
      "BEFORE: <aalco>\n",
      "REAL AFTER: <aalco>\n",
      "PREDICTED AFTER: <aalco> \n",
      "\n",
      "row 113\n",
      "BEFORE: <berpenduduk>\n",
      "REAL AFTER: <duduk>\n",
      "PREDICTED AFTER: <duduk> \n",
      "\n",
      "row 114\n",
      "BEFORE: <municipiului>\n",
      "REAL AFTER: <municipiului>\n",
      "PREDICTED AFTER: <municipiluli> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('BEFORE:',''.join([rev_dictionary_from[n] for n in batch_x[i] if n not in [0,1,2,3]]))\n",
    "    print('REAL AFTER:',''.join([rev_dictionary_to[n] for n in batch_y[i] if n not in[0,1,2,3]]))\n",
    "    print('PREDICTED AFTER:',''.join([rev_dictionary_to[n] for n in predicted[i] if n not in[0,1,2,3]]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
