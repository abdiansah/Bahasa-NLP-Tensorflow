{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import malaya\n",
    "malaya.bump_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = malaya.texts._text_functions.split_into_sentences\n",
    "text_cleaning = malaya.texts._text_functions.summary_textcleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11258"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "files = ['politics.json', 'education.json', 'economy.json', 'business.json']\n",
    "sentences = []\n",
    "for file in files:\n",
    "    with open(file) as fopen:\n",
    "        news = json.load(fopen)\n",
    "    for n in news:\n",
    "        if len(n['text']) > 50:\n",
    "            splitted = text_split(n['text'])\n",
    "            sentences.extend(splitted)\n",
    "            \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [text_cleaning(s)[1] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "n_topics = 10\n",
    "embedding_size = 128\n",
    "epoch = 5\n",
    "switch_loss = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA2VEC:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_unique_documents,\n",
    "        vocab_size,\n",
    "        num_topics,\n",
    "        freqs,\n",
    "        embedding_size = 128,\n",
    "        num_sampled = 40,\n",
    "        learning_rate = 1e-3,\n",
    "        lmbda = 150.0,\n",
    "        alpha = None,\n",
    "        power = 0.75,\n",
    "        batch_size = 32,\n",
    "        clip_gradients = 5.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        moving_avgs = tf.train.ExponentialMovingAverage(0.9)\n",
    "        self.batch_size = batch_size\n",
    "        self.freqs = freqs\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.X = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.Y = tf.placeholder(tf.int64, shape = [None])\n",
    "        self.DOC = tf.placeholder(tf.int32, shape = [None])\n",
    "        step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "        self.switch_loss = tf.Variable(0, trainable = False)\n",
    "        train_labels = tf.reshape(self.Y, [-1, 1])\n",
    "        sampler = tf.nn.fixed_unigram_candidate_sampler(\n",
    "            train_labels,\n",
    "            num_true = 1,\n",
    "            num_sampled = num_sampled,\n",
    "            unique = True,\n",
    "            range_max = vocab_size,\n",
    "            distortion = power,\n",
    "            unigrams = self.freqs,\n",
    "        )\n",
    "\n",
    "        self.word_embedding = tf.Variable(\n",
    "            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)\n",
    "        )\n",
    "        self.nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [vocab_size, embedding_size],\n",
    "                stddev = tf.sqrt(1 / embedding_size),\n",
    "            )\n",
    "        )\n",
    "        self.nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "        scalar = 1 / np.sqrt(num_unique_documents + num_topics)\n",
    "        self.doc_embedding = tf.Variable(\n",
    "            tf.random_normal(\n",
    "                [num_unique_documents, num_topics],\n",
    "                mean = 0,\n",
    "                stddev = 50 * scalar,\n",
    "            )\n",
    "        )\n",
    "        self.topic_embedding = tf.get_variable(\n",
    "            'topic_embedding',\n",
    "            shape = [num_topics, embedding_size],\n",
    "            dtype = tf.float32,\n",
    "            initializer = tf.orthogonal_initializer(gain = scalar),\n",
    "        )\n",
    "        pivot = tf.nn.embedding_lookup(self.word_embedding, self.X)\n",
    "        proportions = tf.nn.embedding_lookup(self.doc_embedding, self.DOC)\n",
    "        doc = tf.matmul(proportions, self.topic_embedding)\n",
    "        doc_context = doc\n",
    "        word_context = pivot\n",
    "        context = tf.add(word_context, doc_context)\n",
    "        loss_word2vec = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights = self.nce_weights,\n",
    "                biases = self.nce_biases,\n",
    "                labels = self.Y,\n",
    "                inputs = context,\n",
    "                num_sampled = num_sampled,\n",
    "                num_classes = vocab_size,\n",
    "                num_true = 1,\n",
    "                sampled_values = sampler,\n",
    "            )\n",
    "        )\n",
    "        self.fraction = tf.Variable(1, trainable = False, dtype = tf.float32)\n",
    "\n",
    "        n_topics = self.doc_embedding.get_shape()[1].value\n",
    "        log_proportions = tf.nn.log_softmax(self.doc_embedding)\n",
    "        if alpha is None:\n",
    "            alpha = 1.0 / n_topics\n",
    "        loss = -(alpha - 1) * log_proportions\n",
    "        prior = tf.reduce_sum(loss)\n",
    "\n",
    "        loss_lda = lmbda * self.fraction * prior\n",
    "        self.cost = tf.cond(\n",
    "            step < self.switch_loss,\n",
    "            lambda: loss_word2vec,\n",
    "            lambda: loss_word2vec + loss_lda,\n",
    "        )\n",
    "        loss_avgs_op = moving_avgs.apply([loss_lda, loss_word2vec, self.cost])\n",
    "        with tf.control_dependencies([loss_avgs_op]):\n",
    "            self.optimizer = tf.contrib.layers.optimize_loss(\n",
    "                self.cost,\n",
    "                tf.train.get_global_step(),\n",
    "                learning_rate,\n",
    "                'Adam',\n",
    "                clip_gradients = clip_gradients,\n",
    "            )\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(\n",
    "        self, pivot_words, target_words, doc_ids, num_epochs, switch_loss = 3\n",
    "    ):\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        temp_fraction = self.batch_size / len(pivot_words)\n",
    "        self.sess.run(tf.assign(self.fraction, temp_fraction))\n",
    "        self.sess.run(tf.assign(self.switch_loss, switch_loss))\n",
    "        for e in range(num_epochs):\n",
    "            pbar = tqdm(\n",
    "                range(0, len(pivot_words), self.batch_size),\n",
    "                desc = 'minibatch loop',\n",
    "            )\n",
    "            for i in pbar:\n",
    "                batch_x = pivot_words[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                batch_y = target_words[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                batch_doc = doc_ids[\n",
    "                    i : min(i + self.batch_size, len(pivot_words))\n",
    "                ]\n",
    "                _, cost = self.sess.run(\n",
    "                    [self.optimizer, self.cost],\n",
    "                    feed_dict = {\n",
    "                        self.X: batch_x,\n",
    "                        self.Y: batch_y,\n",
    "                        self.DOC: batch_doc,\n",
    "                    },\n",
    "                )\n",
    "                pbar.set_postfix(cost = cost, epoch = e + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def skipgrams(\n",
    "    sequence,\n",
    "    vocabulary_size,\n",
    "    window_size = 4,\n",
    "    negative_samples = 1.0,\n",
    "    shuffle = True,\n",
    "    categorical = False,\n",
    "    sampling_table = None,\n",
    "    seed = None,\n",
    "):\n",
    "    couples = []\n",
    "    labels = []\n",
    "    for i, wi in enumerate(sequence):\n",
    "        if not wi:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        for j in range(window_start, window_end):\n",
    "            if j != i:\n",
    "                wj = sequence[j]\n",
    "                if not wj:\n",
    "                    continue\n",
    "                couples.append([wi, wj])\n",
    "                if categorical:\n",
    "                    labels.append([0, 1])\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    if negative_samples > 0:\n",
    "        num_negative_samples = int(len(labels) * negative_samples)\n",
    "        words = [c[0] for c in couples]\n",
    "        random.shuffle(words)\n",
    "\n",
    "        couples += [\n",
    "            [words[i % len(words)], random.randint(1, vocabulary_size - 1)]\n",
    "            for i in range(num_negative_samples)\n",
    "        ]\n",
    "        if categorical:\n",
    "            labels += [[1, 0]] * num_negative_samples\n",
    "        else:\n",
    "            labels += [0] * num_negative_samples\n",
    "\n",
    "    if shuffle:\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 10e6)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(couples)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(labels)\n",
    "\n",
    "    return couples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "bow = CountVectorizer().fit(sentences)\n",
    "transformed = bow.transform(sentences)\n",
    "idx_text_clean, len_idx_text_clean = [], []\n",
    "for text in transformed:\n",
    "    splitted = text.nonzero()[1]\n",
    "    idx_text_clean.append(splitted)\n",
    "    \n",
    "dictionary = {\n",
    "        i: no for no, i in enumerate(bow.get_feature_names())\n",
    "    }\n",
    "reversed_dictionary = {\n",
    "        no: i for no, i in enumerate(bow.get_feature_names())\n",
    "    }\n",
    "freqs = transformed.toarray().sum(axis = 0).tolist()\n",
    "doc_ids = np.arange(len(idx_text_clean))\n",
    "num_unique_documents = doc_ids.max()\n",
    "pivot_words, target_words, doc_ids = [], [], []\n",
    "for i, t in enumerate(idx_text_clean):\n",
    "    pairs, _ = skipgrams(\n",
    "            t,\n",
    "            vocabulary_size = len(dictionary),\n",
    "            window_size = window_size,\n",
    "            shuffle = True,\n",
    "            negative_samples = 0,\n",
    "        )\n",
    "    for pair in pairs:\n",
    "        temp_data = pair\n",
    "        pivot_words.append(temp_data[0])\n",
    "        target_words.append(temp_data[1])\n",
    "        doc_ids.append(i)\n",
    "pivot_words, target_words, doc_ids = shuffle(\n",
    "        pivot_words, target_words, doc_ids, random_state = 10\n",
    ")\n",
    "num_unique_documents = len(idx_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/huseinzol05/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/huseinzol05/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = LDA2VEC(\n",
    "        num_unique_documents,\n",
    "        len(dictionary),\n",
    "        n_topics,\n",
    "        freqs,\n",
    "        embedding_size = embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 45372/45372 [08:43<00:00, 86.60it/s, cost=-2.21e+4, epoch=1]\n",
      "minibatch loop: 100%|██████████| 45372/45372 [08:44<00:00, 86.50it/s, cost=-4.71e+4, epoch=2]\n",
      "minibatch loop: 100%|██████████| 45372/45372 [08:44<00:00, 86.50it/s, cost=-7.2e+4, epoch=3] \n",
      "minibatch loop: 100%|██████████| 45372/45372 [08:44<00:00, 86.44it/s, cost=-9.62e+4, epoch=4]\n",
      "minibatch loop: 100%|██████████| 45372/45372 [08:44<00:00, 86.45it/s, cost=-1.19e+5, epoch=5]\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    pivot_words, target_words, doc_ids, epoch, switch_loss = switch_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embed = model.sess.run(model.doc_embedding)\n",
    "topic_embed = model.sess.run(model.topic_embedding)\n",
    "word_embed = model.sess.run(model.word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1 : g25 hardiknas doktor kashif kompetensi keep banjir harvest ditargetkan\n",
      "topic 2 : g25 izzah kashif ioi halilintar harvest 1984 keep candreva\n",
      "topic 3 : g25 hardiknas 1984 hamisah 2001 kashif doktor keep halilintar\n",
      "topic 4 : g25 hardiknas keep kashif doktor alfamart lombok diimport washing\n",
      "topic 5 : keep g25 harvest halilintar menghiraukan 1984 administrative kejahatan marketplace\n",
      "topic 6 : keep ioi kompetensi washing kashif g25 dominan halilintar asuhan\n",
      "topic 7 : g25 kashif harvest keep kritis diimport chow escas berbau\n",
      "topic 8 : keep citi mulai 1984 escas g25 doktor garis asuhan\n",
      "topic 9 : g25 asuhan 2001 gapoktan doktor halilintar umt kashif harmonis\n",
      "topic 10 : g25 keep harvest doktor kashif tribunkaltim administrative asuhan halilintar\n"
     ]
    }
   ],
   "source": [
    "components = topic_embed.dot(word_embed.T)\n",
    "for no, topic in enumerate(components):\n",
    "    topic_string = ' '.join([reversed_dictionary[i]\n",
    "              for i in topic.argsort()[: -10 : -1]])\n",
    "    print('topic %d : %s'%(no + 1, topic_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
