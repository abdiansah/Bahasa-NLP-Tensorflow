{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import itertools\n",
    "from unidecode import unidecode\n",
    "import malaya\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=2):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words - 10)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen]):\n",
    "            X[i, no] = dic.get(k, UNK)\n",
    "    return X\n",
    "\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 2]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-similarity.json') as fopen:\n",
    "    train = json.load(fopen)\n",
    "    \n",
    "left, right, label = train['left'], train['right'], train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-similarity.json') as fopen:\n",
    "    test = json.load(fopen)\n",
    "test_left, test_right, test_label = test['left'], test['right'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2605321, 1531070]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('similarity-dictionary.json') as fopen:\n",
    "    x = json.load(fopen)\n",
    "dictionary = x['dictionary']\n",
    "rev_dictionary = x['reverse_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(inputs):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    repr_dim = inputs.get_shape()[-1].value\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x =  tf.layers.conv1d(inputs = tf.concat([pad, x, pad], 1),\n",
    "                          filters = hidden_dim,\n",
    "                          kernel_size = kernel_size,\n",
    "                          dilation_rate = dilation_rate)\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, learning_rate, dropout, kernel_size = 5):\n",
    "        \n",
    "        def cnn(x, scope):\n",
    "            x += position_encoding(x)\n",
    "            with tf.variable_scope(scope, reuse = tf.AUTO_REUSE):\n",
    "                for n in range(num_layers):\n",
    "                    dilation_rate = 2 ** n\n",
    "                    pad_sz = (kernel_size - 1) * dilation_rate \n",
    "                    with tf.variable_scope('block_%d'%n,reuse=tf.AUTO_REUSE):\n",
    "                        x += cnn_block(x, dilation_rate, pad_sz, size_layer, kernel_size)\n",
    "                \n",
    "                with tf.variable_scope('logits', reuse=tf.AUTO_REUSE):\n",
    "                    return tf.layers.dense(x, size_layer)[:, -1]\n",
    "        \n",
    "        self.X_left = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_right = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None])\n",
    "        self.batch_size = tf.shape(self.X_left)[0]\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        embedded_left = tf.nn.embedding_lookup(encoder_embeddings, self.X_left)\n",
    "        embedded_right = tf.nn.embedding_lookup(encoder_embeddings, self.X_right)\n",
    "        \n",
    "        def contrastive_loss(y,d):\n",
    "            tmp= y * tf.square(d)\n",
    "            tmp2 = (1-y) * tf.square(tf.maximum((1 - d),0))\n",
    "            return tf.reduce_sum(tmp +tmp2)/tf.cast(self.batch_size,tf.float32)/2\n",
    "        \n",
    "        self.output_left = cnn(embedded_left, 'left')\n",
    "        self.output_right = cnn(embedded_right, 'right')\n",
    "        print(self.output_left, self.output_right)\n",
    "        self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.output_left,self.output_right)),\n",
    "                                              1,keep_dims=True))\n",
    "        self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.output_left),\n",
    "                                                                           1,keep_dims=True)),\n",
    "                                                     tf.sqrt(tf.reduce_sum(tf.square(self.output_right),\n",
    "                                                                           1,keep_dims=True))))\n",
    "        self.distance = tf.reshape(self.distance, [-1])\n",
    "        self.logits = tf.identity(self.distance, name = 'logits')\n",
    "        self.cost = contrastive_loss(self.Y,self.distance)\n",
    "        \n",
    "        self.temp_sim = tf.subtract(tf.ones_like(self.distance),\n",
    "                                    tf.rint(self.distance))\n",
    "        correct_predictions = tf.equal(self.temp_sim, self.Y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 4\n",
    "embedded_size = 128\n",
    "learning_rate = 1e-3\n",
    "maxlen = 50\n",
    "batch_size = 128\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"left/logits/strided_slice:0\", shape=(?, 128), dtype=float32) Tensor(\"right/logits/strided_slice:0\", shape=(?, 128), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-7-aea6b3dea261>:62: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dilated-cnn/model.ckpt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer,num_layers,embedded_size,len(dictionary),learning_rate,dropout)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'dilated-cnn/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and '_power' not in n.name\n",
    "        and 'gradient' not in n.name\n",
    "        and 'Initializer' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [39:10<00:00, 13.75it/s, accuracy=0.549, cost=0.119] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 44.01it/s, accuracy=0.725, cost=0.0879]\n",
      "train minibatch loop:   0%|          | 2/32316 [00:00<39:09, 13.76it/s, accuracy=0.852, cost=0.0575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.754236\n",
      "time taken: 2359.090886592865\n",
      "epoch: 0, training loss: 0.088377, training acc: 0.736193, valid loss: 0.083123, valid acc: 0.754236\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [39:10<00:00, 14.49it/s, accuracy=0.577, cost=0.0901] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 45.23it/s, accuracy=0.712, cost=0.0807]\n",
      "train minibatch loop:   0%|          | 2/32316 [00:00<38:55, 13.84it/s, accuracy=0.898, cost=0.0552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.754236, current acc: 0.770444\n",
      "time taken: 2359.3658940792084\n",
      "epoch: 0, training loss: 0.075040, training acc: 0.782331, valid loss: 0.078482, valid acc: 0.770444\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [39:12<00:00, 14.52it/s, accuracy=0.775, cost=0.0608] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 45.24it/s, accuracy=0.725, cost=0.0816]\n",
      "train minibatch loop:   0%|          | 2/32316 [00:00<39:00, 13.81it/s, accuracy=0.906, cost=0.0465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.770444, current acc: 0.773316\n",
      "time taken: 2360.77591919899\n",
      "epoch: 0, training loss: 0.065331, training acc: 0.815129, valid loss: 0.078364, valid acc: 0.773316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [39:12<00:00, 14.52it/s, accuracy=0.831, cost=0.0589] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 45.04it/s, accuracy=0.712, cost=0.088] \n",
      "train minibatch loop:   0%|          | 2/32316 [00:00<39:09, 13.75it/s, accuracy=0.945, cost=0.0312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 2361.537866592407\n",
      "epoch: 0, training loss: 0.055691, training acc: 0.847078, valid loss: 0.078892, valid acc: 0.772364\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [39:10<00:00, 14.47it/s, accuracy=0.901, cost=0.0346] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 45.22it/s, accuracy=0.725, cost=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 2359.5909333229065\n",
      "epoch: 0, training loss: 0.046877, training acc: 0.875735, valid loss: 0.080502, valid acc: 0.771336\n",
      "\n",
      "break epoch:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(range(0, len(left), batch_size), desc='train minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i+batch_size,len(left))\n",
    "        batch_x_left = str_idx(left[i: index], dictionary, maxlen)\n",
    "        batch_x_right = str_idx(right[i: index], dictionary, maxlen)\n",
    "        batch_y = label[i:index]\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        assert not np.isnan(loss)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    pbar = tqdm(range(0, len(test_left), batch_size), desc='test minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i+batch_size,len(test_left))\n",
    "        batch_x_left = str_idx(test_left[i: index], dictionary, maxlen)\n",
    "        batch_x_right = str_idx(test_right[i: index], dictionary, maxlen)\n",
    "        batch_y = test_label[i: index]\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    train_loss /= (len(left) / batch_size)\n",
    "    train_acc /= (len(left) / batch_size)\n",
    "    test_loss /= (len(test_left) / batch_size)\n",
    "    test_acc /= (len(test_left) / batch_size)\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    \n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
    "                                                                                          train_acc,test_loss,\n",
    "                                                                                          test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dilated-cnn/model.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'dilated-cnn/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.], dtype=float32), array([0.0343591], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = str_idx(['a person is outdoors, on a horse.'], dictionary, maxlen)\n",
    "right = str_idx(['a person on a horse jumps over a broken down airplane.'], dictionary, maxlen)\n",
    "sess.run([model.temp_sim,1-model.distance], feed_dict = {model.X_left : left, \n",
    "                                        model.X_right: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 391/391 [00:08<00:00, 47.37it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_left), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    index = min(i+batch_size,len(test_left))\n",
    "    batch_x_left = str_idx(test_left[i: index], dictionary, maxlen)\n",
    "    batch_x_right = str_idx(test_right[i: index], dictionary, maxlen)\n",
    "    batch_y = test_label[i: index]\n",
    "    predict_Y += sess.run(model.temp_sim, feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y}).tolist()\n",
    "    real_Y += batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "not similar       0.82      0.82      0.82     31524\n",
      "    similar       0.69      0.69      0.69     18476\n",
      "\n",
      "avg / total       0.77      0.77      0.77     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        real_Y, predict_Y, target_names = ['not similar', 'similar']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Variable',\n",
       " 'left/block_0/gamma',\n",
       " 'left/block_0/beta',\n",
       " 'left/block_0/conv1d/kernel',\n",
       " 'left/block_0/conv1d/bias',\n",
       " 'left/block_1/gamma',\n",
       " 'left/block_1/beta',\n",
       " 'left/block_1/conv1d/kernel',\n",
       " 'left/block_1/conv1d/bias',\n",
       " 'left/block_2/gamma',\n",
       " 'left/block_2/beta',\n",
       " 'left/block_2/conv1d/kernel',\n",
       " 'left/block_2/conv1d/bias',\n",
       " 'left/block_3/gamma',\n",
       " 'left/block_3/beta',\n",
       " 'left/block_3/conv1d/kernel',\n",
       " 'left/block_3/conv1d/bias',\n",
       " 'left/logits/dense/kernel',\n",
       " 'left/logits/dense/kernel/read',\n",
       " 'left/logits/dense/bias',\n",
       " 'left/logits/dense/bias/read',\n",
       " 'left/logits/dense/Tensordot/Shape',\n",
       " 'left/logits/dense/Tensordot/Rank',\n",
       " 'left/logits/dense/Tensordot/axes',\n",
       " 'left/logits/dense/Tensordot/GreaterEqual/y',\n",
       " 'left/logits/dense/Tensordot/GreaterEqual',\n",
       " 'left/logits/dense/Tensordot/Cast',\n",
       " 'left/logits/dense/Tensordot/mul',\n",
       " 'left/logits/dense/Tensordot/Less/y',\n",
       " 'left/logits/dense/Tensordot/Less',\n",
       " 'left/logits/dense/Tensordot/Cast_1',\n",
       " 'left/logits/dense/Tensordot/add',\n",
       " 'left/logits/dense/Tensordot/mul_1',\n",
       " 'left/logits/dense/Tensordot/add_1',\n",
       " 'left/logits/dense/Tensordot/range/start',\n",
       " 'left/logits/dense/Tensordot/range/delta',\n",
       " 'left/logits/dense/Tensordot/range',\n",
       " 'left/logits/dense/Tensordot/ListDiff',\n",
       " 'left/logits/dense/Tensordot/GatherV2/axis',\n",
       " 'left/logits/dense/Tensordot/GatherV2',\n",
       " 'left/logits/dense/Tensordot/GatherV2_1/axis',\n",
       " 'left/logits/dense/Tensordot/GatherV2_1',\n",
       " 'left/logits/dense/Tensordot/Const',\n",
       " 'left/logits/dense/Tensordot/Prod',\n",
       " 'left/logits/dense/Tensordot/Const_1',\n",
       " 'left/logits/dense/Tensordot/Prod_1',\n",
       " 'left/logits/dense/Tensordot/concat/axis',\n",
       " 'left/logits/dense/Tensordot/concat',\n",
       " 'left/logits/dense/Tensordot/concat_1/axis',\n",
       " 'left/logits/dense/Tensordot/concat_1',\n",
       " 'left/logits/dense/Tensordot/stack',\n",
       " 'left/logits/dense/Tensordot/transpose',\n",
       " 'left/logits/dense/Tensordot/Reshape',\n",
       " 'left/logits/dense/Tensordot/transpose_1/perm',\n",
       " 'left/logits/dense/Tensordot/transpose_1',\n",
       " 'left/logits/dense/Tensordot/Reshape_1/shape',\n",
       " 'left/logits/dense/Tensordot/Reshape_1',\n",
       " 'left/logits/dense/Tensordot/MatMul',\n",
       " 'left/logits/dense/Tensordot/Const_2',\n",
       " 'left/logits/dense/Tensordot/concat_2/axis',\n",
       " 'left/logits/dense/Tensordot/concat_2',\n",
       " 'left/logits/dense/Tensordot',\n",
       " 'left/logits/dense/BiasAdd',\n",
       " 'left/logits/strided_slice/stack',\n",
       " 'left/logits/strided_slice/stack_1',\n",
       " 'left/logits/strided_slice/stack_2',\n",
       " 'left/logits/strided_slice',\n",
       " 'right/block_0/gamma',\n",
       " 'right/block_0/beta',\n",
       " 'right/block_0/conv1d/kernel',\n",
       " 'right/block_0/conv1d/bias',\n",
       " 'right/block_1/gamma',\n",
       " 'right/block_1/beta',\n",
       " 'right/block_1/conv1d/kernel',\n",
       " 'right/block_1/conv1d/bias',\n",
       " 'right/block_2/gamma',\n",
       " 'right/block_2/beta',\n",
       " 'right/block_2/conv1d/kernel',\n",
       " 'right/block_2/conv1d/bias',\n",
       " 'right/block_3/gamma',\n",
       " 'right/block_3/beta',\n",
       " 'right/block_3/conv1d/kernel',\n",
       " 'right/block_3/conv1d/bias',\n",
       " 'right/logits/dense/kernel',\n",
       " 'right/logits/dense/kernel/read',\n",
       " 'right/logits/dense/bias',\n",
       " 'right/logits/dense/bias/read',\n",
       " 'right/logits/dense/Tensordot/Shape',\n",
       " 'right/logits/dense/Tensordot/Rank',\n",
       " 'right/logits/dense/Tensordot/axes',\n",
       " 'right/logits/dense/Tensordot/GreaterEqual/y',\n",
       " 'right/logits/dense/Tensordot/GreaterEqual',\n",
       " 'right/logits/dense/Tensordot/Cast',\n",
       " 'right/logits/dense/Tensordot/mul',\n",
       " 'right/logits/dense/Tensordot/Less/y',\n",
       " 'right/logits/dense/Tensordot/Less',\n",
       " 'right/logits/dense/Tensordot/Cast_1',\n",
       " 'right/logits/dense/Tensordot/add',\n",
       " 'right/logits/dense/Tensordot/mul_1',\n",
       " 'right/logits/dense/Tensordot/add_1',\n",
       " 'right/logits/dense/Tensordot/range/start',\n",
       " 'right/logits/dense/Tensordot/range/delta',\n",
       " 'right/logits/dense/Tensordot/range',\n",
       " 'right/logits/dense/Tensordot/ListDiff',\n",
       " 'right/logits/dense/Tensordot/GatherV2/axis',\n",
       " 'right/logits/dense/Tensordot/GatherV2',\n",
       " 'right/logits/dense/Tensordot/GatherV2_1/axis',\n",
       " 'right/logits/dense/Tensordot/GatherV2_1',\n",
       " 'right/logits/dense/Tensordot/Const',\n",
       " 'right/logits/dense/Tensordot/Prod',\n",
       " 'right/logits/dense/Tensordot/Const_1',\n",
       " 'right/logits/dense/Tensordot/Prod_1',\n",
       " 'right/logits/dense/Tensordot/concat/axis',\n",
       " 'right/logits/dense/Tensordot/concat',\n",
       " 'right/logits/dense/Tensordot/concat_1/axis',\n",
       " 'right/logits/dense/Tensordot/concat_1',\n",
       " 'right/logits/dense/Tensordot/stack',\n",
       " 'right/logits/dense/Tensordot/transpose',\n",
       " 'right/logits/dense/Tensordot/Reshape',\n",
       " 'right/logits/dense/Tensordot/transpose_1/perm',\n",
       " 'right/logits/dense/Tensordot/transpose_1',\n",
       " 'right/logits/dense/Tensordot/Reshape_1/shape',\n",
       " 'right/logits/dense/Tensordot/Reshape_1',\n",
       " 'right/logits/dense/Tensordot/MatMul',\n",
       " 'right/logits/dense/Tensordot/Const_2',\n",
       " 'right/logits/dense/Tensordot/concat_2/axis',\n",
       " 'right/logits/dense/Tensordot/concat_2',\n",
       " 'right/logits/dense/Tensordot',\n",
       " 'right/logits/dense/BiasAdd',\n",
       " 'right/logits/strided_slice/stack',\n",
       " 'right/logits/strided_slice/stack_1',\n",
       " 'right/logits/strided_slice/stack_2',\n",
       " 'right/logits/strided_slice',\n",
       " 'logits']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from dilated-cnn/model.ckpt\n",
      "INFO:tensorflow:Froze 37 variables.\n",
      "INFO:tensorflow:Converted 37 variables to const ops.\n",
      "875 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('dilated-cnn', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0343591], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = load_graph('dilated-cnn/frozen_model.pb')\n",
    "x1 = g.get_tensor_by_name('import/Placeholder:0')\n",
    "x2 = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "test_sess.run(1-logits, feed_dict = {x1 : left, x2: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21152252, 0.14559478, 0.20776057, 0.70417494, 0.49244803,\n",
       "       0.33945912, 0.9202117 , 0.02324635, 0.12748677, 0.8314166 ,\n",
       "       0.975024  , 0.62006444, 0.18129557, 0.6427861 , 0.07265455,\n",
       "       0.4061333 , 0.18890274, 0.02502632, 0.0484429 , 0.10148406,\n",
       "       0.8321909 , 0.05768776, 0.55261767, 0.6817114 , 0.11403704,\n",
       "       0.44246477, 0.4924479 , 0.18728226, 0.07191038, 0.05914503,\n",
       "       0.0800122 , 0.3046261 , 0.60251844, 0.761145  , 0.95517516,\n",
       "       0.88605934, 0.814803  , 0.07416344, 0.06447667, 0.03957129,\n",
       "       0.03240418, 0.75431895, 0.6757686 , 0.76394105, 0.9388763 ,\n",
       "       0.24763906, 0.98832715, 0.05210805, 0.02429408, 0.12788087,\n",
       "       0.1121434 , 0.8168456 , 0.9283892 , 0.5351901 , 0.01739019,\n",
       "       0.9779401 , 0.02959573, 0.07608068, 0.16026843, 0.07550842,\n",
       "       0.6336924 , 0.23004955, 0.8670918 , 0.68216723, 0.06849951,\n",
       "       0.02407455, 0.01773602, 0.88574535, 0.06930637, 0.01752573,\n",
       "       0.02795351, 0.5855931 , 0.1376006 , 0.958021  , 0.00917709,\n",
       "       0.01847631, 0.8541901 , 0.5811751 , 0.02121222, 0.07525933],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sess.run(1-logits, feed_dict = {x1 : batch_x_left, x2: batch_x_right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
