{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import itertools\n",
    "from unidecode import unidecode\n",
    "import malaya\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=2):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words - 10)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen]):\n",
    "            X[i, no] = dic.get(k, UNK)\n",
    "    return X\n",
    "\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 2]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-similarity.json') as fopen:\n",
    "    train = json.load(fopen)\n",
    "    \n",
    "left, right, label = train['left'], train['right'], train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-similarity.json') as fopen:\n",
    "    test = json.load(fopen)\n",
    "test_left, test_right, test_label = test['left'], test['right'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2605321, 1531070]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('similarity-dictionary.json') as fopen:\n",
    "    x = json.load(fopen)\n",
    "dictionary = x['dictionary']\n",
    "rev_dictionary = x['reverse_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(inputs):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    repr_dim = inputs.get_shape()[-1].value\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def self_attention(inputs, is_training, num_units, num_heads = 8, activation=None):\n",
    "    T_q = T_k = tf.shape(inputs)[1]\n",
    "    Q_K_V = tf.layers.dense(inputs, 3*num_units, activation)\n",
    "    Q, K, V = tf.split(Q_K_V, 3, -1)\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), 0)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), 0)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), 0)\n",
    "    align = tf.matmul(Q_, K_, transpose_b=True)\n",
    "    align *= tf.rsqrt(tf.to_float(K_.get_shape()[-1].value))\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))\n",
    "    lower_tri = tf.ones([T_q, T_k])\n",
    "    lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()\n",
    "    masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0],1,1])\n",
    "    align = tf.where(tf.equal(masks, 0), paddings, align)\n",
    "    align = tf.nn.softmax(align)\n",
    "    align = tf.layers.dropout(align, 0.1, training=is_training) \n",
    "    x = tf.matmul(align, V_)\n",
    "    x = tf.concat(tf.split(x, num_heads, axis=0), 2)\n",
    "    x += inputs\n",
    "    x = layer_norm(x)\n",
    "    return x\n",
    "\n",
    "def ffn(inputs, hidden_dim, activation=tf.nn.relu):\n",
    "    x = tf.layers.conv1d(inputs, 4* hidden_dim, 1, activation=activation) \n",
    "    x = tf.layers.conv1d(x, hidden_dim, 1, activation=None)\n",
    "    x += inputs\n",
    "    x = layer_norm(x)\n",
    "    return x\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, learning_rate, dropout, kernel_size = 5):\n",
    "        \n",
    "        def cnn(x, scope):\n",
    "            x += position_encoding(x)\n",
    "            with tf.variable_scope(scope, reuse = tf.AUTO_REUSE):\n",
    "                for n in range(num_layers):\n",
    "                    with tf.variable_scope('attn_%d'%n,reuse=tf.AUTO_REUSE):\n",
    "                        x = self_attention(x, True, size_layer)\n",
    "                    with tf.variable_scope('ffn_%d'%n, reuse=tf.AUTO_REUSE):\n",
    "                        x = ffn(x, size_layer)\n",
    "                \n",
    "                with tf.variable_scope('logits', reuse=tf.AUTO_REUSE):\n",
    "                    return tf.layers.dense(x, size_layer)[:, -1]\n",
    "        \n",
    "        self.X_left = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_right = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None])\n",
    "        self.batch_size = tf.shape(self.X_left)[0]\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        embedded_left = tf.nn.embedding_lookup(encoder_embeddings, self.X_left)\n",
    "        embedded_right = tf.nn.embedding_lookup(encoder_embeddings, self.X_right)\n",
    "        \n",
    "        def contrastive_loss(y,d):\n",
    "            tmp= y * tf.square(d)\n",
    "            tmp2 = (1-y) * tf.square(tf.maximum((1 - d),0))\n",
    "            return tf.reduce_sum(tmp +tmp2)/tf.cast(self.batch_size,tf.float32)/2\n",
    "        \n",
    "        self.output_left = cnn(embedded_left, 'left')\n",
    "        self.output_right = cnn(embedded_right, 'right')\n",
    "        print(self.output_left, self.output_right)\n",
    "        self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.output_left,self.output_right)),\n",
    "                                              1,keep_dims=True))\n",
    "        self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.output_left),\n",
    "                                                                           1,keep_dims=True)),\n",
    "                                                     tf.sqrt(tf.reduce_sum(tf.square(self.output_right),\n",
    "                                                                           1,keep_dims=True))))\n",
    "        self.distance = tf.reshape(self.distance, [-1])\n",
    "        self.logits = tf.identity(self.distance, name = 'logits')\n",
    "        self.cost = contrastive_loss(self.Y,self.distance)\n",
    "        \n",
    "        self.temp_sim = tf.subtract(tf.ones_like(self.distance),\n",
    "                                    tf.rint(self.distance))\n",
    "        correct_predictions = tf.equal(self.temp_sim, self.Y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 4\n",
    "embedded_size = 128\n",
    "learning_rate = 1e-4\n",
    "maxlen = 50\n",
    "batch_size = 128\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"left/logits/strided_slice:0\", shape=(?, 128), dtype=float32) Tensor(\"right/logits/strided_slice:0\", shape=(?, 128), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-7-7a5aa06b3da9>:80: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'self-attention/model.ckpt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer,num_layers,embedded_size,len(dictionary),learning_rate,dropout)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'self-attention/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and '_power' not in n.name\n",
    "        and 'gradient' not in n.name\n",
    "        and 'Initializer' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:08<00:00,  5.65it/s, accuracy=0.549, cost=0.123] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.98it/s, accuracy=0.725, cost=0.089] \n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s, accuracy=0.727, cost=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.745696\n",
      "time taken: 6518.2529237270355\n",
      "epoch: 0, training loss: 0.091813, training acc: 0.719950, valid loss: 0.085040, valid acc: 0.745696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:08<00:00,  5.66it/s, accuracy=0.662, cost=0.108]  \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.18it/s, accuracy=0.688, cost=0.0854]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s, accuracy=0.648, cost=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.745696, current acc: 0.762640\n",
      "time taken: 6518.055644273758\n",
      "epoch: 0, training loss: 0.079587, training acc: 0.766882, valid loss: 0.080575, valid acc: 0.762640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:  76%|███████▌  | 24603/32316 [1:22:18<25:46,  4.99it/s, accuracy=0.852, cost=0.0366]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:07<00:00,  5.66it/s, accuracy=0.69, cost=0.0892] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.19it/s, accuracy=0.75, cost=0.077]  \n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.762640, current acc: 0.768700\n",
      "time taken: 6516.856334209442\n",
      "epoch: 0, training loss: 0.073044, training acc: 0.790378, valid loss: 0.079073, valid acc: 0.768700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:06<00:00,  5.66it/s, accuracy=0.761, cost=0.067]  \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.23it/s, accuracy=0.762, cost=0.0771]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s, accuracy=0.672, cost=0.0856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.768700, current acc: 0.769312\n",
      "time taken: 6516.553435564041\n",
      "epoch: 0, training loss: 0.067276, training acc: 0.811340, valid loss: 0.079457, valid acc: 0.769312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:07<00:00,  5.66it/s, accuracy=0.887, cost=0.039]  \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.18it/s, accuracy=0.788, cost=0.0811]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s, accuracy=0.703, cost=0.0815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.769312, current acc: 0.771576\n",
      "time taken: 6516.8461327552795\n",
      "epoch: 0, training loss: 0.061893, training acc: 0.830197, valid loss: 0.079747, valid acc: 0.771576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:07<00:00,  5.66it/s, accuracy=0.972, cost=0.0323] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.17it/s, accuracy=0.775, cost=0.0762]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s, accuracy=0.758, cost=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.771576, current acc: 0.773124\n",
      "time taken: 6517.283529996872\n",
      "epoch: 0, training loss: 0.056825, training acc: 0.846921, valid loss: 0.080187, valid acc: 0.773124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:07<00:00,  5.66it/s, accuracy=1, cost=0.0204]     \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.19it/s, accuracy=0.762, cost=0.0719]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6516.9770267009735\n",
      "epoch: 0, training loss: 0.052224, training acc: 0.861832, valid loss: 0.081519, valid acc: 0.770692\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:48:06<00:00,  5.66it/s, accuracy=1, cost=0.0112]     \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 13.18it/s, accuracy=0.75, cost=0.0798] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6516.600977897644\n",
      "epoch: 0, training loss: 0.048151, training acc: 0.874642, valid loss: 0.081898, valid acc: 0.772720\n",
      "\n",
      "break epoch:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(range(0, len(left), batch_size), desc='train minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i+batch_size,len(left))\n",
    "        batch_x_left = str_idx(left[i: index], dictionary, maxlen)\n",
    "        batch_x_right = str_idx(right[i: index], dictionary, maxlen)\n",
    "        batch_y = label[i:index]\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        assert not np.isnan(loss)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    pbar = tqdm(range(0, len(test_left), batch_size), desc='test minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i+batch_size,len(test_left))\n",
    "        batch_x_left = str_idx(test_left[i: index], dictionary, maxlen)\n",
    "        batch_x_right = str_idx(test_right[i: index], dictionary, maxlen)\n",
    "        batch_y = test_label[i: index]\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    train_loss /= (len(left) / batch_size)\n",
    "    train_acc /= (len(left) / batch_size)\n",
    "    test_loss /= (len(test_left) / batch_size)\n",
    "    test_acc /= (len(test_left) / batch_size)\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    \n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
    "                                                                                          train_acc,test_loss,\n",
    "                                                                                          test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self-attention/model.ckpt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'self-attention/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.], dtype=float32), array([0.02327037], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = str_idx(['a person is outdoors, on a horse.'], dictionary, maxlen)\n",
    "right = str_idx(['a person on a horse jumps over a broken down airplane.'], dictionary, maxlen)\n",
    "sess.run([model.temp_sim,1-model.distance], feed_dict = {model.X_left : left, \n",
    "                                        model.X_right: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 391/391 [00:29<00:00, 14.17it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_left), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    index = min(i+batch_size,len(test_left))\n",
    "    batch_x_left = str_idx(test_left[i: index], dictionary, maxlen)\n",
    "    batch_x_right = str_idx(test_right[i: index], dictionary, maxlen)\n",
    "    batch_y = test_label[i: index]\n",
    "    predict_Y += sess.run(model.temp_sim, feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y}).tolist()\n",
    "    real_Y += batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "not similar       0.81      0.83      0.82     31524\n",
      "    similar       0.70      0.67      0.68     18476\n",
      "\n",
      "avg / total       0.77      0.77      0.77     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        real_Y, predict_Y, target_names = ['not similar', 'similar']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Variable',\n",
       " 'left/attn_0/dense/kernel',\n",
       " 'left/attn_0/dense/bias',\n",
       " 'left/attn_0/gamma',\n",
       " 'left/attn_0/beta',\n",
       " 'left/ffn_0/conv1d/kernel',\n",
       " 'left/ffn_0/conv1d/bias',\n",
       " 'left/ffn_0/conv1d_1/kernel',\n",
       " 'left/ffn_0/conv1d_1/bias',\n",
       " 'left/ffn_0/gamma',\n",
       " 'left/ffn_0/beta',\n",
       " 'left/attn_1/dense/kernel',\n",
       " 'left/attn_1/dense/bias',\n",
       " 'left/attn_1/gamma',\n",
       " 'left/attn_1/beta',\n",
       " 'left/ffn_1/conv1d/kernel',\n",
       " 'left/ffn_1/conv1d/bias',\n",
       " 'left/ffn_1/conv1d_1/kernel',\n",
       " 'left/ffn_1/conv1d_1/bias',\n",
       " 'left/ffn_1/gamma',\n",
       " 'left/ffn_1/beta',\n",
       " 'left/attn_2/dense/kernel',\n",
       " 'left/attn_2/dense/bias',\n",
       " 'left/attn_2/gamma',\n",
       " 'left/attn_2/beta',\n",
       " 'left/ffn_2/conv1d/kernel',\n",
       " 'left/ffn_2/conv1d/bias',\n",
       " 'left/ffn_2/conv1d_1/kernel',\n",
       " 'left/ffn_2/conv1d_1/bias',\n",
       " 'left/ffn_2/gamma',\n",
       " 'left/ffn_2/beta',\n",
       " 'left/attn_3/dense/kernel',\n",
       " 'left/attn_3/dense/bias',\n",
       " 'left/attn_3/gamma',\n",
       " 'left/attn_3/beta',\n",
       " 'left/ffn_3/conv1d/kernel',\n",
       " 'left/ffn_3/conv1d/bias',\n",
       " 'left/ffn_3/conv1d_1/kernel',\n",
       " 'left/ffn_3/conv1d_1/bias',\n",
       " 'left/ffn_3/gamma',\n",
       " 'left/ffn_3/beta',\n",
       " 'left/logits/dense/kernel',\n",
       " 'left/logits/dense/kernel/read',\n",
       " 'left/logits/dense/bias',\n",
       " 'left/logits/dense/bias/read',\n",
       " 'left/logits/dense/Tensordot/Shape',\n",
       " 'left/logits/dense/Tensordot/Rank',\n",
       " 'left/logits/dense/Tensordot/axes',\n",
       " 'left/logits/dense/Tensordot/GreaterEqual/y',\n",
       " 'left/logits/dense/Tensordot/GreaterEqual',\n",
       " 'left/logits/dense/Tensordot/Cast',\n",
       " 'left/logits/dense/Tensordot/mul',\n",
       " 'left/logits/dense/Tensordot/Less/y',\n",
       " 'left/logits/dense/Tensordot/Less',\n",
       " 'left/logits/dense/Tensordot/Cast_1',\n",
       " 'left/logits/dense/Tensordot/add',\n",
       " 'left/logits/dense/Tensordot/mul_1',\n",
       " 'left/logits/dense/Tensordot/add_1',\n",
       " 'left/logits/dense/Tensordot/range/start',\n",
       " 'left/logits/dense/Tensordot/range/delta',\n",
       " 'left/logits/dense/Tensordot/range',\n",
       " 'left/logits/dense/Tensordot/ListDiff',\n",
       " 'left/logits/dense/Tensordot/GatherV2/axis',\n",
       " 'left/logits/dense/Tensordot/GatherV2',\n",
       " 'left/logits/dense/Tensordot/GatherV2_1/axis',\n",
       " 'left/logits/dense/Tensordot/GatherV2_1',\n",
       " 'left/logits/dense/Tensordot/Const',\n",
       " 'left/logits/dense/Tensordot/Prod',\n",
       " 'left/logits/dense/Tensordot/Const_1',\n",
       " 'left/logits/dense/Tensordot/Prod_1',\n",
       " 'left/logits/dense/Tensordot/concat/axis',\n",
       " 'left/logits/dense/Tensordot/concat',\n",
       " 'left/logits/dense/Tensordot/concat_1/axis',\n",
       " 'left/logits/dense/Tensordot/concat_1',\n",
       " 'left/logits/dense/Tensordot/stack',\n",
       " 'left/logits/dense/Tensordot/transpose',\n",
       " 'left/logits/dense/Tensordot/Reshape',\n",
       " 'left/logits/dense/Tensordot/transpose_1/perm',\n",
       " 'left/logits/dense/Tensordot/transpose_1',\n",
       " 'left/logits/dense/Tensordot/Reshape_1/shape',\n",
       " 'left/logits/dense/Tensordot/Reshape_1',\n",
       " 'left/logits/dense/Tensordot/MatMul',\n",
       " 'left/logits/dense/Tensordot/Const_2',\n",
       " 'left/logits/dense/Tensordot/concat_2/axis',\n",
       " 'left/logits/dense/Tensordot/concat_2',\n",
       " 'left/logits/dense/Tensordot',\n",
       " 'left/logits/dense/BiasAdd',\n",
       " 'left/logits/strided_slice/stack',\n",
       " 'left/logits/strided_slice/stack_1',\n",
       " 'left/logits/strided_slice/stack_2',\n",
       " 'left/logits/strided_slice',\n",
       " 'right/attn_0/dense/kernel',\n",
       " 'right/attn_0/dense/bias',\n",
       " 'right/attn_0/gamma',\n",
       " 'right/attn_0/beta',\n",
       " 'right/ffn_0/conv1d/kernel',\n",
       " 'right/ffn_0/conv1d/bias',\n",
       " 'right/ffn_0/conv1d_1/kernel',\n",
       " 'right/ffn_0/conv1d_1/bias',\n",
       " 'right/ffn_0/gamma',\n",
       " 'right/ffn_0/beta',\n",
       " 'right/attn_1/dense/kernel',\n",
       " 'right/attn_1/dense/bias',\n",
       " 'right/attn_1/gamma',\n",
       " 'right/attn_1/beta',\n",
       " 'right/ffn_1/conv1d/kernel',\n",
       " 'right/ffn_1/conv1d/bias',\n",
       " 'right/ffn_1/conv1d_1/kernel',\n",
       " 'right/ffn_1/conv1d_1/bias',\n",
       " 'right/ffn_1/gamma',\n",
       " 'right/ffn_1/beta',\n",
       " 'right/attn_2/dense/kernel',\n",
       " 'right/attn_2/dense/bias',\n",
       " 'right/attn_2/gamma',\n",
       " 'right/attn_2/beta',\n",
       " 'right/ffn_2/conv1d/kernel',\n",
       " 'right/ffn_2/conv1d/bias',\n",
       " 'right/ffn_2/conv1d_1/kernel',\n",
       " 'right/ffn_2/conv1d_1/bias',\n",
       " 'right/ffn_2/gamma',\n",
       " 'right/ffn_2/beta',\n",
       " 'right/attn_3/dense/kernel',\n",
       " 'right/attn_3/dense/bias',\n",
       " 'right/attn_3/gamma',\n",
       " 'right/attn_3/beta',\n",
       " 'right/ffn_3/conv1d/kernel',\n",
       " 'right/ffn_3/conv1d/bias',\n",
       " 'right/ffn_3/conv1d_1/kernel',\n",
       " 'right/ffn_3/conv1d_1/bias',\n",
       " 'right/ffn_3/gamma',\n",
       " 'right/ffn_3/beta',\n",
       " 'right/logits/dense/kernel',\n",
       " 'right/logits/dense/kernel/read',\n",
       " 'right/logits/dense/bias',\n",
       " 'right/logits/dense/bias/read',\n",
       " 'right/logits/dense/Tensordot/Shape',\n",
       " 'right/logits/dense/Tensordot/Rank',\n",
       " 'right/logits/dense/Tensordot/axes',\n",
       " 'right/logits/dense/Tensordot/GreaterEqual/y',\n",
       " 'right/logits/dense/Tensordot/GreaterEqual',\n",
       " 'right/logits/dense/Tensordot/Cast',\n",
       " 'right/logits/dense/Tensordot/mul',\n",
       " 'right/logits/dense/Tensordot/Less/y',\n",
       " 'right/logits/dense/Tensordot/Less',\n",
       " 'right/logits/dense/Tensordot/Cast_1',\n",
       " 'right/logits/dense/Tensordot/add',\n",
       " 'right/logits/dense/Tensordot/mul_1',\n",
       " 'right/logits/dense/Tensordot/add_1',\n",
       " 'right/logits/dense/Tensordot/range/start',\n",
       " 'right/logits/dense/Tensordot/range/delta',\n",
       " 'right/logits/dense/Tensordot/range',\n",
       " 'right/logits/dense/Tensordot/ListDiff',\n",
       " 'right/logits/dense/Tensordot/GatherV2/axis',\n",
       " 'right/logits/dense/Tensordot/GatherV2',\n",
       " 'right/logits/dense/Tensordot/GatherV2_1/axis',\n",
       " 'right/logits/dense/Tensordot/GatherV2_1',\n",
       " 'right/logits/dense/Tensordot/Const',\n",
       " 'right/logits/dense/Tensordot/Prod',\n",
       " 'right/logits/dense/Tensordot/Const_1',\n",
       " 'right/logits/dense/Tensordot/Prod_1',\n",
       " 'right/logits/dense/Tensordot/concat/axis',\n",
       " 'right/logits/dense/Tensordot/concat',\n",
       " 'right/logits/dense/Tensordot/concat_1/axis',\n",
       " 'right/logits/dense/Tensordot/concat_1',\n",
       " 'right/logits/dense/Tensordot/stack',\n",
       " 'right/logits/dense/Tensordot/transpose',\n",
       " 'right/logits/dense/Tensordot/Reshape',\n",
       " 'right/logits/dense/Tensordot/transpose_1/perm',\n",
       " 'right/logits/dense/Tensordot/transpose_1',\n",
       " 'right/logits/dense/Tensordot/Reshape_1/shape',\n",
       " 'right/logits/dense/Tensordot/Reshape_1',\n",
       " 'right/logits/dense/Tensordot/MatMul',\n",
       " 'right/logits/dense/Tensordot/Const_2',\n",
       " 'right/logits/dense/Tensordot/concat_2/axis',\n",
       " 'right/logits/dense/Tensordot/concat_2',\n",
       " 'right/logits/dense/Tensordot',\n",
       " 'right/logits/dense/BiasAdd',\n",
       " 'right/logits/strided_slice/stack',\n",
       " 'right/logits/strided_slice/stack_1',\n",
       " 'right/logits/strided_slice/stack_2',\n",
       " 'right/logits/strided_slice',\n",
       " 'logits']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from self-attention/model.ckpt\n",
      "INFO:tensorflow:Froze 85 variables.\n",
      "INFO:tensorflow:Converted 85 variables to const ops.\n",
      "1637 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('self-attention', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01998395], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = load_graph('self-attention/frozen_model.pb')\n",
    "x1 = g.get_tensor_by_name('import/Placeholder:0')\n",
    "x2 = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "test_sess.run(1-logits, feed_dict = {x1 : left, x2: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2318753 , 0.5197979 , 0.2777239 , 0.14316326, 0.8766695 ,\n",
       "       0.22495192, 0.91102034, 0.0115208 , 0.070916  , 0.07542306,\n",
       "       0.94589764, 0.04265296, 0.34291208, 0.43791467, 0.13047814,\n",
       "       0.05099976, 0.04077601, 0.03098774, 0.05358207, 0.09898269,\n",
       "       0.4222178 , 0.07683033, 0.27565062, 0.18730605, 0.34941596,\n",
       "       0.08564615, 0.19999826, 0.05309838, 0.04758018, 0.01607895,\n",
       "       0.13069487, 0.6605412 , 0.9515858 , 0.16830862, 0.5734025 ,\n",
       "       0.5354396 , 0.749179  , 0.2538219 , 0.0801577 , 0.05013776,\n",
       "       0.4355023 , 0.45459825, 0.03258169, 0.15339905, 0.9313603 ,\n",
       "       0.42679828, 0.95682436, 0.07610172, 0.03255141, 0.00740314,\n",
       "       0.52017945, 0.46709698, 0.74399465, 0.45834607, 0.02888119,\n",
       "       0.9627122 , 0.1260702 , 0.03194386, 0.11266536, 0.05345899,\n",
       "       0.5395947 , 0.34424478, 0.73064005, 0.17178106, 0.76854   ,\n",
       "       0.03258795, 0.06777585, 0.8709656 , 0.09303659, 0.03535146,\n",
       "       0.07395506, 0.06536621, 0.1412226 , 0.94608825, 0.07875746,\n",
       "       0.01958525, 0.16110301, 0.19749928, 0.0451234 , 0.03573173],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sess.run(1-logits, feed_dict = {x1 : batch_x_left, x2: batch_x_right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
