{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import itertools\n",
    "from unidecode import unidecode\n",
    "import malaya\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words, atleast=2):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words - 10)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def str_idx(corpus, dic, maxlen, UNK = 3):\n",
    "    X = np.zeros((len(corpus), maxlen))\n",
    "    for i in range(len(corpus)):\n",
    "        for no, k in enumerate(corpus[i][:maxlen]):\n",
    "            X[i, no] = dic.get(k, UNK)\n",
    "    return X\n",
    "\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 2]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-similarity.json') as fopen:\n",
    "    train = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, right, label = train['left'], train['right'], train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test-similarity.json') as fopen:\n",
    "    test = json.load(fopen)\n",
    "test_left, test_right, test_label = test['left'], test['right'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2605321, 1531070]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(label, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 73142\n",
      "Most common words [('saya', 3584482), ('yang', 3541065), ('untuk', 2110965), ('apakah', 1948962), ('dan', 1556927), ('anda', 1375550)]\n",
      "Sample data [7, 355, 325, 2415, 43, 9, 7, 355, 4166, 2415] ['apakah', 'maksud', 'cinta', 'sejati', 'kepada', 'anda', 'apakah', 'maksud', 'memuja', 'sejati']\n"
     ]
    }
   ],
   "source": [
    "concat = list(itertools.chain(*(left + right)))\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size, 1)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('similarity-dictionary.json','w') as fopen:\n",
    "    fopen.write(json.dumps({'dictionary':dictionary,'reverse_dictionary':rev_dictionary}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, learning_rate, dropout):\n",
    "        \n",
    "        def cells(size, reuse=False):\n",
    "            cell = tf.nn.rnn_cell.LSTMCell(size,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
    "            return tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=dropout)\n",
    "        \n",
    "        def rnn(inputs, scope):\n",
    "            with tf.variable_scope(scope, reuse = tf.AUTO_REUSE):\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    num_units = size_layer, memory = inputs)\n",
    "                rnn_cells = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                        [cells(size_layer) for _ in range(num_layers)]\n",
    "                    ),\n",
    "                    attention_mechanism = attention_mechanism,\n",
    "                    attention_layer_size = size_layer,\n",
    "                    alignment_history = True,\n",
    "                )\n",
    "                outputs, last_state = tf.nn.dynamic_rnn(\n",
    "                    rnn_cells, inputs, dtype = tf.float32\n",
    "                )\n",
    "                return outputs[:,-1]\n",
    "        \n",
    "        self.X_left = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_right = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None])\n",
    "        self.batch_size = tf.shape(self.X_left)[0]\n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        embedded_left = tf.nn.embedding_lookup(encoder_embeddings, self.X_left)\n",
    "        embedded_right = tf.nn.embedding_lookup(encoder_embeddings, self.X_right)\n",
    "        \n",
    "        def contrastive_loss(y,d):\n",
    "            tmp= y * tf.square(d)\n",
    "            tmp2 = (1-y) * tf.square(tf.maximum((1 - d),0))\n",
    "            return tf.reduce_sum(tmp +tmp2)/tf.cast(self.batch_size,tf.float32)/2\n",
    "        \n",
    "        self.output_left = rnn(embedded_left, 'left')\n",
    "        self.output_right = rnn(embedded_right, 'right')\n",
    "        self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.output_left,self.output_right)),\n",
    "                                              1,keep_dims=True))\n",
    "        self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.output_left),\n",
    "                                                                           1,keep_dims=True)),\n",
    "                                                     tf.sqrt(tf.reduce_sum(tf.square(self.output_right),\n",
    "                                                                           1,keep_dims=True))))\n",
    "        self.distance = tf.reshape(self.distance, [-1])\n",
    "        self.logits = tf.identity(self.distance, name = 'logits')\n",
    "        self.cost = contrastive_loss(self.Y,self.distance)\n",
    "        \n",
    "        self.temp_sim = tf.subtract(tf.ones_like(self.distance),\n",
    "                                    tf.rint(self.distance))\n",
    "        correct_predictions = tf.equal(self.temp_sim, self.Y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "embedded_size = 128\n",
    "learning_rate = 1e-4\n",
    "maxlen = 50\n",
    "batch_size = 128\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_X_left = str_idx(left, dictionary, maxlen)\n",
    "train_X_right = str_idx(right, dictionary, maxlen)\n",
    "train_Y = label\n",
    "\n",
    "test_X_left = str_idx(test_left, dictionary, maxlen)\n",
    "test_X_right = str_idx(test_right, dictionary, maxlen)\n",
    "test_Y = test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-ef277ddae37f>:6: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-ef277ddae37f>:15: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-ef277ddae37f>:22: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-9-ef277ddae37f>:42: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-9-ef277ddae37f>:46: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer,num_layers,embedded_size,len(dictionary),learning_rate,dropout)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahdanau/model.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables())\n",
    "saver.save(sess, 'bahdanau/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:55:21<00:00,  4.74it/s, accuracy=0.718, cost=0.0761]\n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.29it/s, accuracy=0.775, cost=0.0865]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.746424\n",
      "time taken: 6955.883935451508\n",
      "epoch: 0, training loss: 0.092503, training acc: 0.721196, valid loss: 0.086450, valid acc: 0.746424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:11<00:00,  4.74it/s, accuracy=0.732, cost=0.0744]\n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.30it/s, accuracy=0.75, cost=0.0809] \n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.746424, current acc: 0.763280\n",
      "time taken: 6826.028009176254\n",
      "epoch: 0, training loss: 0.081873, training acc: 0.761882, valid loss: 0.081624, valid acc: 0.763280\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:11<00:00,  4.81it/s, accuracy=0.915, cost=0.0614]\n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, accuracy=0.762, cost=0.0785]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.763280, current acc: 0.771912\n",
      "time taken: 6826.077198982239\n",
      "epoch: 0, training loss: 0.077154, training acc: 0.778432, valid loss: 0.079107, valid acc: 0.771912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:13<00:00,  4.83it/s, accuracy=0.944, cost=0.0466]\n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.29it/s, accuracy=0.775, cost=0.0792]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.771912, current acc: 0.778024\n",
      "time taken: 6828.589703083038\n",
      "epoch: 0, training loss: 0.074152, training acc: 0.789472, valid loss: 0.077665, valid acc: 0.778024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.28it/s, accuracy=0.775, cost=0.0797]0.0492] \n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.778024, current acc: 0.781104\n",
      "time taken: 6830.9069492816925\n",
      "epoch: 0, training loss: 0.071784, training acc: 0.798278, valid loss: 0.076891, valid acc: 0.781104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:   4%|▍         | 1354/32316 [04:43<1:48:08,  4.77it/s, accuracy=0.719, cost=0.0897]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:16<00:00,  4.74it/s, accuracy=0.972, cost=0.0366] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.40it/s, accuracy=0.75, cost=0.0798] \n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.781104, current acc: 0.783340\n",
      "time taken: 6830.440257072449\n",
      "epoch: 0, training loss: 0.069742, training acc: 0.805855, valid loss: 0.076416, valid acc: 0.783340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:10<00:00,  4.72it/s, accuracy=0.944, cost=0.0372] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, accuracy=0.762, cost=0.0789]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.783340, current acc: 0.785572\n",
      "time taken: 6824.842344999313\n",
      "epoch: 0, training loss: 0.067793, training acc: 0.812866, valid loss: 0.075852, valid acc: 0.785572\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:13<00:00,  4.74it/s, accuracy=0.944, cost=0.0349] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.34it/s, accuracy=0.775, cost=0.0784]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.785572, current acc: 0.787424\n",
      "time taken: 6827.9793746471405\n",
      "epoch: 0, training loss: 0.065874, training acc: 0.819701, valid loss: 0.075570, valid acc: 0.787424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:11<00:00,  4.78it/s, accuracy=0.915, cost=0.0403] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s, accuracy=0.788, cost=0.0763]\n",
      "train minibatch loop:   0%|          | 0/32316 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6825.849843502045\n",
      "epoch: 0, training loss: 0.064019, training acc: 0.825954, valid loss: 0.075614, valid acc: 0.786776\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 32316/32316 [1:53:07<00:00,  4.80it/s, accuracy=0.944, cost=0.0329] \n",
      "test minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s, accuracy=0.775, cost=0.0759]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 6821.773227930069\n",
      "epoch: 0, training loss: 0.062121, training acc: 0.832607, valid loss: 0.075962, valid acc: 0.786164\n",
      "\n",
      "break epoch:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 2, 0, 0, 0\n",
    "\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n' % (EPOCH))\n",
    "        break\n",
    "\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    pbar = tqdm(range(0, len(train_X_left), batch_size), desc='train minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x_left = train_X_left[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "        batch_x_right = train_X_right[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "        batch_y = train_Y[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        assert not np.isnan(loss)\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    pbar = tqdm(range(0, len(test_X_left), batch_size), desc='test minibatch loop')\n",
    "    for i in pbar:\n",
    "        batch_x_left = test_X_left[i:min(i+batch_size,test_X_left.shape[0])]\n",
    "        batch_x_right = test_X_right[i:min(i+batch_size,test_X_left.shape[0])]\n",
    "        batch_y = test_Y[i:min(i+batch_size,test_X_left.shape[0])]\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                           feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc)\n",
    "    \n",
    "    train_loss /= (len(train_X_left) / batch_size)\n",
    "    train_acc /= (len(train_X_left) / batch_size)\n",
    "    test_loss /= (len(test_X_left) / batch_size)\n",
    "    test_acc /= (len(test_X_left) / batch_size)\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print(\n",
    "            'epoch: %d, pass acc: %f, current acc: %f'\n",
    "            % (EPOCH, CURRENT_ACC, test_acc)\n",
    "        )\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    \n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
    "                                                                                          train_acc,test_loss,\n",
    "                                                                                          test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Placeholder',\n",
       " 'Placeholder_1',\n",
       " 'Placeholder_2',\n",
       " 'Variable',\n",
       " 'left/memory_layer/kernel',\n",
       " 'left/rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'left/rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'left/rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'left/rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'left/rnn/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'left/rnn/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'left/rnn/attention_wrapper/attention_layer/kernel',\n",
       " 'right/memory_layer/kernel',\n",
       " 'right/rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'right/rnn/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'right/rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/kernel/Read/ReadVariableOp',\n",
       " 'right/rnn/attention_wrapper/multi_rnn_cell/cell_1/lstm_cell/bias/Read/ReadVariableOp',\n",
       " 'right/rnn/attention_wrapper/bahdanau_attention/query_layer/kernel',\n",
       " 'right/rnn/attention_wrapper/bahdanau_attention/attention_v',\n",
       " 'right/rnn/attention_wrapper/attention_layer/kernel',\n",
       " 'logits']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name)\n",
    "        and 'Adam' not in n.name\n",
    "        and '_power' not in n.name\n",
    "        and 'gradient' not in n.name\n",
    "        and 'Initializer' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bahdanau/model.ckpt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'bahdanau/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.], dtype=float32), array([0.11445844], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = str_idx(['a person is outdoors, on a horse.'], dictionary, maxlen)\n",
    "right = str_idx(['a person on a horse jumps over a broken down airplane.'], dictionary, maxlen)\n",
    "sess.run([model.temp_sim,1-model.distance], feed_dict = {model.X_left : left, \n",
    "                                        model.X_right: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation minibatch loop: 100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n"
     ]
    }
   ],
   "source": [
    "real_Y, predict_Y = [], []\n",
    "\n",
    "pbar = tqdm(\n",
    "    range(0, len(test_X_left), batch_size), desc = 'validation minibatch loop'\n",
    ")\n",
    "for i in pbar:\n",
    "    batch_x_left = test_X_left[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "    batch_x_right = test_X_right[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "    batch_y = test_Y[i:min(i+batch_size,train_X_left.shape[0])]\n",
    "    predict_Y += sess.run(model.temp_sim, feed_dict = {model.X_left : batch_x_left, \n",
    "                                        model.X_right: batch_x_right,\n",
    "                                        model.Y : batch_y}).tolist()\n",
    "    real_Y += batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "not similar       0.83      0.83      0.83     31524\n",
      "    similar       0.71      0.71      0.71     18476\n",
      "\n",
      "avg / total       0.79      0.79      0.79     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        real_Y, predict_Y, target_names = ['not similar', 'similar']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from bahdanau/model.ckpt\n",
      "WARNING:tensorflow:From <ipython-input-20-9a7215a4e58a>:23: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 17 variables.\n",
      "INFO:tensorflow:Converted 17 variables to const ops.\n",
      "647 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('bahdanau', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11765248], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = load_graph('bahdanau/frozen_model.pb')\n",
    "x1 = g.get_tensor_by_name('import/Placeholder:0')\n",
    "x2 = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)\n",
    "test_sess.run(1-logits, feed_dict = {x1 : left, x2: right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4636389 , 0.5283668 , 0.43854022, 0.8202803 , 0.64394784,\n",
       "       0.84979135, 0.745062  , 0.01964164, 0.07101661, 0.02169931,\n",
       "       0.8392247 , 0.22707516, 0.19469285, 0.4840045 , 0.05370182,\n",
       "       0.4678564 , 0.4111814 , 0.11001766, 0.20520616, 0.07242185,\n",
       "       0.7431572 , 0.52817804, 0.4351002 , 0.63338685, 0.52839124,\n",
       "       0.07311231, 0.1716168 , 0.09279257, 0.02310717, 0.02681172,\n",
       "       0.2308088 , 0.551746  , 0.8105283 , 0.66022396, 0.739179  ,\n",
       "       0.38779128, 0.8515695 , 0.7534613 , 0.05358309, 0.05516434,\n",
       "       0.63869566, 0.7444098 , 0.63428354, 0.49298012, 0.75610924,\n",
       "       0.54483724, 0.9024776 , 0.05228931, 0.05101156, 0.02496451,\n",
       "       0.7684243 , 0.37446058, 0.8911811 , 0.39399248, 0.04925126,\n",
       "       0.89727813, 0.34909683, 0.09850705, 0.04967946, 0.05255091,\n",
       "       0.58232725, 0.40308565, 0.68486273, 0.41244376, 0.06464297,\n",
       "       0.07472116, 0.06430554, 0.42752308, 0.10852087, 0.0495699 ,\n",
       "       0.11905402, 0.26009667, 0.53447616, 0.88553053, 0.04034108,\n",
       "       0.05235732, 0.43953466, 0.10045218, 0.07925862, 0.06360978],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sess.run(1-logits, feed_dict = {x1 : batch_x_left, x2: batch_x_right})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
