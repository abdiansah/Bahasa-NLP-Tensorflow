{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pointer_generator_helper import (PointerGeneratorDecoder, \n",
    "                                      PointerGeneratorGreedyEmbeddingHelper, \n",
    "                                      PointerGeneratorBahdanauAttention,\n",
    "                                      PointerGeneratorAttentionWrapper)\n",
    "from tensor2tensor.utils import beam_search, rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29855"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('news-30k.json') as fopen:\n",
    "    news = json.load(fopen)\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Ibu saudara Haziq terharu sokongan rakyat Malaysia',\n",
       " 'url': 'https://www.themalaysianinsight.com/bahasa/s/142491',\n",
       " 'news': 'themalaysianinsight',\n",
       " 'language': 'malay',\n",
       " 'top-image': 'https://www.themalaysianinsight.com/resources/stories_images/142491/perhimpunanan_solidarity_kedamaian_03__full.jpg',\n",
       " 'text': 'WARIS keluarga Allahyarham Muhammad Haziq Mohd Tarmizi, 17, yang terkorban dalam tragedi tembakan di Christchurch, New Zealand, pada 15 Mac lepas, melahirkan rasa terharu akan sokongan diberi rakyat Malaysia semasa perhimpunan Solidariti Kedamaian.\\n\\nZarina Shuib , ibu saudara Muhammad Haziq, memanjatkan kesyukuran kepada Allah SWT kerana berkesempatan menyertai rakyat Malaysia dalam perhimpunan itu di Kuala Lumpur hari ini.',\n",
       " 'date': '2019-03-23T03:52:02',\n",
       " 'date_utc': '2019-03-22T19:52:02'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "\n",
    "accept_tokens = ',-.()\"\\''\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 1 or w in accept_tokens]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "def clean_label(label):\n",
    "    string = re.sub('[^A-Za-z\\- ]+', ' ', label)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29855/29855 [00:45<00:00, 662.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_len = 5\n",
    "max_len = 500\n",
    "\n",
    "x, y = [], []\n",
    "for n in tqdm(news):\n",
    "    if len(n['text'].split()) > min_len:\n",
    "        p = preprocessing(n['text'])[:max_len]\n",
    "        x.append(p)\n",
    "        p = preprocessing(n['title'])\n",
    "        y.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29855, 29855)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 88005\n",
      "Most common words [(',', 380933), ('.', 338805), ('yang', 158373), ('dan', 147862), ('di', 124501), ('-', 118778)]\n",
      "Sample data [4340, 287, 1410, 343, 1606, 114, 3583, 4, 10, 4] ['waris', 'keluarga', 'allahyarham', 'muhammad', 'haziq', 'mohd', 'tarmizi', ',', '<NUM>', ',']\n",
      "filtered vocab size: 88009\n",
      "% of vocab used: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "concat = list(itertools.chain(*x))\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n",
    "print('filtered vocab size:',len(dictionary))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary)/vocabulary_size,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary['GO']\n",
    "PAD = dictionary['PAD']\n",
    "EOS = dictionary['EOS']\n",
    "UNK = dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2idx(sent, vocab, UNK=UNK):\n",
    "    tokens = sent\n",
    "    oovs = []\n",
    "    extend_tokens = []\n",
    "    tokenized = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            tokenized.append(UNK)\n",
    "            if token not in oovs:\n",
    "                oovs.append(token)\n",
    "            extend_tokens.append(len(vocab) + oovs.index(token))\n",
    "        else:\n",
    "            extend_tokens.append(vocab[token])\n",
    "            tokenized.append(vocab[token])\n",
    "    return tokenized, extend_tokens, oovs\n",
    "\n",
    "def target2idx(sent, oovs, vocab,UNK=UNK):\n",
    "    tokens = sent\n",
    "    tokenized = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            if token not in oovs:\n",
    "                tokenized.append(UNK)\n",
    "            else:\n",
    "                tokenized.append(len(vocab) + oovs.index(token))\n",
    "        else:\n",
    "            tokenized.append(vocab[token])\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarization:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, dict_size):\n",
    "        \n",
    "        def lstm_cell(reuse=False):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer, reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        self.source_oov_words = tf.placeholder(tf.int32, shape=[])\n",
    "        self.source_extend_tokens = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        \n",
    "        condition = tf.less(decoder_input, dict_size)\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_input_length = self.Y_seq_len\n",
    "        self.predict_count = tf.reduce_sum(self.decoder_input_length)\n",
    "        \n",
    "        embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(embeddings, self.X)\n",
    "        encoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.encoder_out, self.encoder_state = tf.nn.dynamic_rnn(cell = encoder_cells, \n",
    "                                                                 inputs = encoder_embedded, \n",
    "                                                                 sequence_length = self.X_seq_len,\n",
    "                                                                 dtype = tf.float32)\n",
    "        self.decode_initial_state = self.encoder_state[-1]\n",
    "        print(self.decode_initial_state)\n",
    "        \n",
    "        atten_mech = PointerGeneratorBahdanauAttention(\n",
    "                size_layer, self.encoder_out, memory_sequence_length=self.X_seq_len,\n",
    "        coverage = True)\n",
    "        decoder_cells = [lstm_cell() for _ in range(num_layers)]\n",
    "        decoder_cells[0] = PointerGeneratorAttentionWrapper(\n",
    "                cell=decoder_cells[0],\n",
    "                attention_mechanism=atten_mech,\n",
    "                attention_layer_size=size_layer,\n",
    "                alignment_history = True,\n",
    "                coverage = True\n",
    "            )\n",
    "        initial_state = [self.decode_initial_state for i in range(num_layers)]\n",
    "        attention_cell_state = decoder_cells[0].zero_state(\n",
    "                dtype=tf.float32, batch_size=batch_size)\n",
    "        initial_state[0] = attention_cell_state.clone(\n",
    "                cell_state=initial_state[0])\n",
    "        self.initial_state = tuple(initial_state)\n",
    "        decoder_cells = tf.contrib.rnn.MultiRNNCell(decoder_cells)\n",
    "        \n",
    "        decoded = tf.nn.embedding_lookup(embeddings, self.decoder_input)\n",
    "        \n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoded,\n",
    "            self.decoder_input_length\n",
    "        )\n",
    "        dense_layer = tf.layers.Dense(dict_size)\n",
    "        \n",
    "        training_decoder = PointerGeneratorDecoder(\n",
    "            source_extend_tokens = self.source_extend_tokens,\n",
    "            source_oov_words = self.source_oov_words,\n",
    "            coverage = True,\n",
    "            cell=decoder_cells,\n",
    "            helper=training_helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=dense_layer\n",
    "        )\n",
    "        \n",
    "        maxlen = tf.reduce_max(self.decoder_input_length)\n",
    "        train_dec_outputs, train_dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=maxlen,\n",
    "            swap_memory=True)\n",
    "        logits = train_dec_outputs.rnn_output\n",
    "        self.training_logits = logits\n",
    "        \n",
    "        masks = tf.sequence_mask(\n",
    "            self.decoder_input_length, maxlen, \n",
    "            dtype=tf.float32)\n",
    "        \n",
    "        targets = tf.slice(self.Y, [0, 0], [-1, maxlen])\n",
    "        i1, i2 = tf.meshgrid(tf.range(batch_size),\n",
    "                     tf.range(maxlen), indexing=\"ij\")\n",
    "        indices = tf.stack((i1,i2,targets),axis=2)\n",
    "        probs = tf.gather_nd(logits, indices)\n",
    "        probs = tf.where(tf.less_equal(probs,0),tf.ones_like(probs)*1e-10,probs)\n",
    "        crossent = -tf.log(probs)\n",
    "        self.cost = tf.reduce_sum(crossent * masks) / tf.to_float(batch_size)\n",
    "        alignment_history = train_dec_last_state[0].alignment_history.stack()\n",
    "        alignment_history = tf.transpose(alignment_history,[1,2,0])\n",
    "        coverage_loss = tf.minimum(alignment_history,tf.cumsum(alignment_history, axis=2, exclusive=True))\n",
    "        self.coverage_loss = tf.reduce_sum(coverage_loss / tf.to_float(batch_size))\n",
    "        self.cost = self.cost + self.coverage_loss\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.cost)\n",
    "        \n",
    "        helper = PointerGeneratorGreedyEmbeddingHelper(\n",
    "            embedding=embeddings,\n",
    "            start_tokens=tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "            end_token=EOS\n",
    "        )\n",
    "        \n",
    "        inference_decoder = PointerGeneratorDecoder(\n",
    "            source_extend_tokens = self.source_extend_tokens,\n",
    "            source_oov_words = self.source_oov_words,\n",
    "            coverage = True,\n",
    "            cell=decoder_cells,\n",
    "            helper=helper,\n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=dense_layer\n",
    "        )\n",
    "        \n",
    "        dec_outputs, dec_last_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder,\n",
    "            output_time_major=False,\n",
    "            maximum_iterations=tf.reduce_max(self.X_seq_len),\n",
    "            swap_memory=True)\n",
    "        \n",
    "        self.beam_predictions = dec_outputs.sample_id\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        y_t = tf.argmax(logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 2\n",
    "embedded_size = 256\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/while/Exit_4:0\", shape=(?, 256), dtype=float32)\n",
      "<pointer_generator_helper.PointerGeneratorBahdanauAttention object at 0x7fdbc9665ba8>\n",
      "Tensor(\"decoder/while/PGDecoderStep/decoder/multi_rnn_cell/cell_0/cell_0/pointer_generator_attention_wrapper/Softmax:0\", shape=(?, ?), dtype=float32) Tensor(\"decoder/while/PGDecoderStep/decoder/multi_rnn_cell/cell_0/cell_0/pointer_generator_attention_wrapper/Softmax:0\", shape=(?, ?), dtype=float32)\n",
      "<pointer_generator_helper.PointerGeneratorBahdanauAttention object at 0x7fdbc9665ba8>\n",
      "Tensor(\"decoder/while/PGDecoderStep/decoder/multi_rnn_cell/cell_0/cell_0/pointer_generator_attention_wrapper/Softmax_1:0\", shape=(?, ?), dtype=float32) Tensor(\"decoder/while/PGDecoderStep/decoder/multi_rnn_cell/cell_0/cell_0/pointer_generator_attention_wrapper/Softmax_1:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Summarization(size_layer, num_layers, embedded_size, len(dictionary))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(X, Y):\n",
    "    s_, es_, oovs_, target_ = [], [], [], []\n",
    "    for x, y in zip(X, Y):\n",
    "        s,es,oovs = sent2idx(x, dictionary)\n",
    "        target = target2idx(y, oovs,dictionary)\n",
    "        s_.append(s)\n",
    "        es_.append(es)\n",
    "        oovs_.append(oovs)\n",
    "        target_.append(target)\n",
    "    s_ = pad_sequences(s_,padding='post')\n",
    "    es_ = pad_sequences(es_,padding='post')\n",
    "    target_ = pad_sequences(target_,padding='post')\n",
    "    maxlen = max([len(o) for o in oovs_])\n",
    "    return s_, es_, target_, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "def calculate_rouges(predicted, batch_y):\n",
    "    non = np.count_nonzero(batch_y, axis = 1)\n",
    "    o = []\n",
    "    for n in non:\n",
    "        o.append([True for _ in range(n)])\n",
    "    b = sequence.pad_sequences(o, dtype = np.bool, padding = 'post', value = False)\n",
    "    batch_y = np.array(batch_y)\n",
    "    rouges = []\n",
    "    for i in range(predicted.shape[0]):\n",
    "        a = batch_y[i][b[i]]\n",
    "        p = predicted[i][b[i]]\n",
    "        rouges.append(rouge.rouge_n([p], [a]))\n",
    "    return np.mean(rouges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:54<00:00,  2.20it/s, accuracy=0.111, cost=36.1, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.07it/s, accuracy=0.246, cost=91, rouge_2=0.188]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg loss: 43.339262, avg accuracy: 0.280213\n",
      "epoch: 0, avg loss test: 38.250575, avg accuracy test: 0.340743\n",
      "epoch: 0, avg train rouge: 0.109591, avg test rouge: 0.149172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:36<00:00,  2.21it/s, accuracy=0.556, cost=23.3, rouge_2=0.375]  \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.08it/s, accuracy=0.262, cost=82.9, rouge_2=0.258] \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 38.876651, avg accuracy: 0.326685\n",
      "epoch: 1, avg loss test: 36.583588, avg accuracy test: 0.355238\n",
      "epoch: 1, avg train rouge: 0.137114, avg test rouge: 0.158392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:33<00:00,  2.20it/s, accuracy=0.111, cost=36.9, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.05it/s, accuracy=0.197, cost=106, rouge_2=0.0723] \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, avg loss: 31.740749, avg accuracy: 0.403784\n",
      "epoch: 2, avg loss test: 48.051658, avg accuracy test: 0.255413\n",
      "epoch: 2, avg train rouge: 0.195708, avg test rouge: 0.084357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.21it/s, accuracy=0.667, cost=25.3, rouge_2=0.375] \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.12it/s, accuracy=0.23, cost=98, rouge_2=0.22]     \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, avg loss: 37.722801, avg accuracy: 0.329731\n",
      "epoch: 3, avg loss test: 42.431249, avg accuracy test: 0.296041\n",
      "epoch: 3, avg train rouge: 0.134821, avg test rouge: 0.115910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.21it/s, accuracy=0.333, cost=42.8, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.11it/s, accuracy=0.0984, cost=110, rouge_2=0.178]  \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, avg loss: 39.667968, avg accuracy: 0.292814\n",
      "epoch: 4, avg loss test: 46.339806, avg accuracy test: 0.243682\n",
      "epoch: 4, avg train rouge: 0.110209, avg test rouge: 0.074491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:36<00:00,  2.20it/s, accuracy=0.444, cost=32, rouge_2=0.125]    \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:21<00:00,  5.07it/s, accuracy=0.197, cost=98.8, rouge_2=0.0714]\n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, avg loss: 41.869029, avg accuracy: 0.265789\n",
      "epoch: 5, avg loss test: 44.102097, avg accuracy test: 0.266624\n",
      "epoch: 5, avg train rouge: 0.089265, avg test rouge: 0.093176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.21it/s, accuracy=0.333, cost=32.2, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:21<00:00,  5.11it/s, accuracy=0.164, cost=102, rouge_2=0.162]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, avg loss: 40.201741, avg accuracy: 0.274965\n",
      "epoch: 6, avg loss test: 44.676245, avg accuracy test: 0.258674\n",
      "epoch: 6, avg train rouge: 0.094593, avg test rouge: 0.088101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:36<00:00,  2.21it/s, accuracy=0.222, cost=37.9, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.10it/s, accuracy=0.082, cost=106, rouge_2=0.213]  \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, avg loss: 43.714808, avg accuracy: 0.242870\n",
      "epoch: 7, avg loss test: 46.924372, avg accuracy test: 0.234631\n",
      "epoch: 7, avg train rouge: 0.076977, avg test rouge: 0.074650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:35<00:00,  2.21it/s, accuracy=0.333, cost=32.2, rouge_2=0]     \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.04it/s, accuracy=0.197, cost=104, rouge_2=0.039]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, avg loss: 39.921322, avg accuracy: 0.276835\n",
      "epoch: 8, avg loss test: 46.324896, avg accuracy test: 0.242812\n",
      "epoch: 8, avg train rouge: 0.099394, avg test rouge: 0.076692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:39<00:00,  2.22it/s, accuracy=0.444, cost=28.1, rouge_2=0.125]  \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:21<00:00,  5.09it/s, accuracy=0.18, cost=101, rouge_2=0.117]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, avg loss: 40.963847, avg accuracy: 0.258915\n",
      "epoch: 9, avg loss test: 44.381440, avg accuracy test: 0.256690\n",
      "epoch: 9, avg train rouge: 0.088552, avg test rouge: 0.092548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:39<00:00,  2.21it/s, accuracy=0.222, cost=35.4, rouge_2=0]     \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.08it/s, accuracy=0.18, cost=105, rouge_2=0.112]    \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, avg loss: 41.393298, avg accuracy: 0.256007\n",
      "epoch: 10, avg loss test: 46.377687, avg accuracy test: 0.238520\n",
      "epoch: 10, avg train rouge: 0.085633, avg test rouge: 0.072841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.22it/s, accuracy=0.222, cost=35.5, rouge_2=0.125]  \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.03it/s, accuracy=0.0984, cost=106, rouge_2=0.0278]\n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, avg loss: 41.463504, avg accuracy: 0.254737\n",
      "epoch: 11, avg loss test: 48.693768, avg accuracy test: 0.211095\n",
      "epoch: 11, avg train rouge: 0.084007, avg test rouge: 0.053379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:33<00:00,  2.21it/s, accuracy=0.111, cost=39.4, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.05it/s, accuracy=0.115, cost=108, rouge_2=0.0995]  \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, avg loss: 45.051455, avg accuracy: 0.211254\n",
      "epoch: 12, avg loss test: 47.529852, avg accuracy test: 0.230337\n",
      "epoch: 12, avg train rouge: 0.055993, avg test rouge: 0.071783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.21it/s, accuracy=0.222, cost=39.3, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:21<00:00,  5.17it/s, accuracy=0.18, cost=110, rouge_2=0.0361]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, avg loss: 47.724544, avg accuracy: 0.189737\n",
      "epoch: 13, avg loss test: 51.746153, avg accuracy test: 0.185324\n",
      "epoch: 13, avg train rouge: 0.048624, avg test rouge: 0.050050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:32<00:00,  2.21it/s, accuracy=0.222, cost=38.8, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.10it/s, accuracy=0.0984, cost=115, rouge_2=0.118]  \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, avg loss: 43.576472, avg accuracy: 0.216453\n",
      "epoch: 14, avg loss test: 51.974185, avg accuracy test: 0.180553\n",
      "epoch: 14, avg train rouge: 0.064396, avg test rouge: 0.051724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:33<00:00,  2.21it/s, accuracy=0.333, cost=28.2, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.06it/s, accuracy=0.131, cost=112, rouge_2=0.163]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, avg loss: 41.314440, avg accuracy: 0.236375\n",
      "epoch: 15, avg loss test: 47.696124, avg accuracy test: 0.229250\n",
      "epoch: 15, avg train rouge: 0.079887, avg test rouge: 0.079132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:33<00:00,  2.21it/s, accuracy=0.222, cost=38.3, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.12it/s, accuracy=0.164, cost=111, rouge_2=0.0714]  \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, avg loss: 44.112664, avg accuracy: 0.214864\n",
      "epoch: 16, avg loss test: 50.285478, avg accuracy test: 0.193462\n",
      "epoch: 16, avg train rouge: 0.061069, avg test rouge: 0.056818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.21it/s, accuracy=0.333, cost=37.1, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.10it/s, accuracy=0.148, cost=114, rouge_2=0.188]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, avg loss: 43.175520, avg accuracy: 0.219689\n",
      "epoch: 17, avg loss test: 50.357799, avg accuracy test: 0.198479\n",
      "epoch: 17, avg train rouge: 0.065411, avg test rouge: 0.061611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:33<00:00,  2.21it/s, accuracy=0.222, cost=32.1, rouge_2=0]      \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:22<00:00,  5.08it/s, accuracy=0.115, cost=112, rouge_2=0.179]   \n",
      "train minibatch loop:   0%|          | 0/4479 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, avg loss: 42.146986, avg accuracy: 0.234126\n",
      "epoch: 18, avg loss test: 48.526205, avg accuracy test: 0.216265\n",
      "epoch: 18, avg train rouge: 0.080118, avg test rouge: 0.077062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 4479/4479 [41:34<00:00,  2.20it/s, accuracy=0.111, cost=30.1, rouge_2=0.125]  \n",
      "test minibatch loop: 100%|██████████| 498/498 [01:21<00:00,  5.11it/s, accuracy=0.131, cost=108, rouge_2=0.0625]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, avg loss: 39.921742, avg accuracy: 0.251466\n",
      "epoch: 19, avg loss test: 48.348149, avg accuracy test: 0.217966\n",
      "epoch: 19, avg train rouge: 0.089897, avg test rouge: 0.078513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "for EPOCH in range(20):\n",
    "    lasttime = time.time()\n",
    "    total_loss, total_accuracy, total_loss_test, total_accuracy_test = 0, 0, 0, 0\n",
    "    rouge_train, rouge_test = 0, 0\n",
    "    pbar = tqdm(range(0, len(train_X), batch_size), desc='train minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(train_X))\n",
    "        batch_x, batch_es, batch_y, maxlen = batching(train_X[k: index],\n",
    "                                                     train_Y[k: index])\n",
    "        l, acc, loss, _ = sess.run([model.training_logits, model.accuracy, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                 model.source_extend_tokens:batch_es,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.source_oov_words:maxlen})\n",
    "        total_loss += loss\n",
    "        total_accuracy += acc\n",
    "        r = calculate_rouges(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_train += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc='test minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(test_X))\n",
    "        batch_x, batch_es, batch_y, maxlen = batching(test_X[k: index],\n",
    "                                                     test_Y[k: index])\n",
    "        l, acc, loss = sess.run([model.training_logits, model.accuracy, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                 model.source_extend_tokens:batch_es,\n",
    "                                                model.Y:batch_y,\n",
    "                                                model.source_oov_words:maxlen})\n",
    "        total_loss_test += loss\n",
    "        total_accuracy_test += acc\n",
    "        r = calculate_rouges(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_test += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    total_loss /= (len(train_X) / batch_size)\n",
    "    total_accuracy /= (len(train_X) / batch_size)\n",
    "    total_loss_test /= (len(test_X) / batch_size)\n",
    "    total_accuracy_test /= (len(test_X) / batch_size)\n",
    "    rouge_train /= (len(train_X) / batch_size)\n",
    "    rouge_test /= (len(test_X) / batch_size)\n",
    "        \n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(EPOCH, total_loss, total_accuracy))\n",
    "    print('epoch: %d, avg loss test: %f, avg accuracy test: %f'%(EPOCH, total_loss_test, total_accuracy_test))\n",
    "    print('epoch: %d, avg train rouge: %f, avg test rouge: %f'%(EPOCH, rouge_train, rouge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_es, batch_y, maxlen = batching(test_X[:1], test_Y[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f7(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return ' '.join([x for x in seq if not (x in seen or seen_add(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sess.run(model.beam_predictions, feed_dict = {model.X: batch_x,\n",
    "                                             model.Y: batch_y,\n",
    "                                             model.source_extend_tokens:batch_es,\n",
    "                                             model.source_oov_words:maxlen})[0]\n",
    "out = [rev_dictionary[i] for i in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['putrajaya', ',', ',', 'EOS']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'putrajaya , EOS'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f7(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rakyat',\n",
       " 'akan',\n",
       " 'sedar',\n",
       " 'kami',\n",
       " 'lebih',\n",
       " 'baik',\n",
       " 'berbanding',\n",
       " 'bn',\n",
       " ',',\n",
       " 'kata',\n",
       " 'kok',\n",
       " 'EOS']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
