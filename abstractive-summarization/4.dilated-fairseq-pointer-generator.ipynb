{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensor2tensor.utils import beam_search, rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29855"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('news-30k.json') as fopen:\n",
    "    news = json.load(fopen)\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "import re\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "\n",
    "accept_tokens = ',-.()\"\\''\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 1 or w in accept_tokens]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "def clean_label(label):\n",
    "    string = re.sub('[^A-Za-z\\- ]+', ' ', label)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29855/29855 [00:45<00:00, 663.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_len = 5\n",
    "max_len = 500\n",
    "\n",
    "x, y = [], []\n",
    "for n in tqdm(news):\n",
    "    if len(n['text'].split()) > min_len:\n",
    "        p = preprocessing(n['text'])[:max_len]\n",
    "        x.append(p)\n",
    "        p = preprocessing(n['title'])\n",
    "        y.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 88005\n",
      "Most common words [(',', 380933), ('.', 338805), ('yang', 158373), ('dan', 147862), ('di', 124501), ('-', 118778)]\n",
      "Sample data [4340, 287, 1410, 343, 1606, 114, 3583, 4, 10, 4] ['waris', 'keluarga', 'allahyarham', 'muhammad', 'haziq', 'mohd', 'tarmizi', ',', '<NUM>', ',']\n",
      "filtered vocab size: 88009\n",
      "% of vocab used: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "concat = list(itertools.chain(*x))\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n",
    "print('filtered vocab size:',len(dictionary))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary)/vocabulary_size,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary['GO']\n",
    "PAD = dictionary['PAD']\n",
    "EOS = dictionary['EOS']\n",
    "UNK = dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic, UNK=3):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i:\n",
    "            ints.append(dic.get(k, UNK))\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = str_idx(x, dictionary)\n",
    "Y = str_idx(y, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(inputs):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    repr_dim = inputs.get_shape()[-1].value\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x =  tf.layers.conv1d(inputs = tf.concat([pad, x, pad], 1),\n",
    "                          filters = hidden_dim,\n",
    "                          kernel_size = kernel_size,\n",
    "                          dilation_rate = dilation_rate)\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "class Summarization:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 dict_size, learning_rate, \n",
    "                 kernel_size = 2, n_attn_heads = 16):\n",
    "\n",
    "        self.X = tf.placeholder(tf.int32, [None, max_len])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        \n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype = tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype = tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        self.batch_size = batch_size\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        \n",
    "        self.embedding = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.size_layer = size_layer\n",
    "        self.n_attn_heads = n_attn_heads\n",
    "        self.dict_size = dict_size\n",
    "        \n",
    "        self.training_logits, coverage_loss = self.forward(self.X, decoder_input)\n",
    "        maxlen = tf.reduce_max(self.Y_seq_len)\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        \n",
    "        targets = tf.slice(self.Y, [0, 0], [-1, maxlen])\n",
    "        i1, i2 = tf.meshgrid(tf.range(batch_size),\n",
    "                     tf.range(maxlen), indexing=\"ij\")\n",
    "        indices = tf.stack((i1,i2,targets),axis=2)\n",
    "        probs = tf.gather_nd(self.training_logits, indices)\n",
    "        probs = tf.where(tf.less_equal(probs,0),tf.ones_like(probs)*1e-10,probs)\n",
    "        crossent = -tf.log(probs)\n",
    "        self.cost = tf.reduce_sum(crossent * masks) / tf.to_float(batch_size)\n",
    "        self.coverage_loss = tf.reduce_sum(coverage_loss / tf.to_float(batch_size))\n",
    "        self.cost = self.cost + self.coverage_loss\n",
    "    \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "    def _calc_final_dist(self, x, gens, vocab_dists, attn_dists):\n",
    "        with tf.variable_scope('final_distribution', reuse=tf.AUTO_REUSE):\n",
    "            vocab_dists = gens * vocab_dists\n",
    "            attn_dists = (1-gens) * attn_dists\n",
    "            batch_size = tf.shape(attn_dists)[0]\n",
    "            dec_t = tf.shape(attn_dists)[1]\n",
    "            attn_len = tf.shape(attn_dists)[2]\n",
    "\n",
    "            dec = tf.range(0, limit=dec_t) # [dec]\n",
    "            dec = tf.expand_dims(dec, axis=-1) # [dec, 1]\n",
    "            dec = tf.tile(dec, [1, attn_len]) # [dec, atten_len]\n",
    "            dec = tf.expand_dims(dec, axis=0) # [1, dec, atten_len]\n",
    "            dec = tf.tile(dec, [batch_size, 1, 1]) # [batch_size, dec, atten_len]\n",
    "\n",
    "            x = tf.expand_dims(x, axis=1) # [batch_size, 1, atten_len]\n",
    "            x = tf.tile(x, [1, dec_t, 1]) # [batch_size, dec, atten_len]\n",
    "            x = tf.stack([dec, x], axis=3)\n",
    "\n",
    "            attn_dists_projected = tf.map_fn(fn=lambda y: \\\n",
    "                                            tf.scatter_nd(y[0], y[1], [dec_t, self.dict_size]),\n",
    "                                            elems=(x, attn_dists), dtype=tf.float32)\n",
    "\n",
    "            final_dists = attn_dists_projected + vocab_dists\n",
    "            return final_dists\n",
    "    def forward(self, x, y, reuse = False):\n",
    "        with tf.variable_scope('forward',reuse=reuse):\n",
    "            with tf.variable_scope('forward',reuse=reuse):\n",
    "                encoder_embedded = tf.nn.embedding_lookup(self.embedding, x)\n",
    "                decoder_embedded = tf.nn.embedding_lookup(self.embedding, y)\n",
    "\n",
    "                encoder_embedded += position_encoding(encoder_embedded)\n",
    "                decoder_embedded += position_encoding(decoder_embedded)\n",
    "                \n",
    "                for i in range(self.num_layers): \n",
    "                    dilation_rate = 2 ** i\n",
    "                    pad_sz = (self.kernel_size - 1) * dilation_rate \n",
    "                    with tf.variable_scope('block_%d'%i,reuse=reuse):\n",
    "                        encoder_embedded += cnn_block(encoder_embedded, dilation_rate, \n",
    "                                                      pad_sz, self.size_layer, self.kernel_size)\n",
    "                \n",
    "                g = tf.identity(decoder_embedded)\n",
    "                dec = decoder_embedded\n",
    "                attn_dists = []\n",
    "                for i in range(self.num_layers):\n",
    "                    dilation_rate = 2 ** i\n",
    "                    pad_sz = (self.kernel_size - 1) * dilation_rate\n",
    "                    with tf.variable_scope('decode_%d'%i,reuse=reuse):\n",
    "                        attn_res = h = cnn_block(dec, dilation_rate, \n",
    "                                                 pad_sz, self.size_layer, self.kernel_size)\n",
    "                        C = []\n",
    "                        for j in range(self.n_attn_heads):\n",
    "                            h_ = tf.layers.dense(h, self.size_layer//self.n_attn_heads)\n",
    "                            g_ = tf.layers.dense(g, self.size_layer//self.n_attn_heads)\n",
    "                            zu_ = tf.layers.dense(encoder_embedded, self.size_layer//self.n_attn_heads)\n",
    "                            ze_ = tf.layers.dense(encoder_embedded, self.size_layer//self.n_attn_heads)\n",
    "\n",
    "                            d = tf.layers.dense(h_, self.size_layer//self.n_attn_heads) + g_\n",
    "                            dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                            a = tf.nn.softmax(dz)\n",
    "                            attn_dists.append(a)\n",
    "                            c_ = tf.matmul(a, ze_)\n",
    "                            C.append(c_)\n",
    "\n",
    "                        c = tf.concat(C, 2)\n",
    "                        h = tf.layers.dense(attn_res + c, self.size_layer)\n",
    "                        dec += h\n",
    "                \n",
    "\n",
    "                weights = tf.transpose(self.embedding)\n",
    "                logits = tf.einsum('ntd,dk->ntk', dec, weights)\n",
    "                print(decoder_embedded, dec, attn_dists[-1])\n",
    "\n",
    "                with tf.variable_scope(\"gen\", reuse=tf.AUTO_REUSE):\n",
    "                    gens = tf.layers.dense(tf.concat([decoder_embedded, dec, attn_dists[-1]], axis=-1), \n",
    "                                               units=1, activation=tf.sigmoid, use_bias=False)\n",
    "                    \n",
    "                logits = tf.nn.softmax(logits)\n",
    "                \n",
    "                print(gens)\n",
    "                alignment_history = tf.transpose(attn_dists[-1],[1,2,0])\n",
    "                coverage_loss = tf.minimum(alignment_history,tf.cumsum(alignment_history, axis=2, exclusive=True))\n",
    "                        \n",
    "                return self._calc_final_dist(x, gens, logits, attn_dists[-1]), coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 4\n",
    "embedded_size = 256\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoding(length = 20, beam_width = 5):\n",
    "    initial_ids = tf.fill([model.batch_size], GO)\n",
    "    \n",
    "    def symbols_to_logits(ids):\n",
    "        x = tf.contrib.seq2seq.tile_batch(model.X, beam_width)\n",
    "        logits, _ = model.forward(x, ids, reuse = True)\n",
    "        return logits[:, tf.shape(ids)[1]-1, :]\n",
    "\n",
    "    final_ids, final_probs = beam_search.beam_search(\n",
    "        symbols_to_logits,\n",
    "        initial_ids,\n",
    "        beam_width,\n",
    "        length,\n",
    "        len(dictionary),\n",
    "        0.0,\n",
    "        eos_id = EOS)\n",
    "    \n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"forward/forward/add_1:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"forward/forward/decode_3/add_19:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"forward/forward/decode_3/Reshape_31:0\", shape=(?, ?, 500), dtype=float32)\n",
      "Tensor(\"forward/forward/gen/dense/Sigmoid:0\", shape=(?, ?, 1), dtype=float32)\n",
      "Tensor(\"while/forward/forward/add_1:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"while/forward/forward/decode_3/add_19:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"while/forward/forward/decode_3/Reshape_31:0\", shape=(?, ?, 500), dtype=float32)\n",
      "Tensor(\"while/forward/forward/gen/dense/Sigmoid:0\", shape=(?, ?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Summarization(size_layer, num_layers, embedded_size, \n",
    "                      len(dictionary), learning_rate)\n",
    "model.generate = beam_search_decoding()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int, maxlen = None):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    if maxlen:\n",
    "        max_sentence_len = maxlen\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:47<00:00,  3.35it/s, accuracy=0.267, cost=64.9, rouge_2=0.154]  \n",
      "test minibatch loop: 100%|██████████| 187/187 [00:25<00:00,  7.68it/s, accuracy=0.275, cost=99.7, rouge_2=0.142]\n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg loss: 86.886271, avg accuracy: 0.223780\n",
      "epoch: 0, avg loss test: 81.606984, avg accuracy test: 0.290207\n",
      "epoch: 0, avg train rouge: 0.107952, avg test rouge: 0.185198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:43<00:00,  3.66it/s, accuracy=0.378, cost=58.7, rouge_2=0.259]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.42it/s, accuracy=0.343, cost=96.5, rouge_2=0.218] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 78.740744, avg accuracy: 0.330643\n",
      "epoch: 1, avg loss test: 79.704315, avg accuracy test: 0.322280\n",
      "epoch: 1, avg train rouge: 0.207342, avg test rouge: 0.204983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.70it/s, accuracy=0.556, cost=54.3, rouge_2=0.343]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.31it/s, accuracy=0.363, cost=96.1, rouge_2=0.22] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, avg loss: 75.497073, avg accuracy: 0.382726\n",
      "epoch: 2, avg loss test: 79.292818, avg accuracy test: 0.336576\n",
      "epoch: 2, avg train rouge: 0.240604, avg test rouge: 0.214688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:41<00:00,  3.70it/s, accuracy=0.644, cost=51.2, rouge_2=0.438]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.55it/s, accuracy=0.373, cost=96.3, rouge_2=0.229]\n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, avg loss: 72.983652, avg accuracy: 0.426988\n",
      "epoch: 3, avg loss test: 79.604103, avg accuracy test: 0.343427\n",
      "epoch: 3, avg train rouge: 0.271075, avg test rouge: 0.222683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:41<00:00,  3.71it/s, accuracy=0.667, cost=49.6, rouge_2=0.456]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.39it/s, accuracy=0.402, cost=96.9, rouge_2=0.263]\n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, avg loss: 70.839966, avg accuracy: 0.468914\n",
      "epoch: 4, avg loss test: 80.619893, avg accuracy test: 0.347623\n",
      "epoch: 4, avg train rouge: 0.302369, avg test rouge: 0.229399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.72it/s, accuracy=0.689, cost=48.1, rouge_2=0.466]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.44it/s, accuracy=0.382, cost=98.4, rouge_2=0.267]\n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, avg loss: 69.008020, avg accuracy: 0.506389\n",
      "epoch: 5, avg loss test: 82.161340, avg accuracy test: 0.347463\n",
      "epoch: 5, avg train rouge: 0.332098, avg test rouge: 0.228164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.71it/s, accuracy=0.711, cost=47.7, rouge_2=0.494]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.98it/s, accuracy=0.363, cost=102, rouge_2=0.242] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, avg loss: 67.498303, avg accuracy: 0.538135\n",
      "epoch: 6, avg loss test: 83.989594, avg accuracy test: 0.345328\n",
      "epoch: 6, avg train rouge: 0.359017, avg test rouge: 0.229533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.62it/s, accuracy=0.711, cost=47.7, rouge_2=0.534]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.49it/s, accuracy=0.392, cost=103, rouge_2=0.263] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, avg loss: 66.321964, avg accuracy: 0.562008\n",
      "epoch: 7, avg loss test: 86.405367, avg accuracy test: 0.340035\n",
      "epoch: 7, avg train rouge: 0.379941, avg test rouge: 0.225653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.70it/s, accuracy=0.733, cost=47.4, rouge_2=0.529]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.41it/s, accuracy=0.431, cost=105, rouge_2=0.298] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, avg loss: 65.396839, avg accuracy: 0.582453\n",
      "epoch: 8, avg loss test: 88.289230, avg accuracy test: 0.345654\n",
      "epoch: 8, avg train rouge: 0.398326, avg test rouge: 0.231356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.71it/s, accuracy=0.733, cost=47.7, rouge_2=0.516]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.51it/s, accuracy=0.382, cost=107, rouge_2=0.235] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, avg loss: 64.631575, avg accuracy: 0.599115\n",
      "epoch: 9, avg loss test: 90.864960, avg accuracy test: 0.343545\n",
      "epoch: 9, avg train rouge: 0.413573, avg test rouge: 0.232010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.69it/s, accuracy=0.689, cost=47.7, rouge_2=0.467]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.61it/s, accuracy=0.392, cost=111, rouge_2=0.237] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, avg loss: 63.980870, avg accuracy: 0.615343\n",
      "epoch: 10, avg loss test: 92.486121, avg accuracy test: 0.348880\n",
      "epoch: 10, avg train rouge: 0.429714, avg test rouge: 0.235515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.71it/s, accuracy=0.733, cost=48.3, rouge_2=0.534]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.96it/s, accuracy=0.373, cost=113, rouge_2=0.256] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, avg loss: 63.571819, avg accuracy: 0.625877\n",
      "epoch: 11, avg loss test: 94.316892, avg accuracy test: 0.350494\n",
      "epoch: 11, avg train rouge: 0.439594, avg test rouge: 0.237123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.66it/s, accuracy=0.733, cost=47.7, rouge_2=0.534]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.48it/s, accuracy=0.412, cost=115, rouge_2=0.276] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, avg loss: 63.195373, avg accuracy: 0.636367\n",
      "epoch: 12, avg loss test: 95.957268, avg accuracy test: 0.357102\n",
      "epoch: 12, avg train rouge: 0.450115, avg test rouge: 0.241499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.68it/s, accuracy=0.756, cost=47.2, rouge_2=0.534]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.54it/s, accuracy=0.402, cost=117, rouge_2=0.274] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, avg loss: 62.964358, avg accuracy: 0.643115\n",
      "epoch: 13, avg loss test: 96.862583, avg accuracy test: 0.355776\n",
      "epoch: 13, avg train rouge: 0.456431, avg test rouge: 0.243206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.67it/s, accuracy=0.733, cost=48.2, rouge_2=0.534]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.50it/s, accuracy=0.392, cost=115, rouge_2=0.254] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, avg loss: 62.759829, avg accuracy: 0.649096\n",
      "epoch: 14, avg loss test: 99.870215, avg accuracy test: 0.356450\n",
      "epoch: 14, avg train rouge: 0.458547, avg test rouge: 0.242300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.71it/s, accuracy=0.756, cost=47.2, rouge_2=0.539]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.96it/s, accuracy=0.412, cost=115, rouge_2=0.267] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, avg loss: 62.570542, avg accuracy: 0.654845\n",
      "epoch: 15, avg loss test: 101.179394, avg accuracy test: 0.362391\n",
      "epoch: 15, avg train rouge: 0.469231, avg test rouge: 0.247402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.69it/s, accuracy=0.689, cost=50.6, rouge_2=0.494]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.52it/s, accuracy=0.402, cost=119, rouge_2=0.267] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, avg loss: 62.424470, avg accuracy: 0.659636\n",
      "epoch: 16, avg loss test: 102.360128, avg accuracy test: 0.355939\n",
      "epoch: 16, avg train rouge: 0.475005, avg test rouge: 0.244429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.65it/s, accuracy=0.689, cost=49.4, rouge_2=0.444]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.95it/s, accuracy=0.412, cost=120, rouge_2=0.276] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, avg loss: 62.314220, avg accuracy: 0.662996\n",
      "epoch: 17, avg loss test: 103.197917, avg accuracy test: 0.360235\n",
      "epoch: 17, avg train rouge: 0.477931, avg test rouge: 0.246485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.66it/s, accuracy=0.711, cost=48.6, rouge_2=0.469]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  7.97it/s, accuracy=0.392, cost=120, rouge_2=0.275] \n",
      "train minibatch loop:   0%|          | 0/1680 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, avg loss: 62.291030, avg accuracy: 0.664689\n",
      "epoch: 18, avg loss test: 105.030493, avg accuracy test: 0.361679\n",
      "epoch: 18, avg train rouge: 0.467598, avg test rouge: 0.248399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 1680/1680 [08:42<00:00,  3.69it/s, accuracy=0.756, cost=46.4, rouge_2=0.544]\n",
      "test minibatch loop: 100%|██████████| 187/187 [00:23<00:00,  8.50it/s, accuracy=0.422, cost=121, rouge_2=0.288] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, avg loss: 62.148162, avg accuracy: 0.668568\n",
      "epoch: 19, avg loss test: 105.179892, avg accuracy test: 0.363310\n",
      "epoch: 19, avg train rouge: 0.469557, avg test rouge: 0.248378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "for EPOCH in range(20):\n",
    "    lasttime = time.time()\n",
    "    total_loss, total_accuracy, total_loss_test, total_accuracy_test = 0, 0, 0, 0\n",
    "    rouge_train, rouge_test = 0, 0\n",
    "    pbar = tqdm(range(0, len(train_X), batch_size), desc='train minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(train_X))\n",
    "        batch_x, _ = pad_sentence_batch(train_X[k: index], PAD, maxlen = max_len)\n",
    "        batch_y, _ = pad_sentence_batch(train_Y[k: index], PAD)\n",
    "        l, acc, loss, _ = sess.run([model.training_logits, model.accuracy, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss += loss\n",
    "        total_accuracy += acc\n",
    "        r = rouge.rouge_n(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_train += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc='test minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(test_X))\n",
    "        batch_x, _ = pad_sentence_batch(test_X[k: index], PAD, maxlen = max_len)\n",
    "        batch_y, _ = pad_sentence_batch(test_Y[k: index], PAD)\n",
    "        l, acc, loss = sess.run([model.training_logits, model.accuracy, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss_test += loss\n",
    "        total_accuracy_test += acc\n",
    "        r = rouge.rouge_n(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_test += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    total_loss /= (len(train_X) / batch_size)\n",
    "    total_accuracy /= (len(train_X) / batch_size)\n",
    "    total_loss_test /= (len(test_X) / batch_size)\n",
    "    total_accuracy_test /= (len(test_X) / batch_size)\n",
    "    rouge_train /= (len(train_X) / batch_size)\n",
    "    rouge_test /= (len(test_X) / batch_size)\n",
    "        \n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(EPOCH, total_loss, total_accuracy))\n",
    "    print('epoch: %d, avg loss test: %f, avg accuracy test: %f'%(EPOCH, total_loss_test, total_accuracy_test))\n",
    "    print('epoch: %d, avg train rouge: %f, avg test rouge: %f'%(EPOCH, rouge_train, rouge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
