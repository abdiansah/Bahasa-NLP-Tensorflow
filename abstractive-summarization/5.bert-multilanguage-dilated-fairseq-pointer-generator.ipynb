{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "# !multi_cased_L-12_H-768_A-12.zip\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/news-30k/news-30k.json.zip\n",
    "# !unzip news-30k.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import beam_search, rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from unidecode import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29855"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('news-30k.json') as fopen:\n",
    "    news = json.load(fopen)\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:26: DeprecationWarning: invalid escape sequence \\-\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:26: DeprecationWarning: invalid escape sequence \\-\n",
      "<>:8: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:26: DeprecationWarning: invalid escape sequence \\-\n",
      "<ipython-input-5-85c9669409f4>:8: DeprecationWarning: invalid escape sequence \\d\n",
      "  if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
      "<ipython-input-5-85c9669409f4>:26: DeprecationWarning: invalid escape sequence \\-\n",
      "  string = re.sub('[^A-Za-z\\- ]+', ' ', label)\n"
     ]
    }
   ],
   "source": [
    "import malaya\n",
    "import re\n",
    "tokenizer = malaya.preprocessing.SocialTokenizer().tokenize\n",
    "\n",
    "accept_tokens = ',-.()\"\\''\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    tokenized = tokenizer(string)\n",
    "    tokenized = [w.lower() for w in tokenized if len(w) > 1 or w in accept_tokens]\n",
    "    tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "    tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "def clean_label(label):\n",
    "    string = re.sub('[^A-Za-z\\- ]+', ' ', label)\n",
    "    return re.sub(r'[ ]+', ' ', string.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB = 'multi_cased_L-12_H-768_A-12/vocab.txt'\n",
    "BERT_INIT_CHKPNT = 'multi_cased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = 'multi_cased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29855/29855 [01:04<00:00, 459.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_len = 5\n",
    "max_len = 400\n",
    "\n",
    "x, y = [], []\n",
    "for n in tqdm(news):\n",
    "    if len(n['text'].split()) > min_len:\n",
    "        p = preprocessing(n['text'])[:max_len]\n",
    "        x.append(p)\n",
    "        p = preprocessing(n['title'])\n",
    "        y.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "def build_dataset(words, n_words, atleast=1):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 85652\n",
      "Most common words [(',', 362826), ('.', 320679), ('yang', 149889), ('dan', 139864), ('di', 119719), ('-', 112657)]\n",
      "Sample data [4340, 288, 1409, 340, 1580, 110, 3480, 4, 10, 4] ['waris', 'keluarga', 'allahyarham', 'muhammad', 'haziq', 'mohd', 'tarmizi', ',', '<NUM>', ',']\n",
      "filtered vocab size: 85656\n",
      "% of vocab used: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "concat = list(itertools.chain(*x))\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n",
    "print('filtered vocab size:',len(dictionary))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary)/vocabulary_size,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 400\n",
    "\n",
    "tokenization.validate_case_matches_checkpoint(False,BERT_INIT_CHKPNT)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_VOCAB, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29855/29855 [02:31<00:00, 197.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_ids, input_masks, segment_ids = [], [], []\n",
    "\n",
    "for text in tqdm(x):\n",
    "    tokens_a = tokenizer.tokenize(' '.join(text))\n",
    "    if len(tokens_a) > MAX_SEQ_LENGTH - 2:\n",
    "        tokens_a = tokens_a[:(MAX_SEQ_LENGTH - 2)]\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "    segment_id = [0] * len(tokens)\n",
    "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_id)\n",
    "    padding = [0] * (MAX_SEQ_LENGTH - len(input_id))\n",
    "    input_id += padding\n",
    "    input_mask += padding\n",
    "    segment_id += padding\n",
    "    \n",
    "    input_ids.append(input_id)\n",
    "    input_masks.append(input_mask)\n",
    "    segment_ids.append(segment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = dictionary['GO']\n",
    "PAD = dictionary['PAD']\n",
    "EOS = dictionary['EOS']\n",
    "UNK = dictionary['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i].append('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "epoch = 20\n",
    "batch_size = 5\n",
    "warmup_proportion = 0.1\n",
    "num_train_steps = int(len(input_ids) / batch_size * epoch)\n",
    "num_warmup_steps = int(num_train_steps * warmup_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_seq(x, vocab_sz, embed_dim, name, zero_pad=True): \n",
    "    embedding = tf.get_variable(name, [vocab_sz, embed_dim]) \n",
    "    if zero_pad:\n",
    "        embedding = tf.concat([tf.zeros([1, embed_dim]), embedding[1:, :]], 0) \n",
    "    x = tf.nn.embedding_lookup(embedding, x)\n",
    "    return x\n",
    "\n",
    "def position_encoding(inputs):\n",
    "    T = tf.shape(inputs)[1]\n",
    "    repr_dim = inputs.get_shape()[-1].value\n",
    "    pos = tf.reshape(tf.range(0.0, tf.to_float(T), dtype=tf.float32), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x =  tf.layers.conv1d(inputs = tf.concat([pad, x, pad], 1),\n",
    "                          filters = hidden_dim,\n",
    "                          kernel_size = kernel_size,\n",
    "                          dilation_rate = dilation_rate)\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size, \n",
    "                 to_dict_size, learning_rate, kernel_size = 2, n_attn_heads = 16):\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, max_len])\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.X_seq_len = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        self.embedding = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.size_layer = size_layer\n",
    "        self.n_attn_heads = n_attn_heads\n",
    "        self.dict_size = to_dict_size\n",
    "        \n",
    "        self.training_logits, coverage_loss = self.forward(self.X, decoder_input)\n",
    "        maxlen = tf.reduce_max(self.Y_seq_len)\n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        \n",
    "        targets = tf.slice(self.Y, [0, 0], [-1, maxlen])\n",
    "        i1, i2 = tf.meshgrid(tf.range(batch_size),\n",
    "                     tf.range(maxlen), indexing=\"ij\")\n",
    "        indices = tf.stack((i1,i2,targets),axis=2)\n",
    "        probs = tf.gather_nd(self.training_logits, indices)\n",
    "        probs = tf.where(tf.less_equal(probs,0),tf.ones_like(probs)*1e-10,probs)\n",
    "        crossent = -tf.log(probs)\n",
    "        self.cost = tf.reduce_sum(crossent * masks) / tf.to_float(batch_size)\n",
    "        self.coverage_loss = tf.reduce_sum(coverage_loss / tf.to_float(batch_size))\n",
    "        self.cost = self.cost + self.coverage_loss\n",
    "    \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    def _calc_final_dist(self, x, gens, vocab_dists, attn_dists):\n",
    "        with tf.variable_scope('final_distribution', reuse=tf.AUTO_REUSE):\n",
    "            vocab_dists = gens * vocab_dists\n",
    "            attn_dists = (1-gens) * attn_dists\n",
    "            batch_size = tf.shape(attn_dists)[0]\n",
    "            dec_t = tf.shape(attn_dists)[1]\n",
    "            attn_len = tf.shape(attn_dists)[2]\n",
    "\n",
    "            dec = tf.range(0, limit=dec_t) # [dec]\n",
    "            dec = tf.expand_dims(dec, axis=-1) # [dec, 1]\n",
    "            dec = tf.tile(dec, [1, attn_len]) # [dec, atten_len]\n",
    "            dec = tf.expand_dims(dec, axis=0) # [1, dec, atten_len]\n",
    "            dec = tf.tile(dec, [batch_size, 1, 1]) # [batch_size, dec, atten_len]\n",
    "\n",
    "            x = tf.expand_dims(x, axis=1) # [batch_size, 1, atten_len]\n",
    "            x = tf.tile(x, [1, dec_t, 1]) # [batch_size, dec, atten_len]\n",
    "            x = tf.stack([dec, x], axis=3)\n",
    "\n",
    "            attn_dists_projected = tf.map_fn(fn=lambda y: \\\n",
    "                                            tf.scatter_nd(y[0], y[1], [dec_t, self.dict_size]),\n",
    "                                            elems=(x, attn_dists), dtype=tf.float32)\n",
    "\n",
    "            final_dists = attn_dists_projected + vocab_dists\n",
    "            return final_dists\n",
    "        \n",
    "    def forward(self, x, y, reuse = False):\n",
    "        with tf.variable_scope('forward',reuse=reuse):\n",
    "            with tf.variable_scope('forward',reuse=reuse):\n",
    "                model = modeling.BertModel(\n",
    "                config=bert_config,\n",
    "                is_training=True,\n",
    "                input_ids=x,\n",
    "                use_one_hot_embeddings=False)\n",
    "                encoder_embedded = model.get_sequence_output()\n",
    "                decoder_embedded = tf.nn.embedding_lookup(self.embedding, y)\n",
    "                decoder_embedded += position_encoding(decoder_embedded)\n",
    "                \n",
    "                g = tf.identity(decoder_embedded)\n",
    "                dec = decoder_embedded\n",
    "                attn_dists = []\n",
    "                for i in range(self.num_layers):\n",
    "                    dilation_rate = 2 ** i\n",
    "                    pad_sz = (self.kernel_size - 1) * dilation_rate\n",
    "                    with tf.variable_scope('decode_%d'%i,reuse=reuse):\n",
    "                        attn_res = h = cnn_block(dec, dilation_rate, \n",
    "                                                 pad_sz, self.size_layer, self.kernel_size)\n",
    "                        C = []\n",
    "                        for j in range(self.n_attn_heads):\n",
    "                            h_ = tf.layers.dense(h, self.size_layer//self.n_attn_heads)\n",
    "                            g_ = tf.layers.dense(g, self.size_layer//self.n_attn_heads)\n",
    "                            zu_ = tf.layers.dense(encoder_embedded, self.size_layer//self.n_attn_heads)\n",
    "                            ze_ = tf.layers.dense(encoder_embedded, self.size_layer//self.n_attn_heads)\n",
    "\n",
    "                            d = tf.layers.dense(h_, self.size_layer//self.n_attn_heads) + g_\n",
    "                            dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                            a = tf.nn.softmax(dz)\n",
    "                            attn_dists.append(a)\n",
    "                            c_ = tf.matmul(a, ze_)\n",
    "                            C.append(c_)\n",
    "\n",
    "                        c = tf.concat(C, 2)\n",
    "                        h = tf.layers.dense(attn_res + c, self.size_layer)\n",
    "                        dec += h\n",
    "\n",
    "                weights = tf.transpose(self.embedding)\n",
    "                logits = tf.einsum('ntd,dk->ntk', dec, weights)\n",
    "                print(decoder_embedded, dec, attn_dists[-1])\n",
    "\n",
    "                with tf.variable_scope(\"gen\", reuse=tf.AUTO_REUSE):\n",
    "                    gens = tf.layers.dense(tf.concat([decoder_embedded, dec, attn_dists[-1]], axis=-1), \n",
    "                                               units=1, activation=tf.sigmoid, use_bias=False)\n",
    "                    \n",
    "                logits = tf.nn.softmax(logits)\n",
    "                \n",
    "                print(gens)\n",
    "                alignment_history = tf.transpose(attn_dists[-1],[1,2,0])\n",
    "                coverage_loss = tf.minimum(alignment_history,tf.cumsum(alignment_history, axis=2, exclusive=True))\n",
    "                        \n",
    "                return self._calc_final_dist(x, gens, logits, attn_dists[-1]), coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 256\n",
    "num_layers = 4\n",
    "embedded_size = 256\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoding(length = 20, beam_width = 5):\n",
    "    initial_ids = tf.fill([model.batch_size], GO)\n",
    "    \n",
    "    def symbols_to_logits(ids):\n",
    "        x = tf.contrib.seq2seq.tile_batch(model.X, beam_width)\n",
    "        logits, _ = model.forward(x, ids, reuse = True)\n",
    "        return logits[:, tf.shape(ids)[1]-1, :]\n",
    "\n",
    "    final_ids, final_probs, _ = beam_search.beam_search(\n",
    "        symbols_to_logits,\n",
    "        initial_ids,\n",
    "        beam_width,\n",
    "        length,\n",
    "        len(dictionary),\n",
    "        0.0,\n",
    "        eos_id = EOS)\n",
    "    \n",
    "    return final_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"forward/forward/add:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"forward/forward/decode_3/add_19:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"forward/forward/decode_3/Softmax_15:0\", shape=(?, ?, 400), dtype=float32)\n",
      "Tensor(\"forward/forward/gen/dense/Sigmoid:0\", shape=(?, ?, 1), dtype=float32)\n",
      "Tensor(\"while/forward/forward/add:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"while/forward/forward/decode_3/add_19:0\", shape=(?, ?, 256), dtype=float32) Tensor(\"while/forward/forward/decode_3/Softmax_15:0\", shape=(?, ?, 400), dtype=float32)\n",
      "Tensor(\"while/forward/forward/gen/dense/Sigmoid:0\", shape=(?, ?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Translator(size_layer, num_layers, embedded_size, \n",
    "                len(dictionary), learning_rate)\n",
    "model.generate = beam_search_decoding()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "    \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
    "    assignment_map = {}\n",
    "    initialized_variable_names = {}\n",
    "\n",
    "    name_to_variable = collections.OrderedDict()\n",
    "    for var in tvars:\n",
    "        name = var.name\n",
    "        m = re.match('^(.*):\\\\d+$', name)\n",
    "        if m is not None:\n",
    "            name = m.group(1)\n",
    "        name_to_variable[name] = var\n",
    "\n",
    "    init_vars = tf.train.list_variables(init_checkpoint)\n",
    "\n",
    "    assignment_map = collections.OrderedDict()\n",
    "    for x in init_vars:\n",
    "        (name, var) = (x[0], x[1])\n",
    "        if 'bert' not in name:\n",
    "            continue\n",
    "        assignment_map[name] = name_to_variable['forward/forward/'+name]\n",
    "        initialized_variable_names[name] = 1\n",
    "        initialized_variable_names[name + ':0'] = 1\n",
    "\n",
    "    return (assignment_map, initialized_variable_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "assignment_map, initialized_variable_names = get_assignment_map_from_checkpoint(tvars, \n",
    "                                                                                BERT_INIT_CHKPNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from multi_cased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(var_list = assignment_map)\n",
    "saver.restore(sess, BERT_INIT_CHKPNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_idx(corpus, dic):\n",
    "    X = []\n",
    "    for i in corpus:\n",
    "        ints = []\n",
    "        for k in i:\n",
    "            ints.append(dic.get(k,UNK))\n",
    "        X.append(ints)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = str_idx(y, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(input_ids, Y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    1, 36811, 36811, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635],\n",
       "        [    1, 36811, 36811, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635, 75635, 38993, 70602, 70602, 70602, 70602,\n",
       "         70602, 70602, 70602],\n",
       "        [    1, 36811, 36811, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635, 38541, 38541, 38541, 38541, 38541, 38541,\n",
       "         38541, 38541, 38541],\n",
       "        [    1, 36811, 36811, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 48340],\n",
       "        [    1, 36811, 36811, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635, 75635,\n",
       "         75635, 75635, 47233]]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(model.generate, feed_dict = {model.X: [test_X[0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "def calculate_rouges(predicted, batch_y):\n",
    "    non = np.count_nonzero(batch_y, axis = 1)\n",
    "    o = []\n",
    "    for n in non:\n",
    "        o.append([True for _ in range(n)])\n",
    "    b = sequence.pad_sequences(o, dtype = np.bool, padding = 'post', value = False)\n",
    "    batch_y = np.array(batch_y)\n",
    "    rouges = []\n",
    "    for i in range(predicted.shape[0]):\n",
    "        a = batch_y[i][b[i]]\n",
    "        p = predicted[i][b[i]]\n",
    "        rouges.append(rouge.rouge_n([p], [a]))\n",
    "    return np.mean(rouges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:36<00:00,  1.36s/it, accuracy=0.0811, cost=184, rouge_2=0.118]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:03<00:00,  2.50it/s, accuracy=0, cost=174, rouge_2=0]          \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg loss: 279.728556, avg accuracy: 0.046760\n",
      "epoch: 0, avg loss test: 192.069527, avg accuracy test: 0.059605\n",
      "epoch: 0, avg train rouge: 0.145015, avg test rouge: 0.062441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:17<00:00,  1.34s/it, accuracy=0.0811, cost=132, rouge_2=0.0857]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:01<00:00,  2.47it/s, accuracy=0, cost=99.7, rouge_2=0]          \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, avg loss: 159.724705, avg accuracy: 0.077480\n",
      "epoch: 1, avg loss test: 136.929695, avg accuracy test: 0.085008\n",
      "epoch: 1, avg train rouge: 0.035897, avg test rouge: 0.039846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:29<00:00,  1.34s/it, accuracy=0.189, cost=103, rouge_2=0.0857]    \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:59<00:00,  2.44it/s, accuracy=0.222, cost=67.9, rouge_2=0]      \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, avg loss: 120.742791, avg accuracy: 0.113831\n",
      "epoch: 2, avg loss test: 111.724015, avg accuracy test: 0.120622\n",
      "epoch: 2, avg train rouge: 0.031655, avg test rouge: 0.038480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:05:55<00:00,  1.34s/it, accuracy=0.189, cost=86.7, rouge_2=0.05]     \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:59<00:00,  2.40it/s, accuracy=0.444, cost=22.3, rouge_2=0.25]   \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, avg loss: 98.622402, avg accuracy: 0.162444\n",
      "epoch: 3, avg loss test: 95.355379, avg accuracy test: 0.174597\n",
      "epoch: 3, avg train rouge: 0.048908, avg test rouge: 0.066184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:   0%|          | 4/5374 [00:05<2:05:31,  1.40s/it, accuracy=0.0968, cost=67.3, rouge_2=0]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop:  64%|██████▎   | 3420/5374 [1:20:15<45:51,  1.41s/it, accuracy=0.306, cost=55.4, rouge_2=0.22]     IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:17<00:00,  1.34s/it, accuracy=0.216, cost=81.8, rouge_2=0.0685] \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:01<00:00,  2.50it/s, accuracy=1, cost=4.37, rouge_2=1]          \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, avg loss: 82.856719, avg accuracy: 0.220810\n",
      "epoch: 4, avg loss test: 84.591152, avg accuracy test: 0.223030\n",
      "epoch: 4, avg train rouge: 0.088370, avg test rouge: 0.096655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:45<00:00,  1.35s/it, accuracy=0.243, cost=70.4, rouge_2=0]        \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:03<00:00,  2.40it/s, accuracy=0.889, cost=2.5, rouge_2=0.75]    \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, avg loss: 70.802102, avg accuracy: 0.275528\n",
      "epoch: 5, avg loss test: 77.260227, avg accuracy test: 0.257529\n",
      "epoch: 5, avg train rouge: 0.128788, avg test rouge: 0.123859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:16<00:00,  1.33s/it, accuracy=0.297, cost=64, rouge_2=0.0549]    \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:00<00:00,  2.47it/s, accuracy=1, cost=0.331, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, avg loss: 61.001875, avg accuracy: 0.324600\n",
      "epoch: 6, avg loss test: 72.770369, avg accuracy test: 0.280650\n",
      "epoch: 6, avg train rouge: 0.168213, avg test rouge: 0.145642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:25<00:00,  1.35s/it, accuracy=0.297, cost=55.4, rouge_2=0.0542]  \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:01<00:00,  2.46it/s, accuracy=1, cost=0.533, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, avg loss: 52.677047, avg accuracy: 0.369189\n",
      "epoch: 7, avg loss test: 70.335309, avg accuracy test: 0.302585\n",
      "epoch: 7, avg train rouge: 0.203515, avg test rouge: 0.162327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:28<00:00,  1.35s/it, accuracy=0.486, cost=52.4, rouge_2=0.253]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:01<00:00,  2.46it/s, accuracy=1, cost=0.426, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, avg loss: 45.481930, avg accuracy: 0.413688\n",
      "epoch: 8, avg loss test: 68.989110, avg accuracy test: 0.313892\n",
      "epoch: 8, avg train rouge: 0.239550, avg test rouge: 0.173769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:42<00:00,  1.35s/it, accuracy=0.459, cost=43.6, rouge_2=0.233]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:03<00:00,  2.45it/s, accuracy=1, cost=0.185, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, avg loss: 39.179898, avg accuracy: 0.456189\n",
      "epoch: 9, avg loss test: 68.158611, avg accuracy test: 0.309600\n",
      "epoch: 9, avg train rouge: 0.274925, avg test rouge: 0.178431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:45<00:00,  1.34s/it, accuracy=0.514, cost=33.7, rouge_2=0.319]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:58<00:00,  2.46it/s, accuracy=1, cost=0.113, rouge_2=1]        \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, avg loss: 33.258062, avg accuracy: 0.504054\n",
      "epoch: 10, avg loss test: 67.311925, avg accuracy test: 0.322226\n",
      "epoch: 10, avg train rouge: 0.318112, avg test rouge: 0.190904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:34<00:00,  1.34s/it, accuracy=0.541, cost=26.3, rouge_2=0.306]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:01<00:00,  2.50it/s, accuracy=1, cost=0.0634, rouge_2=1]        \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, avg loss: 28.219660, avg accuracy: 0.548686\n",
      "epoch: 11, avg loss test: 67.920936, avg accuracy test: 0.330781\n",
      "epoch: 11, avg train rouge: 0.361645, avg test rouge: 0.200074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:19<00:00,  1.34s/it, accuracy=0.568, cost=21.6, rouge_2=0.268]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:59<00:00,  2.43it/s, accuracy=0.889, cost=3.43, rouge_2=0.75]   \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, avg loss: 23.527363, avg accuracy: 0.593937\n",
      "epoch: 12, avg loss test: 69.472376, avg accuracy test: 0.330330\n",
      "epoch: 12, avg train rouge: 0.403499, avg test rouge: 0.202501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:33<00:00,  1.35s/it, accuracy=0.676, cost=15.8, rouge_2=0.507]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:00<00:00,  2.52it/s, accuracy=0.889, cost=1.47, rouge_2=0.75]  \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, avg loss: 19.328228, avg accuracy: 0.639390\n",
      "epoch: 13, avg loss test: 71.438703, avg accuracy test: 0.339273\n",
      "epoch: 13, avg train rouge: 0.452059, avg test rouge: 0.208727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:33<00:00,  1.34s/it, accuracy=0.73, cost=12.4, rouge_2=0.487]   \n",
      "test minibatch loop: 100%|██████████| 598/598 [05:00<00:00,  2.46it/s, accuracy=1, cost=0.409, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, avg loss: 15.666254, avg accuracy: 0.683783\n",
      "epoch: 14, avg loss test: 73.400504, avg accuracy test: 0.348599\n",
      "epoch: 14, avg train rouge: 0.501307, avg test rouge: 0.213838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:57<00:00,  1.36s/it, accuracy=0.811, cost=7.39, rouge_2=0.677]  \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:55<00:00,  2.51it/s, accuracy=1, cost=0.486, rouge_2=1]         \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, avg loss: 12.631437, avg accuracy: 0.723540\n",
      "epoch: 15, avg loss test: 75.934246, avg accuracy test: 0.344803\n",
      "epoch: 15, avg train rouge: 0.548946, avg test rouge: 0.213791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:  13%|█▎        | 693/5374 [16:24<1:50:43,  1.42s/it, accuracy=0.857, cost=8.29, rouge_2=0.687] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop:  73%|███████▎  | 3947/5374 [1:33:09<33:37,  1.41s/it, accuracy=0.763, cost=8.89, rouge_2=0.559]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "test minibatch loop: 100%|██████████| 598/598 [04:58<00:00,  2.49it/s, accuracy=0.889, cost=4.25, rouge_2=0.75]  \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, avg loss: 10.116217, avg accuracy: 0.762534\n",
      "epoch: 16, avg loss test: 77.958384, avg accuracy test: 0.355144\n",
      "epoch: 16, avg train rouge: 0.599804, avg test rouge: 0.222278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:  25%|██▍       | 1341/5374 [31:27<1:34:03,  1.40s/it, accuracy=0.795, cost=5.49, rouge_2=0.693]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop:  86%|████████▋ | 4639/5374 [1:49:05<17:23,  1.42s/it, accuracy=0.745, cost=10.8, rouge_2=0.495]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop:  38%|███▊      | 2058/5374 [48:36<1:18:27,  1.42s/it, accuracy=0.732, cost=9.74, rouge_2=0.502] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "train minibatch loop: 100%|██████████| 5374/5374 [2:06:51<00:00,  1.34s/it, accuracy=0.892, cost=3.79, rouge_2=0.807] \n",
      "test minibatch loop: 100%|██████████| 598/598 [04:58<00:00,  2.52it/s, accuracy=1, cost=0.189, rouge_2=1]        \n",
      "train minibatch loop:   0%|          | 0/5374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, avg loss: 6.683461, avg accuracy: 0.824960\n",
      "epoch: 18, avg loss test: 81.337019, avg accuracy test: 0.360266\n",
      "epoch: 18, avg train rouge: 0.690060, avg test rouge: 0.230481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop:  81%|████████  | 4366/5374 [1:43:03<23:46,  1.41s/it, accuracy=0.94, cost=4.71, rouge_2=0.877]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "for EPOCH in range(20):\n",
    "    lasttime = time.time()\n",
    "    total_loss, total_accuracy, total_loss_test, total_accuracy_test = 0, 0, 0, 0\n",
    "    rouge_train, rouge_test = 0, 0\n",
    "    pbar = tqdm(range(0, len(train_X), batch_size), desc='train minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(train_X))\n",
    "        batch_x = train_X[k: index]\n",
    "        batch_y, _ = pad_sentence_batch(train_Y[k: index], PAD)\n",
    "        l, acc, loss, _ = sess.run([model.training_logits, model.accuracy, model.cost, model.optimizer], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss += loss\n",
    "        total_accuracy += acc\n",
    "        r = calculate_rouges(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_train += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    pbar = tqdm(range(0, len(test_X), batch_size), desc='test minibatch loop')\n",
    "    for k in pbar:\n",
    "        index = min(k+batch_size,len(test_X))\n",
    "        batch_x = test_X[k: index]\n",
    "        batch_y, _ = pad_sentence_batch(test_Y[k: index], PAD)\n",
    "        l, acc, loss = sess.run([model.training_logits, model.accuracy, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "        total_loss_test += loss\n",
    "        total_accuracy_test += acc\n",
    "        r = calculate_rouges(np.argmax(l, axis = 2), batch_y)\n",
    "        rouge_test += r\n",
    "        pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "        \n",
    "    total_loss /= (len(train_X) / batch_size)\n",
    "    total_accuracy /= (len(train_X) / batch_size)\n",
    "    total_loss_test /= (len(test_X) / batch_size)\n",
    "    total_accuracy_test /= (len(test_X) / batch_size)\n",
    "    rouge_train /= (len(train_X) / batch_size)\n",
    "    rouge_test /= (len(test_X) / batch_size)\n",
    "        \n",
    "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(EPOCH, total_loss, total_accuracy))\n",
    "    print('epoch: %d, avg loss test: %f, avg accuracy test: %f'%(EPOCH, total_loss_test, total_accuracy_test))\n",
    "    print('epoch: %d, avg train rouge: %f, avg test rouge: %f'%(EPOCH, rouge_train, rouge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GO tempatan EOS PAD'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = [rev_dictionary[i] for i in sess.run(model.generate, feed_dict = {model.X: [test_X[0]]})[0,0,:]]\n",
    "' '.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pas biadab kata anwar bekas banduan EOS'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([rev_dictionary[i] for i in test_Y[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test minibatch loop: 100%|██████████| 598/598 [04:57<00:00,  2.51it/s, accuracy=1, cost=1.18, rouge_2=1]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, avg loss test: 83.170347, avg accuracy test: 0.371790\n",
      "epoch: 19, avg test rouge: 0.237134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge_test = 0\n",
    "total_loss_test, total_accuracy_test = 0, 0\n",
    "pbar = tqdm(range(0, len(test_X), batch_size), desc='test minibatch loop')\n",
    "for k in pbar:\n",
    "    index = min(k+batch_size,len(test_X))\n",
    "    batch_x = test_X[k: index]\n",
    "    batch_y, _ = pad_sentence_batch(test_Y[k: index], PAD)\n",
    "    l, acc, loss = sess.run([model.training_logits, model.accuracy, model.cost], \n",
    "                                      feed_dict={model.X:batch_x,\n",
    "                                                model.Y:batch_y})\n",
    "    total_loss_test += loss\n",
    "    total_accuracy_test += acc\n",
    "    r = calculate_rouges(np.argmax(l, axis = 2), batch_y)\n",
    "    rouge_test += r\n",
    "    pbar.set_postfix(cost=loss, accuracy = acc, rouge_2 = r)\n",
    "\n",
    "total_loss_test /= (len(test_X) / batch_size)\n",
    "total_accuracy_test /= (len(test_X) / batch_size)\n",
    "rouge_test /= (len(test_X) / batch_size)\n",
    "print('epoch: %d, avg loss test: %f, avg accuracy test: %f'%(EPOCH, total_loss_test, total_accuracy_test))\n",
    "print('epoch: %d, avg test rouge: %f'%(EPOCH, rouge_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
