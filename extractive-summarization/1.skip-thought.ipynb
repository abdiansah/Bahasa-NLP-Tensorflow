{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir('news')\n",
    "news = ['news/' + i for i in labels if '.json' in i]\n",
    "labels = [i.replace('.json','') for i in labels]\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "tokenizer = malaya.preprocessing._SocialTokenizer().tokenize\n",
    "split_sentence = malaya.texts._text_functions.split_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_tokens = ',-.()\"\\''\n",
    "\n",
    "def is_number_regex(s):\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", s) is None:\n",
    "        return s.isdigit()\n",
    "    return True\n",
    "\n",
    "def detect_money(word):\n",
    "    if word[:2] == 'rm' and is_number_regex(word[2:]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preprocessing(string):\n",
    "    splitted = split_sentence(string)\n",
    "    for i, string in enumerate(splitted):\n",
    "        tokenized = tokenizer(string)\n",
    "        tokenized = [w.lower() for w in tokenized if len(w) > 1]\n",
    "        tokenized = ['<NUM>' if is_number_regex(w) else w for w in tokenized]\n",
    "        tokenized = ['<MONEY>' if detect_money(w) else w for w in tokenized]\n",
    "        splitted[i] = tokenized\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263638"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len = 20\n",
    "x = []\n",
    "for no, n in enumerate(news):\n",
    "    with open(n) as fopen: \n",
    "        news_ = json.load(fopen)\n",
    "    for row in news_:\n",
    "        if len(row['text'].split()) > min_len:\n",
    "            p = preprocessing(row['text'])\n",
    "            x.extend(p)\n",
    "            \n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.sample(x, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def batch_sequence(sentences, dictionary, maxlen = 50):\n",
    "    np_array = np.zeros((len(sentences), maxlen), dtype = np.int32)\n",
    "    for no_sentence, sentence in enumerate(sentences):\n",
    "        current_no = 0\n",
    "        for no, word in enumerate(sentence[: maxlen - 2]):\n",
    "            np_array[no_sentence, no] = dictionary.get(word, 1)\n",
    "            current_no = no\n",
    "        np_array[no_sentence, current_no + 1] = 3\n",
    "    return np_array\n",
    "\n",
    "def build_dataset(words, n_words, atleast=2):\n",
    "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
    "    counter = collections.Counter(words).most_common(n_words)\n",
    "    counter = [i for i in counter if i[1] >= atleast]\n",
    "    count.extend(counter)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "X = list(itertools.chain(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 50\n",
    "vocabulary_size = len(set(X))\n",
    "embedding_size = 256\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9998, 9998, 9998)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "stride = 1\n",
    "t_range = int((len(x) - 3) / stride + 1)\n",
    "left, middle, right = [], [], []\n",
    "for i in range(t_range):\n",
    "    slices = x[i * stride : i * stride + 3]\n",
    "    left.append(slices[0])\n",
    "    middle.append(slices[1])\n",
    "    right.append(slices[2])\n",
    "\n",
    "left, middle, right = shuffle(left, middle, right)\n",
    "len(left), len(middle), len(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 24667\n",
      "Most common words [('yang', 3919), ('the', 3820), ('dan', 3525), ('<NUM>', 2603), ('di', 2357), ('ini', 1876)]\n",
      "Sample data [2292, 173, 1674, 2485, 12, 1859, 337, 0, 5462, 356] ['meanwhile', 'sabah', 'tourism', 'culture', 'and', 'environment', 'minister', 'PAD', 'liew', 'when']\n",
      "filtered vocab size: 12246\n",
      "% of vocab used: 49.65%\n"
     ]
    }
   ],
   "source": [
    "concat = X\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])\n",
    "print('filtered vocab size:',len(dictionary))\n",
    "print(\"% of vocab used: {}%\".format(round(len(dictionary)/vocabulary_size,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,maxlen=50, \n",
    "                 vocabulary_size=20000,\n",
    "                 learning_rate=1e-3,\n",
    "                 embedding_size = 256):\n",
    "        self.output_size = embedding_size\n",
    "        self.maxlen = maxlen\n",
    "        word_embeddings = tf.Variable(\n",
    "            tf.random_uniform(\n",
    "                [vocabulary_size, embedding_size], -np.sqrt(3), np.sqrt(3)\n",
    "            )\n",
    "        )\n",
    "        self.global_step = tf.get_variable(\n",
    "            \"global_step\", shape=[], trainable=False,\n",
    "            initializer=tf.initializers.zeros())\n",
    "        self.embeddings = word_embeddings\n",
    "        self.output_layer = tf.layers.Dense(vocabulary_size, name=\"output_layer\")\n",
    "        self.output_layer.build(self.output_size)\n",
    "        \n",
    "        self.BEFORE = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        self.INPUT = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        self.AFTER = tf.placeholder(tf.int32,[None,maxlen])\n",
    "        self.batch_size = tf.shape(self.INPUT)[0]\n",
    "        \n",
    "        self.get_thought = self.thought(self.INPUT)\n",
    "        self.attention = tf.matmul(\n",
    "            self.get_thought, tf.transpose(self.embeddings), name = 'attention'\n",
    "        )\n",
    "        fw_logits = self.decoder(self.get_thought, self.AFTER)\n",
    "        bw_logits = self.decoder(self.get_thought, self.BEFORE)\n",
    "        self.loss = self.calculate_loss(fw_logits, self.AFTER) + self.calculate_loss(bw_logits, self.BEFORE)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "    def get_embedding(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "        \n",
    "    def thought(self, inputs):\n",
    "        encoder_in = self.get_embedding(inputs)\n",
    "        fw_cell = tf.nn.rnn_cell.GRUCell(self.output_size)\n",
    "        bw_cell = tf.nn.rnn_cell.GRUCell(self.output_size)\n",
    "        sequence_length = tf.reduce_sum(tf.sign(inputs), axis=1)\n",
    "        rnn_output = tf.nn.bidirectional_dynamic_rnn(\n",
    "            fw_cell, bw_cell, encoder_in, sequence_length=sequence_length,\n",
    "            dtype=tf.float32)[1]\n",
    "        return sum(rnn_output)\n",
    "        \n",
    "    def decoder(self, thought, labels):\n",
    "        main = tf.strided_slice(labels, [0, 0], [self.batch_size, -1], [1, 1])\n",
    "        shifted_labels = tf.concat([tf.fill([self.batch_size, 1], 2), main], 1)\n",
    "        decoder_in = self.get_embedding(shifted_labels)\n",
    "        cell = tf.nn.rnn_cell.GRUCell(self.output_size)\n",
    "        max_seq_lengths = tf.fill([self.batch_size], self.maxlen)\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_in, max_seq_lengths, time_major = False\n",
    "        )\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, thought)\n",
    "        decoder_out = tf.contrib.seq2seq.dynamic_decode(decoder)[0].rnn_output\n",
    "        return decoder_out\n",
    "        \n",
    "    def calculate_loss(self, outputs, labels):\n",
    "        mask = tf.cast(tf.sign(labels), tf.float32)\n",
    "        logits = self.output_layer(outputs)\n",
    "        return tf.contrib.seq2seq.sequence_loss(logits, labels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(vocabulary_size = len(dictionary), embedding_size = embedding_size)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.37it/s, cost=12.7]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:58<00:00,  5.37it/s, cost=11.1]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:58<00:00,  5.32it/s, cost=9.72]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.38it/s, cost=8.56]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:58<00:00,  5.39it/s, cost=7.55]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.39it/s, cost=6.65]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.44it/s, cost=5.95]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.34it/s, cost=5.34]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:59<00:00,  5.34it/s, cost=4.85]\n",
      "train minibatch loop: 100%|██████████| 625/625 [01:58<00:00,  5.40it/s, cost=4.36]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in range(10):\n",
    "    pbar = tqdm(range(0, len(middle), batch_size), desc='train minibatch loop')\n",
    "    for p in pbar:\n",
    "        index = min(p + batch_size, len(middle))\n",
    "        batch_x = batch_sequence(\n",
    "                middle[p : index],\n",
    "                dictionary,\n",
    "                maxlen = maxlen,\n",
    "        )\n",
    "        batch_y_before = batch_sequence(\n",
    "                left[p : index],\n",
    "                dictionary,\n",
    "                maxlen = maxlen,\n",
    "        )\n",
    "        batch_y_after = batch_sequence(\n",
    "                right[p : index],\n",
    "                dictionary,\n",
    "                maxlen = maxlen,\n",
    "        )\n",
    "        loss, _ = sess.run([model.loss, model.optimizer], \n",
    "                           feed_dict = {model.BEFORE: batch_y_before,\n",
    "                                        model.INPUT: batch_x,\n",
    "                                        model.AFTER: batch_y_after,})\n",
    "        pbar.set_postfix(cost=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = random.sample(x, 100)\n",
    "\n",
    "sequences = batch_sequence(test, dictionary, maxlen = maxlen)\n",
    "encoded, attention = sess.run([model.get_thought, model.attention],feed_dict={model.INPUT:sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans = kmeans.fit(encoded)\n",
    "avg = []\n",
    "closest = []\n",
    "for j in range(n_clusters):\n",
    "    idx = np.where(kmeans.labels_ == j)[0]\n",
    "    avg.append(np.mean(idx))\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,encoded)\n",
    "ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "sentences = [test[closest[idx]] for idx in ordering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kita sudah banyak pegawai tadbir diplomatik dalam sektor awam dan syarikat syarikat besar kepunyaan kerajaan juga perlu bersaing dengan syarikat gergasi antarabangsa. dalam tempoh sama mcmc juga melaksanakan tindakan sekatan terhadap <NUM> laman sesawang portal dan blog yang menyebarkan kandungan atau berita palsu. gambas instagram bahkan ia menambahkan konservasi alam di bali pun erat kaitannya dengan budaya. begitu juga bn. sikap keterbukaan dan faham memahami amat diperlukan di antara umat islam dan bukan islam bagi menjamin keharmonian kaum. sesungguhnya yang demikian itu mengandungi tanda tanda membuktikan kekuasaan allah bagi kaum yang berfikir untuk memahaminya. khoo added that it was difficult for the police to take any action because these are civil cases between tnb and the property owners. usaha ini boleh menambah pendapatan penduduk jika ia dikendalikan dengan penuh minat sebelum ia mengeluarkan hasil nanti katanya. sebagai contoh insentif untuk meningkatkan perbelanjaan pengguna buat hatchback dan sedan permulaan bagi golongan berpendapatan rendah dan penjawat awam serta sedan dan crossover pertengahan bagi golongan berpendapatan sederhana jelasnya. sementara itu pengerusi celcom axiata berhad tan sri jamaluddin ibrahim berkata tajaan tersebut adalah salah satu usaha membantu skuad badminton negara ke sukan olimpik tokyo <NUM>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [' '.join(s) for s in sentences]\n",
    "'. '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garden',\n",
       " 'ditanggung',\n",
       " 'majesty',\n",
       " 'maritime',\n",
       " 'himpunan',\n",
       " 'statik',\n",
       " 'mbm',\n",
       " 'permukaan',\n",
       " 'trial',\n",
       " 'pass']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.argsort(attention.mean(axis=0))[::-1]\n",
    "rev_dictionary = {v:k for k, v in dictionary.items()}\n",
    "[rev_dictionary[i] for i in indices[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
