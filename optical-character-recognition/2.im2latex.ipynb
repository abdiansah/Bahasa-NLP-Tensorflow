{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://malaya-dataset.s3-ap-southeast-1.amazonaws.com/jawi-rumi.tar.gz\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/Malaya-Dataset/master/ocr/train-test-rumi-to-jawi.json\n",
    "# !tar -zxf jawi-rumi.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24840, 6211)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('train-test-rumi-to-jawi.json') as fopen:\n",
    "    dataset = json.load(fopen)\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/husein/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize as imresize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [f.split('/')[1].split('.')[0].lower() for f in dataset['train']]\n",
    "test_labels = [f.split('/')[1].split('.')[0].lower() for f in dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread(dataset['train'][0], 0).astype(np.float32)/255.)\n",
    "plt.title(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = list(set(''.join(train_labels + test_labels)))\n",
    "num_classes = len(charset) + 2\n",
    "encode_maps = {}\n",
    "decode_maps = {}\n",
    "for i, char in enumerate(charset, 3):\n",
    "    encode_maps[char] = i\n",
    "    decode_maps[i] = char\n",
    "    \n",
    "SPACE_INDEX = 0\n",
    "SPACE_TOKEN = '<PAD>'\n",
    "encode_maps[SPACE_TOKEN] = SPACE_INDEX\n",
    "decode_maps[SPACE_INDEX] = SPACE_TOKEN\n",
    "\n",
    "GO_INDEX = 1\n",
    "GO_TOKEN = '<GO>'\n",
    "encode_maps[GO_TOKEN] = GO_INDEX\n",
    "decode_maps[GO_INDEX] = GO_TOKEN\n",
    "\n",
    "EOS_INDEX = 2\n",
    "EOS_TOKEN = '<EOS>'\n",
    "encode_maps[EOS_TOKEN] = EOS_INDEX\n",
    "decode_maps[EOS_INDEX] = EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 3,\n",
       " 'x': 4,\n",
       " 'w': 5,\n",
       " 'q': 6,\n",
       " 'r': 7,\n",
       " 'u': 8,\n",
       " 'o': 9,\n",
       " 'd': 10,\n",
       " \"'\": 11,\n",
       " 'z': 12,\n",
       " 'a': 13,\n",
       " ' ': 14,\n",
       " 'v': 15,\n",
       " 'h': 16,\n",
       " 'g': 17,\n",
       " 'f': 18,\n",
       " 'i': 19,\n",
       " 't': 20,\n",
       " '-': 21,\n",
       " 'n': 22,\n",
       " 'k': 23,\n",
       " 'j': 24,\n",
       " 'm': 25,\n",
       " 'p': 26,\n",
       " '?': 27,\n",
       " 'b': 28,\n",
       " 'c': 29,\n",
       " 'y': 30,\n",
       " 's': 31,\n",
       " 'l': 32,\n",
       " '<PAD>': 0,\n",
       " '<GO>': 1,\n",
       " '<EOS>': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 13, 20, 13, 26, 13, 22, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encode_maps[c] for c in train_labels[0]] + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO = 1\n",
    "PAD = 0\n",
    "EOS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 60\n",
    "image_width = 240\n",
    "image_channel = 1\n",
    "max_stepsize = 128\n",
    "num_hidden = 256\n",
    "epoch = 20\n",
    "batch_size = 128\n",
    "initial_learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAACDCAYAAACUaEA8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAX90lEQVR4nO3deZwcZZnA8d/T1dNzZJKZ3OeEHMwICYEAMZwiAkIguBEXISIEETfCgp+AyKWi4qq7riCIAgoLC6xyxIXlCogRErwCIYFcJCE3DJOZZHJM5p4+6tk/qmbSGXsyd8909/P9fPoz3dX1Vr31fnqefvup960SVcUYY0zqCfR1BYwxxnSNBXBjjElRFsCNMSZFWQA3xpgUZQHcGGNSlAVwY4xJURbAjTEmRVkAN2lLRL4iIn/t63oY01ssgJuUJSLBvq6DMX3JArhJKSKyQ0RuFZE1QJ2IfFdEtopIjYisF5GL/PWOBn4NnCIitSJS5S+fLSLviUi1iJSKyA/itj1BRFRE5ovIThEpF5Fvxb0/U0SWiUiV/96vRCQU976KyDUistlf534RkWS1jck8FsBNKvoSMBsoBD4APgUUAHcCvxWR0aq6AbgGWKaq+apa6JetA+b5ZWcD14rI51tt/zNAMXAucKuInOMvjwE3AsOAU4CzgX9tVfZC4JPAscAlwHk9csTGJGAB3KSi+1S1VFUbVPX3qrpTVV1VfQbYDMxsq6CqLlXVtf76a4CngE+3Wu1OVa1T1bXAf+N9YaCqK1X1LVWNquoO4DcJyv6Hqlap6kfAEmB6jxyxMQlYADepqLT5iYjME5FVfsqiCjgGr4eckIicJCJLRKRSRA7g9dJbr18a9/xDYIxftkREXhaRChGpBn6SoGxF3PN6IL+zB2dMR1kAN6lIAUTkCOBh4HpgqJ8mWQdI/HqtPAm8CBSpagFenrx1nroo7vl4YKf//EFgI1CsqoOAbycoa0zSWAA3qWwAXpCuBBCRq/B64M12AePiTzQCA4F9qtooIjOByxJs9w4RyRORqcBVwDNxZauBWhE5Cri2R4/GmE6yAG5SlqquB+4GluEF62nA3+JWeQN4H6gQkT3+sn8FfigiNcD3gIUJNv0msAV4HbhLVf/oL/8WXsCvwev5P5OgrDFJI3ZDB2M8IjIB2A5kqWq0b2tjTPusB26MMSnKArgxxqSobgVwEZklIh+IyBYRua2nKmVMX1DVHaoqlj4xqaLLOXARcYBNwGeBj4F3gC/5J5aMMcb0su70wGcCW1R1m6qGgaeBOT1TLWOMMe3pztXcxnLojLWPgZMOV2DYEEcnFGV1Y5fGGJN5Vq5p2qOqw1sv7/XLcYrIfGA+wPixQZa/VtROCWOMMfGc0Vs+TLS8OymUMg6dcjzOX3YIVX1IVWeo6ozhQ51u7M4YY0y87gTwd4BiEZnoT1Wei3eNCWOMMUnQ5RSKqkZF5HrgNcABHlXV93usZsYYYw6rWzlwVX0FeKWH6mKMMaYTbCamMcakKAvgxhiToiyAG2NMirIAbowxKcoCuDHGpCgL4MYYk6IsgBtjTIqyAG6MMSnKArgxxqSotA7gMXX7ugrGGNNr0jqAG2NMOkvrAO5IWh+eMSbD9foNHfpSRGMEEMCCuTEm/VhUM8aYFJXWPfAlDTnkBCIAnJHTx5UxxpgeltYB/Nl9M9iwfxQAS4551tIoxpi0ktYB/IOqkexfPBqApqlR8iTUxzUyxpieY11SY4xJUWkdwHfuG8TYN2sY+2YNW6I2qccYk17aDeAiUiQiS0RkvYi8LyIL/OVDRGSxiGz2/w7u/ep2TrQyF6fyAE7lATaHR/TKPiIaY2VTmJVNYZv5aYxJqo70wKPATao6BTgZuE5EpgC3Aa+rajHwuv/aGGNMkrR7ElNVy4Fy/3mNiGwAxgJzgDP91R4HlgK39kotuyCmLjm7HTSUBUChU9cr+/nl/mJevPlsAObe/QrXFJb1yn6MMaa1TuXARWQCcDzwNjDSD+4AFcDIHq1ZD8jdrWgoCw1lMcqp7fHtb43U8tz3P0v2H1aQ/YcV/PaOC/koWstH0Z7flzHGtNbhYYQikg88C9ygqtUi0vKeqqqIaBvl5gPzAcaPTd6oxSaNkrPPxc3zeuAFgViP72NteBSDNlbRvOXGIQHy4trFGGN6U4d64CKShRe8f6eqz/mLd4nIaP/90cDuRGVV9SFVnaGqM4YPdXqizsYYY+jYKBQBHgE2qOrP4956EbjSf34l8ELPV6/r6jVC9oEY4SE5hIfkkJOgZxxTl5i61LqNRDTW8rp5Wa3beMh6zaNMmp+fmrOLLd/NQU89Dj31OG65+UmyJECWP+Ozdbm2xL/fkfWNMQY6lkI5DbgCWCsiq/xl3wb+A1goIlcDHwKX9E4Vu6ZeldC+Rqon5wO0BNV4+90GAE5eeBMXnrmCu0ctB+CXVZN48LnzAXj9yp8x2slrmYYfU5eX6gcB8L1fX89/XvMEFQ8XAPDy3uP47v9eBsDfr7iLwYHcDtc3ol4i5rPv/zOnj9jKj0as7fQxG2MyS0dGofwVaCuxe3bPVscYY0xHpe21UGpcB2dfLU0nDgQgi0Pz7zF1OXvl1wAouXM9axcdx+8e2AHAs3ecx6Q/rAbgU2NuYOOsB3H8bNNet4E777kegDEPr+A7oXn8+1cfA6DimxOZvG6dV674Gtac+hhZ0n7e35EAP983GYC86x3+PuEklv/mXQBmZmd1tQmMMWkubQP4XjcX6hsID/R+PLQOpOWxegb/xkuvxKqrydlQxveXfgGAo15ZjRv2LkNbsCpE43lRssULpE9WT2XMc1sBiEYjDNkY48a3LwWg+J21NA/FiW3Jxz3VBdoP4LVuI8/ccy4AQze/RfZH2dz4gbfNvx373OGKGmMyWNoG8B3hYWg0SsTrgLfcmafZaCePvFu8STeyZgzr/20Mb5ztnaO9dNnNFG6qB+COb/yWfMluKTe/YBP33+Dlx4ufKODCH77BJYPeA+CKz9+E0+SF8IVz7yVIx3rP+YEcRlz+offixWGUf7GY16b+zH93QOcO3BiTMdL6YlbGGJPO0rYH/nF4KMRcogMSD8lzJMBTxc8CcMEjX+YvU+5lXNBLqSz49kLerpkEwOcHVAEHh/rlBUL8/tJ7AbjrjPO4Ych6guQB8KUfvoKr3nfitFDHc9cxdVvqMv2n3+AXn3qMoZ0YwWKMyUxpG8B3NhWi0SixNgI4QIEfJJdO+z1Zkt+y/MsD9zI3vxJIfDPk6dleSuXxI97AkYOB+rrC0pZA39m7/zSnaTae9yDBDuTNjTHGUijGGJOi0rYHXhXJBTeC5EWBw/eIEw3160gPOtE6bZWLqYvrj1EpjzUwPpifsIxj36nGmA5K22hRHcmBWIxgyHu0p6PT3hOJaKxlJmVbHAmwrMlhWZPDOU/ezP5Yfaf3Y4wx8dI2gNdFQqgqTtDFCbZ/LZId0c4H1Fq3kYu3nkPJq1+n5NWv82ztoDa/BGLq8kD5WTxQfhbFD5Vz374Znd6fMcbES9sAbowx6S5tA3h9JAtcJRiMEQwePr1RHqvngt/ezO5YPbs7kNqod8PUu2GO/dN11F+klMx/j5L57/HAtV9kcUMuixsSDwFcvqqY5auKie4oZeHTZ9KkEZo00qXjM8aYtD2J2RjOoqCD6z6yfyZHPlLOvRecDsBPR65qc92YutxWcRoAR99eRnTPXvAvVRtcsoobn7gagDPm30teINRSLkqMwrX+96W6FP3hAFu+7p1gndqJMePGGNMsbXvgxhiT7vplD3xT5OANiCcHczs9KQYgHG1/MkzzycbHV59MSek6nl90CgA/uerdNvfZoGH++oh3AnJ4xVs4hQXIkMEARLd/yMQnvduELpk3iNl5jS3lKmNNDNnY1PJ6+8WDKMkKYYwxXdUvA/g3t1/MB8snALD6sl+QJ50PdEGn/eGAUf9ulgPW5aCxGKEqLxXiom3OhdwSEUb9eR8Amp/Px4+N4d+P+T8A7rv8EnTVJgAeLvs0s4tfaym3J5ZFaLd/s+OCQZx33op/uMCWMcZ0Rr9KoTQPwXt/+xhKfllKyS9L+VFl14bbDcxpgsDhA2SjRmnUKIO2x3CGFHLpvDe4dN4bh72G96qmIqiohIpKwjNLeOH4h5mVW8+s3Ho2fc0buqiqrCsbfUi5slgB7K2CvVXESsbztWF/wZFAl35dNLt46zlsj9SyPVLb5W0YY1JXvwrgxhhjOq5fBXAXxUUJ7QwRLSsnWlbOi0+f3qXZkYOyG9tdJ6IuEXXJPhCj6dgJfKVwBV8pXHHYMpXRgeAquEpNUYghjtPSkx48ogYRQURww4f24tc1FEFTEzQ1UX76QI4Mdr3pm3+prP1zMQurj2dh9fFd3pYxJnV1OIqIiCMi74nIy/7riSLytohsEZFnRLqQqG6leUp69r6DqY+6idEupRlG5tYgwcOn+F3/EYi4HJgcYkjAexzOpFAlBIMQDDJ0xX6WNRayJ1bHnlgdzvNDWtYbN3rfIeXKmgoPHtPxDWRL108/NH9hRI9o5KWyabxUNq3L2zLGpK7ORMYFwIa41z8F7lHVI4H9wNU9WTFjjDGH16FuoIiMA2YDPwa+KSICnAVc5q/yOPAD4MHuVMbFS5WEDiiBHO/62Kcdu6lL25qSv5NdOUd0eP26MdKhGxB/MmcnD04tAsD582pueuRqGqc0AFDy5HsExowCYMHE1w8pt6cpH/y0yVklm7p18rLZ0eMqWL92PACxaW6PbNMYkzo6+jv+XuAWwL/DJEOBKlWN+q8/Bsb2VKWcJggM9dIRV418tUvbOCF3B0tzj0JE218ZCA9uO88en4Mf6+Sx7SteiqdkWZCiu1Ygjhc43XCEHfPGAHB+3h7gYDqmKpyL5HvNfVrB2k4dS1tOHbKNHWUTAW9IpF2K1pjM0m4AF5ELgd2qulJEzuzsDkRkPjAfYPzYw+8u5l8vOxBTIuOHATApeADIP0ypxIqC1bhDB6Ha9lDC5nDnOoI6ygHXO/HptBqfXacuEf97IEvgJ6d4d4p/9IR/QpatRqP+VPojirjiC17P+4AbplYj1LheweqmHAYO8gL6pNDuTh9PIsfkljJgp7f9GjdMtmNT8o3JJB3pgZ8G/JOIXADkAIOAXwCFIhL0e+HjgLJEhVX1IeAhgBnH5XSsO2yMMaZd7QZwVb0duB3A74F/S1W/LCK/By4GngauBF7obmWae77RbKFmfA4Aw52ujdYY6QSoOXIQUNPu/iIDg5Q8Vsucv9yUcL2smhhZ9V62KDIgSO5Ob6p/YMNGVA721mMf7+Svc48DYPH4M3CzhND+MAB5rqKOt41RTh0woEvHFW+EU0N2tTebtMZVhtmtNI3JKN2ZSn8r8LSI/Ah4D3iku5UJ+EmNaK5QN8YLjNnStbRAvmSz/xMOzmFy4M1D+epGBsh7fj35K9v/gZANxGfLJSt0MAfe2EhsvXfSNbQ+QeEZxwCQRc/8EAnjkOUH8Eo3m4k9slVjTKroVABX1aXAUv/5NmBmz1fJGGNMR/SrYQuOCI4I0QGQV6HkVWi795pse1sBosfVItDmJaOyJUi2BDlweiMSCnnX9ZZ2LjAlggSDSDCIUzyJzY9OJbpoONFFw5ETp3o98qxQwu0ESncTKN3NUwdO7NLs0tYqogUEa8MEa8PUuDnd3p4xJrX0q6sRBv1rADaMdBn3xwMAbIoo07O7tr0rjl7OqzuntPl+87jpl0+/n9l33UjuzvaTyG4IOMbLq99z/ELOzq1vuarg2v+NcPW6KwCoe3cYTtxsfnGhaJE3O/PRxZO4ae46mjMpXR2/vXj/MQTC3hec00NpGWNM6uhXAbyZU1SPbPcGtdy67Z957eiXu7SdSwpWUtHU/n15jg7lsfkLB+cgdS6gHgz607OzWXniQu/FiYeu1aQRpucuAKD4if0smZPPrLwmuuNPq6dwlOt9mRQGGvAy9MaYTNGvUijGGGM6rl8F8OaLNJ00fgcai6GxGNX/NY49sbr2CycwMZjDtcOXcu3wpR3ed29NRw/iMPCTlQz8ZCVs+Yjrn7+qyzc1Lo/WUh6t5YjnIVqQS7Qgl2GO3RzZmEwjqsnLnc44LkeXv1bU7nqL6nN44NxZALgVu3FeKeCF4kVA1/PF/cGieu9E4/2zzocDtYx8yUuS/1fRmx0+rpi6fG7ThQDo7CoqL/PGnS/9wT3kB+xEpjHpyBm9ZaWq/sPdbVI3GhpjTIbrlwH8pOy91EwbQc20Ebj19TR9fxQrwzFWhrs2pLC/+HROFZ/OqaLsc6OJ7d3HrsuHsevyYczZPJv9sXr2x+oPW77eDXNLxQx0wSDvEQ6z9+QIe0+OkNv9y7EbY1JMvxyFMjiQS+nnvHHSJS85OH9by2XPfQOANZfeR147N13or5pTHOdesYz1T48kumW798ZFgznrsm8BMOILH3Hd+DcYG6wCYEdkGC/v89Ikf198DEc+VIpbuhGA4OhRfPOUxUBqp5aMMV3TL3PgAL+rGQrAk+eeSvTDUpyRIwBwn8ripU+8CNCh63f3R/VumGOfWsCRt78DgMYO/rII5OYieblIlncJAY1E0CbveipuXT24MQh4x112y0n87fq7ASgI5CbzEIwxSWQ5cGOMSTP9MoUCcNGAcgB+OH8cE+7YSWyXdw1t55rJzH/iTODg6I3maempkkbIC4RYfMnPmFNxCwBj7luBRr1hgG59PTQ0HFog/ldSwCF87gkA/Opffk2+2OQdYzJVv02hNHurMcb3vvxVZNlqb4EIQT+dsv5HRbxwzq+YmuXlxFMlgIM3HLDav4HEWe9+lWF3eSmQrLXbcOsaIO5aKZLtBWktmcDWuYN49tJ7ADg2ZMMGjckElkIxxpg00+974AB3Vk7hrXneSAx3zcaWlIIEgzDtE2y62rvl2lfPeJObh3r3m+zqdcT7Snm0FoC/NI7l9aop7PBP4o7P389pBZsB+FTuNiYE81Lql4Yxpvva6oGnRACPaIyf7zsKgBd+fDYFL3lB2q33xk0Hcr30Q/iUo/nxww8BcHJOao5QMcaY1lI6gMfbGqllwfYvAvDhoonUTo5y4tRtAFw+6i1m53mXoU3VIYbGGNOa5cCNMSbN9NthhG2ZnJXPyyWvAhArTjR80HrexpjMkHIBPJ6dzDPGZDKLgMYYk6IsgBtjTIpK6igUEakE6oA9SdtpahiGtUki1i6JWbskls7tcoSqDm+9MKkBHEBEViQaDpPJrE0Ss3ZJzNolsUxsF0uhGGNMirIAbowxKaovAvhDfbDP/s7aJDFrl8SsXRLLuHZJeg7cGGNMz7AUijHGpKikBXARmSUiH4jIFhG5LVn77Y9EZIeIrBWRVSKywl82REQWi8hm/+/gvq5nbxORR0Vkt4isi1uWsB3Ec5//+VkjIif0Xc17Txtt8gMRKfM/L6tE5IK492732+QDETmvb2rd+0SkSESWiMh6EXlfRBb4yzP685KUAC4iDnA/cD4wBfiSiExJxr77sc+o6vS4YU+3Aa+rajHwuv863T0GzGq1rK12OB8o9h/zgQeTVMdke4x/bBOAe/zPy3RVfQXA/x+aC0z1yzzg/6+loyhwk6pOAU4GrvOPP6M/L8nqgc8EtqjqNlUNA08Dc5K071QxB3jcf/448Pk+rEtSqOqfgX2tFrfVDnOAJ9TzFlAoIqOTU9PkaaNN2jIHeFpVm1R1O7AF738t7ahquaq+6z+vATYAY8nwz0uyAvhYoDTu9cf+skylwB9FZKWIzPeXjVTVcv95BTCyb6rW59pqh0z/DF3vpwIejUuvZWSbiMgE4HjgbTL882InMfvG6ap6At7PvOtE5Iz4N9UbGpTxw4OsHVo8CEwGpgPlwN19W52+IyL5wLPADapaHf9eJn5ekhXAy4D4W/GM85dlJFUt8//uBv4P72fvruafeP7f3X1Xwz7VVjtk7GdIVXepakxVXeBhDqZJMqpNRCQLL3j/TlWf8xdn9OclWQH8HaBYRCaKSAjvxMuLSdp3vyIiA0RkYPNz4FxgHV57XOmvdiXwQt/UsM+11Q4vAvP80QUnAwfifjqntVa524vwPi/gtclcEckWkYl4J+yWJ7t+ySAiAjwCbFDVn8e9ldmfF1VNygO4ANgEbAW+k6z99rcHMAlY7T/eb24LYCjeWfTNwJ+AIX1d1yS0xVN4KYEIXo7y6rbaARC8kUxbgbXAjL6ufxLb5H/8Y16DF5hGx63/Hb9NPgDO7+v692K7nI6XHlkDrPIfF2T658VmYhpjTIqyk5jGGJOiLIAbY0yKsgBujDEpygK4McakKAvgxhiToiyAG2NMirIAbowxKcoCuDHGpKj/Bzsu+Bc/fXdKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resized = imresize(cv2.flip((cv2.imread(dataset['train'][0], 0).astype(np.float32)/255.), 1), (image_height,\n",
    "                                                                        image_width,\n",
    "                                                                        image_channel))\n",
    "plt.imshow(resized[:,:,0])\n",
    "plt.title(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24840/24840 [01:52<00:00, 221.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "train_X = []\n",
    "for img in tqdm.tqdm(dataset['train']):\n",
    "    resized = imresize(cv2.flip((cv2.imread(img, 0).astype(np.float32)/255.), 1), (image_height,\n",
    "                                                                        image_width,\n",
    "                                                                        image_channel))\n",
    "    train_X.append(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6211/6211 [00:28<00:00, 221.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "test_X = []\n",
    "for img in tqdm.tqdm(dataset['test']):\n",
    "    resized = imresize(cv2.flip((cv2.imread(img, 0).astype(np.float32)/255.), 1), (image_height,\n",
    "                                                                        image_width,\n",
    "                                                                        image_channel))\n",
    "    test_X.append(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = []\n",
    "for label in train_labels:\n",
    "    train_Y.append([encode_maps[c] for c in label] + [EOS])\n",
    "    \n",
    "test_Y = []\n",
    "for label in test_labels:\n",
    "    test_Y.append([encode_maps[c] for c in label] + [EOS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 240, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/guillaumegenthial/im2latex/blob/master/model/components/attention_mechanism.py\n",
    "\n",
    "class AttentionMechanism(object):\n",
    "    \"\"\"Class to compute attention over an image\"\"\"\n",
    "\n",
    "    def __init__(self, img, dim_e, tiles=1):\n",
    "        \"\"\"Stores the image under the right shape.\n",
    "        We loose the H, W dimensions and merge them into a single\n",
    "        dimension that corresponds to \"regions\" of the image.\n",
    "        Args:\n",
    "            img: (tf.Tensor) image\n",
    "            dim_e: (int) dimension of the intermediary vector used to\n",
    "                compute attention\n",
    "            tiles: (int) default 1, input to context h may have size\n",
    "                    (tile * batch_size, ...)\n",
    "        \"\"\"\n",
    "        if len(img.shape) == 3:\n",
    "            self._img = img\n",
    "        elif len(img.shape) == 4:\n",
    "            N    = tf.shape(img)[0]\n",
    "            H, W = tf.shape(img)[1], tf.shape(img)[2] # image\n",
    "            C    = img.shape[3].value                 # channels\n",
    "            self._img = tf.reshape(img, shape=[N, H*W, C])\n",
    "        else:\n",
    "            print(\"Image shape not supported\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dimensions\n",
    "        self._n_regions  = tf.shape(self._img)[1]\n",
    "        self._n_channels = self._img.shape[2].value\n",
    "        self._dim_e      = dim_e\n",
    "        self._tiles      = tiles\n",
    "        self._scope_name = \"att_mechanism\"\n",
    "\n",
    "        # attention vector over the image\n",
    "        self._att_img = tf.layers.dense(\n",
    "            inputs=self._img,\n",
    "            units=self._dim_e,\n",
    "            use_bias=False,\n",
    "            name=\"att_img\")\n",
    "\n",
    "\n",
    "    def context(self, h):\n",
    "        \"\"\"Computes attention\n",
    "        Args:\n",
    "            h: (batch_size, num_units) hidden state\n",
    "        Returns:\n",
    "            c: (batch_size, channels) context vector\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(self._scope_name):\n",
    "            if self._tiles > 1:\n",
    "                att_img = tf.expand_dims(self._att_img, axis=1)\n",
    "                att_img = tf.tile(att_img, multiples=[1, self._tiles, 1, 1])\n",
    "                att_img = tf.reshape(att_img, shape=[-1, self._n_regions,\n",
    "                        self._dim_e])\n",
    "                img = tf.expand_dims(self._img, axis=1)\n",
    "                img = tf.tile(img, multiples=[1, self._tiles, 1, 1])\n",
    "                img = tf.reshape(img, shape=[-1, self._n_regions,\n",
    "                        self._n_channels])\n",
    "            else:\n",
    "                att_img = self._att_img\n",
    "                img     = self._img\n",
    "\n",
    "            # computes attention over the hidden vector\n",
    "            att_h = tf.layers.dense(inputs=h, units=self._dim_e, use_bias=False)\n",
    "\n",
    "            # sums the two contributions\n",
    "            att_h = tf.expand_dims(att_h, axis=1)\n",
    "            att = tf.tanh(att_img + att_h)\n",
    "\n",
    "            # computes scalar product with beta vector\n",
    "            # works faster with a matmul than with a * and a tf.reduce_sum\n",
    "            att_beta = tf.get_variable(\"att_beta\", shape=[self._dim_e, 1],\n",
    "                    dtype=tf.float32)\n",
    "            att_flat = tf.reshape(att, shape=[-1, self._dim_e])\n",
    "            e = tf.matmul(att_flat, att_beta)\n",
    "            e = tf.reshape(e, shape=[-1, self._n_regions])\n",
    "\n",
    "            # compute weights\n",
    "            a = tf.nn.softmax(e)\n",
    "            a = tf.expand_dims(a, axis=-1)\n",
    "            c = tf.reduce_sum(a * img, axis=1)\n",
    "\n",
    "            return c\n",
    "\n",
    "\n",
    "    def initial_cell_state(self, cell):\n",
    "        \"\"\"Returns initial state of a cell computed from the image\n",
    "        Assumes cell.state_type is an instance of named_tuple.\n",
    "        Ex: LSTMStateTuple\n",
    "        Args:\n",
    "            cell: (instance of RNNCell) must define _state_size\n",
    "        \"\"\"\n",
    "        _states_0 = []\n",
    "        for hidden_name in cell._state_size._fields:\n",
    "            hidden_dim = getattr(cell._state_size, hidden_name)\n",
    "            h = self.initial_state(hidden_name, hidden_dim)\n",
    "            _states_0.append(h)\n",
    "\n",
    "        initial_state_cell = type(cell.state_size)(*_states_0)\n",
    "\n",
    "        return initial_state_cell\n",
    "\n",
    "\n",
    "    def initial_state(self, name, dim):\n",
    "        \"\"\"Returns initial state of dimension specified by dim\"\"\"\n",
    "        with tf.variable_scope(self._scope_name):\n",
    "            img_mean = tf.reduce_mean(self._img, axis=1)\n",
    "            W = tf.get_variable(\"W_{}_0\".format(name), shape=[self._n_channels,\n",
    "                    dim])\n",
    "            b = tf.get_variable(\"b_{}_0\".format(name), shape=[dim])\n",
    "            h = tf.tanh(tf.matmul(img_mean, W) + b)\n",
    "\n",
    "            return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/guillaumegenthial/im2latex/blob/master/model/components/attention_cell.py\n",
    "\n",
    "import collections\n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\n",
    "\n",
    "\n",
    "AttentionState = collections.namedtuple(\"AttentionState\", (\"cell_state\", \"o\"))\n",
    "\n",
    "\n",
    "class AttentionCell(RNNCell):\n",
    "    def __init__(self, cell, attention_mechanism, dropout, dim_e,\n",
    "                 dim_o, num_units,\n",
    "        num_proj, dtype=tf.float32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cell: (RNNCell)\n",
    "            attention_mechanism: (AttentionMechanism)\n",
    "            dropout: (tf.float)\n",
    "            attn_cell_config: (dict) hyper params\n",
    "        \"\"\"\n",
    "        # variables and tensors\n",
    "        self._cell                = cell\n",
    "        self._attention_mechanism = attention_mechanism\n",
    "        self._dropout             = dropout\n",
    "\n",
    "        # hyperparameters and shapes\n",
    "        self._n_channels     = self._attention_mechanism._n_channels\n",
    "        self._dim_e          = dim_e\n",
    "        self._dim_o          = dim_o\n",
    "        self._num_units      = num_units\n",
    "        self._num_proj       = num_proj\n",
    "        self._dtype          = dtype\n",
    "\n",
    "        # for RNNCell\n",
    "        self._state_size = AttentionState(self._cell._state_size, self._dim_o)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_proj\n",
    "\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        return self._dtype\n",
    "\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"Returns initial state for the lstm\"\"\"\n",
    "        initial_cell_state = self._attention_mechanism.initial_cell_state(self._cell)\n",
    "        initial_o          = self._attention_mechanism.initial_state(\"o\", self._dim_o)\n",
    "\n",
    "        return AttentionState(initial_cell_state, initial_o)\n",
    "\n",
    "\n",
    "    def step(self, embedding, attn_cell_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding: shape = (batch_size, dim_embeddings) embeddings\n",
    "                from previous time step\n",
    "            attn_cell_state: (AttentionState) state from previous time step\n",
    "        \"\"\"\n",
    "        prev_cell_state, o = attn_cell_state\n",
    "\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            # compute new h\n",
    "            x                     = tf.concat([embedding, o], axis=-1)\n",
    "            new_h, new_cell_state = self._cell.__call__(x, prev_cell_state)\n",
    "            new_h = tf.nn.dropout(new_h, self._dropout)\n",
    "\n",
    "            # compute attention\n",
    "            c = self._attention_mechanism.context(new_h)\n",
    "\n",
    "            # compute o\n",
    "            o_W_c = tf.get_variable(\"o_W_c\", dtype=tf.float32,\n",
    "                    shape=(self._n_channels, self._dim_o))\n",
    "            o_W_h = tf.get_variable(\"o_W_h\", dtype=tf.float32,\n",
    "                    shape=(self._num_units, self._dim_o))\n",
    "\n",
    "            new_o = tf.tanh(tf.matmul(new_h, o_W_h) + tf.matmul(c, o_W_c))\n",
    "            new_o = tf.nn.dropout(new_o, self._dropout)\n",
    "\n",
    "            y_W_o = tf.get_variable(\"y_W_o\", dtype=tf.float32,\n",
    "                    shape=(self._dim_o, self._num_proj))\n",
    "            logits = tf.matmul(new_o, y_W_o)\n",
    "\n",
    "            # new Attn cell state\n",
    "            new_state = AttentionState(new_cell_state, new_o)\n",
    "\n",
    "            return logits, new_state\n",
    "\n",
    "\n",
    "    def __call__(self, inputs, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: the embedding of the previous word for training only\n",
    "            state: (AttentionState) (h, o) where h is the hidden state and\n",
    "                o is the vector used to make the prediction of\n",
    "                the previous word\n",
    "        \"\"\"\n",
    "        new_output, new_state = self.step(inputs, state)\n",
    "\n",
    "        return (new_output, new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# taken from https://github.com/tensorflow/tensor2tensor/blob/37465a1759e278e8f073cd04cd9b4fe377d3c740/tensor2tensor/layers/common_attention.py\n",
    "\n",
    "# taken from https://raw.githubusercontent.com/guillaumegenthial/im2latex/master/model/components/positional.py\n",
    "\n",
    "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
    "    \"\"\"Adds a bunch of sinusoids of different frequencies to a Tensor.\n",
    "\n",
    "    Each channel of the input Tensor is incremented by a sinusoid of a difft\n",
    "    frequency and phase in one of the positional dimensions.\n",
    "\n",
    "    This allows attention to learn to use absolute and relative positions.\n",
    "    Timing signals should be added to some precursors of both the query and the\n",
    "    memory inputs to attention.\n",
    "\n",
    "    The use of relative position is possible because sin(a+b) and cos(a+b) can\n",
    "    be experessed in terms of b, sin(a) and cos(a).\n",
    "\n",
    "    x is a Tensor with n \"positional\" dimensions, e.g. one dimension for a\n",
    "    sequence or two dimensions for an image\n",
    "\n",
    "    We use a geometric sequence of timescales starting with\n",
    "    min_timescale and ending with max_timescale.  The number of different\n",
    "    timescales is equal to channels // (n * 2). For each timescale, we\n",
    "    generate the two sinusoidal signals sin(timestep/timescale) and\n",
    "    cos(timestep/timescale).  All of these sinusoids are concatenated in\n",
    "    the channels dimension.\n",
    "\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, d1 ... dn, channels]\n",
    "        min_timescale: a float\n",
    "        max_timescale: a float\n",
    "\n",
    "    Returns:\n",
    "        a Tensor the same shape as x.\n",
    "\n",
    "    \"\"\"\n",
    "    static_shape = x.get_shape().as_list()\n",
    "    num_dims = len(static_shape) - 2\n",
    "    channels = tf.shape(x)[-1]\n",
    "    num_timescales = channels // (num_dims * 2)\n",
    "    log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            (tf.to_float(num_timescales) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "            tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n",
    "    for dim in xrange(num_dims):\n",
    "        length = tf.shape(x)[dim + 1]\n",
    "        position = tf.to_float(tf.range(length))\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
    "                inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        prepad = dim * 2 * num_timescales\n",
    "        postpad = channels - (dim + 1) * 2 * num_timescales\n",
    "        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
    "        for _ in xrange(1 + dim):\n",
    "            signal = tf.expand_dims(signal, 0)\n",
    "        for _ in xrange(num_dims - 1 - dim):\n",
    "            signal = tf.expand_dims(signal, -2)\n",
    "        x += signal\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_size = 256\n",
    "size_layer = 256\n",
    "embedded_size = 256\n",
    "beam_width = 15\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN part I took from https://github.com/guillaumegenthial/im2latex/blob/master/model/encoder.py\n",
    "# I use tf.contrib.seq2seq as decoder part\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.X = tf.placeholder(tf.float32, shape=(None, 60, 240, 1))\n",
    "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y_seq_len = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "        x_len = tf.shape(self.X)[2] // 2\n",
    "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
    "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
    "        \n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([len(encode_maps), embedded_size], -1, 1))\n",
    "        \n",
    "        img = self.X\n",
    "        \n",
    "        out = tf.layers.conv2d(img, 64, 3, 1, \"SAME\",\n",
    "                activation=tf.nn.relu)\n",
    "        out = tf.layers.max_pooling2d(out, 2, 2, \"SAME\")\n",
    "\n",
    "        out = tf.layers.conv2d(out, 128, 3, 1, \"SAME\",\n",
    "                activation=tf.nn.relu)\n",
    "        out = tf.layers.max_pooling2d(out, 2, 2, \"SAME\")\n",
    "\n",
    "        out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\",\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        out = tf.layers.conv2d(out, 256, 3, 1, \"SAME\",\n",
    "                activation=tf.nn.relu)\n",
    "        out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), \"SAME\")\n",
    "        out = tf.layers.conv2d(out, 512, 3, 1, \"SAME\",\n",
    "                activation=tf.nn.relu)\n",
    "        out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), \"SAME\")\n",
    "        out = tf.layers.conv2d(out, 512, 3, 1, \"VALID\",\n",
    "                activation=tf.nn.relu)\n",
    "        img = add_timing_signal_nd(out)\n",
    "        print(img)\n",
    "        \n",
    "        with tf.variable_scope(\"attn_cell\", reuse=False):\n",
    "            attn_meca = AttentionMechanism(img, attention_size)\n",
    "            recu_cell = tf.nn.rnn_cell.LSTMCell(size_layer)\n",
    "            attn_cell = AttentionCell(recu_cell, attn_meca, 1.0,\n",
    "                        attention_size, attention_size, size_layer, len(encode_maps))\n",
    "\n",
    "            encoder_state = attn_cell.initial_state()\n",
    "\n",
    "            training_helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
    "                    sequence_length = self.Y_seq_len,\n",
    "                    embedding = decoder_embeddings,\n",
    "                    sampling_probability = 0.5,\n",
    "                    time_major = False)\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                    cell = attn_cell,\n",
    "                    helper = training_helper,\n",
    "                    initial_state = encoder_state,\n",
    "                    output_layer = None)\n",
    "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder = training_decoder,\n",
    "                    impute_finished = True,\n",
    "                    maximum_iterations = tf.reduce_max(self.Y_seq_len))\n",
    "        \n",
    "        with tf.variable_scope(\"attn_cell\", reuse=True):\n",
    "            attn_meca = AttentionMechanism(img, attention_size, tiles=beam_width)\n",
    "            recu_cell = tf.nn.rnn_cell.LSTMCell(size_layer, reuse = True)\n",
    "            attn_cell = AttentionCell(recu_cell, attn_meca, 1.0,\n",
    "                        attention_size, attention_size, size_layer, len(encode_maps))\n",
    "            \n",
    "            encoder_state = attn_cell.initial_state()\n",
    "            \n",
    "            predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                cell = attn_cell,\n",
    "                embedding = decoder_embeddings,\n",
    "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
    "                end_token = EOS,\n",
    "                initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, beam_width),\n",
    "                beam_width = beam_width,\n",
    "                output_layer = None,\n",
    "                length_penalty_weight = 0.0)\n",
    "            predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                decoder = predicting_decoder,\n",
    "                impute_finished = False,\n",
    "                maximum_iterations = x_len)\n",
    "            \n",
    "        self.training_logits = training_decoder_output.rnn_output\n",
    "        self.predicting_ids = predicting_decoder_output.predicted_ids\n",
    "        \n",
    "        masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
    "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
    "                                                     targets = self.Y,\n",
    "                                                     weights = masks)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
    "        y_t = tf.argmax(self.training_logits,axis=2)\n",
    "        y_t = tf.cast(y_t, tf.int32)\n",
    "        self.prediction = tf.boolean_mask(y_t, masks)\n",
    "        mask_label = tf.boolean_mask(self.Y, masks)\n",
    "        correct_pred = tf.equal(self.prediction, mask_label)\n",
    "        correct_index = tf.cast(correct_pred, tf.float32)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(?, 6, 28, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'attn_cell/decoder/transpose:0' shape=(?, ?, 33) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.936648, 0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = train_X[:5]\n",
    "batch_x = np.array(batch_x).reshape((len(batch_x), image_height, image_width,image_channel))\n",
    "y = train_Y[:5]\n",
    "batch_y, _ = pad_sentence_batch(y, 0)\n",
    "loss, logits, acc = sess.run([model.cost, model.training_logits, model.accuracy], feed_dict = {model.X: batch_x,\n",
    "                                                          model.Y: batch_y})\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 120, 15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = train_X[:5]\n",
    "batch_x = np.array(batch_x).reshape((len(batch_x), image_height, image_width,image_channel))\n",
    "y = train_Y[:5]\n",
    "batch_y, _ = pad_sentence_batch(y, 0)\n",
    "logits = sess.run(model.predicting_ids, feed_dict = {model.X: batch_x})\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:35<00:00,  4.82it/s, accuracy=0.291, cost=2.39] \n",
      "minibatch loop: 100%|██████████| 49/49 [00:04<00:00,  7.96it/s, accuracy=0.222, cost=2.53]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.91it/s, accuracy=0.229, cost=2.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, training avg loss 2.604917, training avg acc 0.218242\n",
      "epoch 1, testing avg loss 2.533392, testing avg acc 0.233053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.581, cost=1.29]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.43it/s, accuracy=0.527, cost=1.58]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.84it/s, accuracy=0.513, cost=1.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, training avg loss 1.962246, training avg acc 0.406816\n",
      "epoch 2, testing avg loss 1.629862, testing avg acc 0.501623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.767, cost=0.678]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.59it/s, accuracy=0.737, cost=0.814]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.83it/s, accuracy=0.753, cost=0.801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, training avg loss 1.003462, training avg acc 0.695401\n",
      "epoch 3, testing avg loss 0.832456, testing avg acc 0.744788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.91it/s, accuracy=0.884, cost=0.432]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.53it/s, accuracy=0.802, cost=0.588]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.77it/s, accuracy=0.806, cost=0.631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, training avg loss 0.657970, training avg acc 0.799655\n",
      "epoch 4, testing avg loss 0.656520, testing avg acc 0.800337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.895, cost=0.31] \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.48it/s, accuracy=0.772, cost=0.74] \n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.77it/s, accuracy=0.794, cost=0.627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, training avg loss 0.527271, training avg acc 0.837683\n",
      "epoch 5, testing avg loss 0.702134, testing avg acc 0.787779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:32<00:00,  5.91it/s, accuracy=0.919, cost=0.282]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.55it/s, accuracy=0.853, cost=0.447]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.87it/s, accuracy=0.855, cost=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, training avg loss 0.456834, training avg acc 0.858383\n",
      "epoch 6, testing avg loss 0.474705, testing avg acc 0.851827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.919, cost=0.287]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.47it/s, accuracy=0.867, cost=0.383]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.88it/s, accuracy=0.879, cost=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, training avg loss 0.409481, training avg acc 0.871234\n",
      "epoch 7, testing avg loss 0.419370, testing avg acc 0.865804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.965, cost=0.188]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.51it/s, accuracy=0.858, cost=0.447]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.92it/s, accuracy=0.877, cost=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, training avg loss 0.372560, training avg acc 0.882091\n",
      "epoch 8, testing avg loss 0.458827, testing avg acc 0.858051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=1, cost=0.0806]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.52it/s, accuracy=0.87, cost=0.388] \n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.72it/s, accuracy=0.873, cost=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, training avg loss 0.346476, training avg acc 0.889209\n",
      "epoch 9, testing avg loss 0.433993, testing avg acc 0.866077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.988, cost=0.0778]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.61it/s, accuracy=0.873, cost=0.376]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.85it/s, accuracy=0.893, cost=0.332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, training avg loss 0.325099, training avg acc 0.895045\n",
      "epoch 10, testing avg loss 0.401680, testing avg acc 0.875152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.988, cost=0.0811]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.56it/s, accuracy=0.875, cost=0.454]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.83it/s, accuracy=0.903, cost=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, training avg loss 0.310844, training avg acc 0.900292\n",
      "epoch 11, testing avg loss 0.383502, testing avg acc 0.878552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.89it/s, accuracy=1, cost=0.0416]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.47it/s, accuracy=0.88, cost=0.383] \n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.78it/s, accuracy=0.91, cost=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, training avg loss 0.291802, training avg acc 0.904957\n",
      "epoch 12, testing avg loss 0.372356, testing avg acc 0.884806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=1, cost=0.0474]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.41it/s, accuracy=0.883, cost=0.439]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.71it/s, accuracy=0.894, cost=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, training avg loss 0.274372, training avg acc 0.911635\n",
      "epoch 13, testing avg loss 0.375943, testing avg acc 0.884004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:32<00:00,  5.91it/s, accuracy=1, cost=0.0333]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.51it/s, accuracy=0.886, cost=0.383]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.71it/s, accuracy=0.912, cost=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, training avg loss 0.261932, training avg acc 0.914897\n",
      "epoch 14, testing avg loss 0.353538, testing avg acc 0.890513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.988, cost=0.0482]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.49it/s, accuracy=0.886, cost=0.357]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.88it/s, accuracy=0.914, cost=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, training avg loss 0.253819, training avg acc 0.917633\n",
      "epoch 15, testing avg loss 0.340455, testing avg acc 0.893801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.953, cost=0.118]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.61it/s, accuracy=0.894, cost=0.404]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.96it/s, accuracy=0.919, cost=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, training avg loss 0.247588, training avg acc 0.919401\n",
      "epoch 16, testing avg loss 0.354268, testing avg acc 0.890617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=1, cost=0.0488]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.43it/s, accuracy=0.889, cost=0.345]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:33,  5.78it/s, accuracy=0.926, cost=0.239]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, training avg loss 0.238150, training avg acc 0.922132\n",
      "epoch 17, testing avg loss 0.350447, testing avg acc 0.890544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=1, cost=0.0248]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.55it/s, accuracy=0.88, cost=0.361] \n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:32,  5.89it/s, accuracy=0.912, cost=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, training avg loss 0.218020, training avg acc 0.927836\n",
      "epoch 18, testing avg loss 0.377691, testing avg acc 0.885804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=1, cost=0.0275]   \n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.55it/s, accuracy=0.891, cost=0.331]\n",
      "minibatch loop:   1%|          | 1/195 [00:00<00:34,  5.68it/s, accuracy=0.927, cost=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, training avg loss 0.212462, training avg acc 0.929392\n",
      "epoch 19, testing avg loss 0.376349, testing avg acc 0.883670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch loop: 100%|██████████| 195/195 [00:33<00:00,  5.90it/s, accuracy=0.977, cost=0.078]\n",
      "minibatch loop: 100%|██████████| 49/49 [00:03<00:00, 13.59it/s, accuracy=0.88, cost=0.345] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, training avg loss 0.200291, training avg acc 0.933694\n",
      "epoch 20, testing avg loss 0.356056, testing avg acc 0.890788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(train_X), batch_size), desc = 'minibatch loop')\n",
    "    train_loss, train_acc, test_loss, test_acc = [], [], [], []\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(train_X))\n",
    "        batch_x = train_X[i : index]\n",
    "        batch_x = np.array(batch_x).reshape((len(batch_x), image_height, image_width,image_channel))\n",
    "        y = train_Y[i : index]\n",
    "        batch_y, _ = pad_sentence_batch(y, 0)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y}\n",
    "        accuracy, loss, _ = sess.run([model.accuracy,model.cost,model.optimizer],\n",
    "                                    feed_dict = feed)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    \n",
    "    pbar = tqdm.tqdm(\n",
    "        range(0, len(test_X), batch_size), desc = 'minibatch loop')\n",
    "    for i in pbar:\n",
    "        index = min(i + batch_size, len(test_X))\n",
    "        batch_x = test_X[i : index]\n",
    "        batch_x = np.array(batch_x).reshape((len(batch_x), image_height, image_width,image_channel))\n",
    "        y = test_Y[i : index]\n",
    "        batch_y, _ = pad_sentence_batch(y, 0)\n",
    "        feed = {model.X: batch_x,\n",
    "                model.Y: batch_y,}\n",
    "        accuracy, loss = sess.run([model.accuracy,model.cost],\n",
    "                                    feed_dict = feed)\n",
    "\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(accuracy)\n",
    "        pbar.set_postfix(cost = loss, accuracy = accuracy)\n",
    "    \n",
    "    print('epoch %d, training avg loss %f, training avg acc %f'%(e+1,\n",
    "                                                                 np.mean(train_loss),np.mean(train_acc)))\n",
    "    print('epoch %d, testing avg loss %f, testing avg acc %f'%(e+1,\n",
    "                                                              np.mean(test_loss),np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = sess.run(model.predicting_ids, feed_dict = {model.X: batch_x[3:4],\n",
    "                                          model.Y: batch_y[3:4]})[0]\n",
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n",
      "pemutung\n"
     ]
    }
   ],
   "source": [
    "for i in range(decoded.shape[1]):\n",
    "    d = decoded[:,0]\n",
    "    print(''.join([decode_maps[i] for i in d if i not in [0,1,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.flip(batch_x[3][:,:,0], 1))\n",
    "decoded = ''.join([decode_maps[i] for i in decoded[:,0] if i not in [0,1,2]])\n",
    "actual = ''.join([decode_maps[i] for i in batch_y[3] if i not in [0,1,2]])\n",
    "plt.title('predict: %s, actual: %s'%(decoded, actual))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
